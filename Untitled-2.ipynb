{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1230a7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "/var/folders/k9/ywt32ng54d766jxqldqb4y7c0000gn/T/ipykernel_25130/3419538590.py:110: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  diff = y[i + 1] - y[i]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Attention, TimeDistributed, RepeatVector, Input, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pykalman import KalmanFilter\n",
    "import gym\n",
    "import gym.spaces\n",
    "import stable_baselines3 as sb3\n",
    "from stable_baselines3 import PPO\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Integer, Real\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Input, LSTM, GRU, Dense, Dropout, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from ta.volatility import BollingerBands\n",
    "\n",
    "import pywt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score, classification_report\n",
    "\n",
    "\n",
    "# Step 1: Download the data\n",
    "data = yf.download('^GSPC', start='2010-01-01', end='2025-04-12')\n",
    "\n",
    "# Step 2: Compute indicators\n",
    "data['SMA_crossover'] = data['Close'].rolling(window=200).mean() - data['Close'].rolling(window=50).mean()\n",
    "\n",
    "#def stochastic_oscillator(df, window=14):\n",
    "#    low_min = df['Low'].rolling(window).min()\n",
    "#    high_max = df['High'].rolling(window).max()\n",
    "#    df['%K'] = 100 * ((df['Close'] - low_min) / (high_max - low_min))\n",
    "#    return df\n",
    "\n",
    "#data = stochastic_oscillator(data)\n",
    "\n",
    "def average_true_range(df, window=14):\n",
    "    high_low = df['High'] - df['Low']\n",
    "    high_close = abs(df['High'] - df['Close'].shift(1))\n",
    "    low_close = abs(df['Low'] - df['Close'].shift(1))\n",
    "    true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    df['ATR_14'] = true_range.rolling(window).mean()  # SMA of TR\n",
    "    return df\n",
    "\n",
    "data = average_true_range(data)  \n",
    "\n",
    "def on_balance_volume(df):\n",
    "    df['OBV'] = (np.sign(df['Close'].diff()) * df['Volume']).cumsum()\n",
    "    return df\n",
    "\n",
    "data = on_balance_volume(data)  # Adds OBV column\n",
    "\n",
    "def vwap(df):\n",
    "    df['VWAP'] = (df['Close'] * df['Volume']).cumsum() / df['Volume'].cumsum()\n",
    "    return df\n",
    "\n",
    "data = vwap(data)  # Adds VWAP column\n",
    "\n",
    "delta = data['Close'].diff()\n",
    "gain = delta.where(delta > 0, 0)\n",
    "loss = -delta.where(delta < 0, 0)\n",
    "avg_gain = gain.rolling(window=14).mean()\n",
    "avg_loss = loss.rolling(window=14).mean()\n",
    "rs = avg_gain / avg_loss\n",
    "data['RSI_14'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "ema_12 = data['Close'].ewm(span=12, adjust=False).mean()\n",
    "ema_26 = data['Close'].ewm(span=26, adjust=False).mean()\n",
    "data['MACD'] = ema_12 - ema_26\n",
    "\n",
    "data['Return'] = data['Close'].pct_change()\n",
    "data['Volatility'] = data['Return'].rolling(window=10).std()\n",
    "\n",
    "bb_indicator = BollingerBands(close=data['Close'].squeeze(), window=20, window_dev=2)\n",
    "data['BB_bandWidth'] = bb_indicator.bollinger_hband() - bb_indicator.bollinger_lband()\n",
    "\n",
    "#data['Return_3'] = data['Close'].pct_change(3).shift(1)  # 3-day cumulative return\n",
    "#data['Return_5'] = data['Close'].pct_change(5).shift(1)  # 5-day cumulative return\n",
    "data['SMA_17'] = data['Close'].rolling(window=17).mean()\n",
    "data['SMA_5'] = data['Close'].rolling(window=5).mean()\n",
    "data['SMA_9'] = data['Close'].rolling(window=9).mean()\n",
    "data['SMA_crossover_binary'] = (data['Close'].rolling(window=10).mean() > data['Close'].rolling(window=50).mean()).astype(int)\n",
    "\n",
    "#data['Volatility_Trend'] = data['Volatility'] - data['Volatility'].shift(1)\n",
    "\n",
    "data['MACD_Signal'] = data['MACD'].ewm(span=9, adjust=False).mean()\n",
    "data['MACD_Histogram'] = data['MACD'] - data['MACD_Signal']\n",
    "\n",
    "kf = KalmanFilter(\n",
    "    transition_matrices=[1],\n",
    "    observation_matrices=[1],\n",
    "    initial_state_mean=data['Close'].shift(-3).values[0],\n",
    "    initial_state_covariance=1.0,\n",
    "    observation_covariance=1.0,\n",
    "    transition_covariance=0.1\n",
    ")\n",
    "data['Target'], _ = kf.filter(data['Close'].shift(-3).values)\n",
    "\n",
    "def create_labels(y):\n",
    "    labels = np.zeros((len(y), 3))\n",
    "    for i in range(len(y) - 1):\n",
    "        diff = y[i + 1] - y[i]\n",
    "        if diff > 0.006:  # Threshold for UP\n",
    "            labels[i, 0] = 1  # UP\n",
    "        elif diff < -0.006:  # Threshold for DOWN\n",
    "            labels[i, 1] = 1  # DOWN\n",
    "        else:\n",
    "            labels[i, 2] = 1  # NEUTRAL\n",
    "    return labels\n",
    "\n",
    "\n",
    "data.dropna(inplace=True)\n",
    "features = ['SMA_crossover','SMA_17' ,'SMA_9','SMA_5','ATR_14', 'OBV', 'VWAP','Volume','BB_bandWidth','MACD','RSI_14','MACD_Histogram','Volatility']\n",
    "\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "data[features] = scaler_X.fit_transform(data[features])\n",
    "data['Target'] = scaler_y.fit_transform(data['Target'].values.reshape(-1, 1))\n",
    "\n",
    "X = data[features].values\n",
    "y = data['Target']\n",
    "y_class = np.argmax(create_labels(data['Target']), axis=1)\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test, y_train_class, y_test_class = y[:split], y[split:], y_class[:split], y_class[split:]\n",
    "\n",
    "prices = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edd8c177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape after dropna: (3641, 21)\n",
      "Feature data shape: (3641, 13)\n",
      "Prices shape: (3641,)\n",
      "Trend labels shape: (3641,)\n",
      "Data shape: (2912, 13), Prices shape: (2912,), Labels shape: (2912,)\n",
      "Observation space shape: (16,)\n",
      "Data shape: (729, 13), Prices shape: (729,), Labels shape: (729,)\n",
      "Observation space shape: (16,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 12:07:29.596201: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2025-04-20 12:07:29.596546: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2025-04-20 12:07:29.596565: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2025-04-20 12:07:29.596884: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-04-20 12:07:29.597344: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN model input shape: (16,)\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 12:07:30.255198: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Episode 1/100, Total Reward: -40.05, Portfolio Value: 10020.16, Profit/Loss: 20.16, Sharpe: 2.95, Accuracy: 0.24, Time: 5.87s\n",
      "Train Episode 2/100, Total Reward: -29.56, Portfolio Value: 10004.69, Profit/Loss: 4.69, Sharpe: 1.10, Accuracy: 0.30, Time: 4.57s\n",
      "Train Episode 3/100, Total Reward: -35.94, Portfolio Value: 10045.21, Profit/Loss: 45.21, Sharpe: 2.81, Accuracy: 0.35, Time: 5.27s\n",
      "Train Episode 4/100, Total Reward: -45.54, Portfolio Value: 10083.34, Profit/Loss: 83.34, Sharpe: 2.67, Accuracy: 0.11, Time: 5.17s\n",
      "Train Episode 5/100, Total Reward: -38.69, Portfolio Value: 10067.74, Profit/Loss: 67.74, Sharpe: 2.63, Accuracy: 0.28, Time: 4.70s\n",
      "Train Episode 6/100, Total Reward: -45.08, Portfolio Value: 10070.25, Profit/Loss: 70.25, Sharpe: 2.63, Accuracy: 0.12, Time: 5.07s\n",
      "Train Episode 7/100, Total Reward: -42.33, Portfolio Value: 10025.58, Profit/Loss: 25.58, Sharpe: 2.27, Accuracy: 0.19, Time: 4.74s\n",
      "Train Episode 8/100, Total Reward: -49.67, Portfolio Value: 10033.52, Profit/Loss: 33.52, Sharpe: 3.00, Accuracy: 0.01, Time: 4.88s\n",
      "Train Episode 9/100, Total Reward: -49.90, Portfolio Value: 10022.31, Profit/Loss: 22.31, Sharpe: 1.97, Accuracy: 0.00, Time: 4.78s\n",
      "Train Episode 10/100, Total Reward: -49.83, Portfolio Value: 10022.77, Profit/Loss: 22.77, Sharpe: 1.80, Accuracy: 0.00, Time: 4.72s\n",
      "Train Episode 11/100, Total Reward: -49.94, Portfolio Value: 10018.07, Profit/Loss: 18.07, Sharpe: 2.48, Accuracy: 0.00, Time: 4.71s\n",
      "Train Episode 12/100, Total Reward: -49.71, Portfolio Value: 10044.58, Profit/Loss: 44.58, Sharpe: 2.37, Accuracy: 0.01, Time: 4.67s\n",
      "Train Episode 13/100, Total Reward: -49.73, Portfolio Value: 10063.64, Profit/Loss: 63.64, Sharpe: 2.67, Accuracy: 0.01, Time: 4.67s\n",
      "Train Episode 14/100, Total Reward: -49.88, Portfolio Value: 10043.87, Profit/Loss: 43.87, Sharpe: 2.51, Accuracy: 0.00, Time: 5.02s\n",
      "Train Episode 15/100, Total Reward: -49.30, Portfolio Value: 10012.56, Profit/Loss: 12.56, Sharpe: 3.24, Accuracy: 0.02, Time: 6.19s\n",
      "Train Episode 16/100, Total Reward: -49.78, Portfolio Value: 10085.65, Profit/Loss: 85.65, Sharpe: 2.62, Accuracy: 0.00, Time: 5.92s\n",
      "Train Episode 17/100, Total Reward: -48.48, Portfolio Value: 10056.90, Profit/Loss: 56.90, Sharpe: 2.55, Accuracy: 0.04, Time: 6.05s\n",
      "Early stopping triggered.\n",
      "\n",
      "Testing...\n",
      "Test Results: Total Reward: -36.04, Portfolio Value: 10080.54, Profit/Loss: 80.54, Sharpe: 0.94, Accuracy: 0.05, Time: 0.89s\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "import yfinance as yf\n",
    "from ta.volatility import BollingerBands\n",
    "from pykalman import KalmanFilter\n",
    "\n",
    "# TradingEnvironment\n",
    "class TradingEnvironment(gym.Env):\n",
    "    def __init__(self, data, features, prices, trend_labels, initial_cash=10000, max_steps=1000):\n",
    "        super(TradingEnvironment, self).__init__()\n",
    "        self.data = data\n",
    "        self.features = features\n",
    "        self.prices = prices\n",
    "        self.trend_labels = np.array(trend_labels)  # For metrics only\n",
    "        self.initial_cash = initial_cash\n",
    "        self.max_steps = max_steps\n",
    "        self.current_step = 0\n",
    "        self.cash = initial_cash\n",
    "        self.holdings = 0\n",
    "        self.portfolio_values = []\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf,\n",
    "            shape=(len(features) + 3,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        self.feature_means = np.mean(data, axis=0)\n",
    "        self.feature_stds = np.std(data, axis=0) + 1e-6\n",
    "        self.normalized_data = (data - self.feature_means) / self.feature_stds\n",
    "        self.actions = []\n",
    "        self.buy_count = 0\n",
    "        self.sell_count = 0\n",
    "        self.hold_count = 0\n",
    "        self.prev_action = 2\n",
    "        print(f\"Data shape: {self.data.shape}, Prices shape: {self.prices.shape}, Labels shape: {self.trend_labels.shape}\")\n",
    "        print(f\"Observation space shape: {self.observation_space.shape}\")\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.cash = self.initial_cash\n",
    "        self.holdings = 0\n",
    "        self.portfolio_values = []\n",
    "        self.actions = []\n",
    "        self.buy_count = 0\n",
    "        self.sell_count = 0\n",
    "        self.hold_count = 0\n",
    "        self.prev_action = 2\n",
    "        return self._get_observation()\n",
    "\n",
    "    def _get_observation(self):\n",
    "        if self.current_step >= len(self.data):\n",
    "            return np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        features = self.normalized_data[self.current_step]\n",
    "        state = np.concatenate([\n",
    "            features,\n",
    "            [(self.cash / self.initial_cash) - 1],\n",
    "            [self.holdings],\n",
    "            [self.prev_action]\n",
    "        ])\n",
    "        assert state.shape == (len(self.features) + 3,), f\"Invalid state shape: {state.shape}\"\n",
    "        return state.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        current_price = self.prices[self.current_step]\n",
    "        reward = 0\n",
    "        if action == 0 and self.cash >= current_price:\n",
    "            self.holdings += 1\n",
    "            self.cash -= current_price\n",
    "            self.buy_count += 1\n",
    "            reward -= 0.05\n",
    "        elif action == 1 and self.holdings > 0:\n",
    "            self.holdings -= 1\n",
    "            self.cash += current_price\n",
    "            self.sell_count += 1\n",
    "            reward -= 0.05\n",
    "        else:\n",
    "            self.hold_count += 1\n",
    "            reward -= 0.01  # Penalize holding\n",
    "        self.actions.append(action)\n",
    "        self.prev_action = action\n",
    "        portfolio_value = self.cash + self.holdings * current_price\n",
    "        self.portfolio_values.append(portfolio_value)\n",
    "        if len(self.portfolio_values) > 1:\n",
    "            portfolio_change = (portfolio_value - self.portfolio_values[-2]) / self.initial_cash\n",
    "            reward += portfolio_change * 30  # Increased weight\n",
    "            if len(self.portfolio_values) >= 11:\n",
    "                returns = np.diff(self.portfolio_values[-10:]) / self.portfolio_values[-11:-2]\n",
    "                volatility = np.std(returns)\n",
    "                reward -= volatility * 5\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= min(len(self.data), self.max_steps)\n",
    "        next_state = self._get_observation() if not done else np.zeros_like(self._get_observation())\n",
    "        info = {'portfolio_value': portfolio_value}\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def get_metrics(self):\n",
    "        total_steps = len(self.actions)\n",
    "        accuracy = np.mean([a == t for a, t in zip(self.actions, self.trend_labels[:total_steps])]) if total_steps > 0 else 0\n",
    "        final_value = self.portfolio_values[-1] if self.portfolio_values else self.initial_cash\n",
    "        sharpe = 0\n",
    "        if len(self.portfolio_values) > 1:\n",
    "            returns = np.diff(self.portfolio_values) / np.array(self.portfolio_values[:-1])\n",
    "            if len(returns) > 0 and np.std(returns) > 0:\n",
    "                sharpe = np.mean(returns) / np.std(returns) * np.sqrt(252)\n",
    "        return {\n",
    "            'final_portfolio_value': final_value,\n",
    "            'profit_loss': final_value - self.initial_cash,\n",
    "            'sharpe_ratio': sharpe,\n",
    "            'buy_count': self.buy_count,\n",
    "            'sell_count': self.sell_count,\n",
    "            'hold_count': self.hold_count,\n",
    "            'action_accuracy': accuracy\n",
    "        }\n",
    "\n",
    "# DQNAgent with Double DQN\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_shape, action_size, learning_rate=0.0005, gamma=0.99):\n",
    "        self.state_shape = state_shape\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.99\n",
    "        self.learning_rate = learning_rate\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.priorities = deque(maxlen=2000)\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "        print(f\"DQN model input shape: {self.state_shape}\")\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=self.state_shape),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(self.action_size, activation='linear')\n",
    "        ])\n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        priority = abs(reward) + 0.1\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        self.priorities.append(priority)\n",
    "\n",
    "    def act(self, state, explore=True):\n",
    "        if explore and np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.action_size)\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        q_values = self.model.predict_on_batch(state)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        priorities = np.array(self.priorities)\n",
    "        probs = priorities / priorities.sum()\n",
    "        indices = np.random.choice(len(self.memory), batch_size, p=probs)\n",
    "        minibatch = [self.memory[i] for i in indices]\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "        states = np.array(states)\n",
    "        next_states = np.array(next_states)\n",
    "        actions = np.array(actions)\n",
    "        rewards = np.array(rewards)\n",
    "        dones = np.array(dones)\n",
    "        #print(f\"Replay states shape: {states.shape}, next_states shape: {next_states.shape}\")\n",
    "        q_values = self.model.predict_on_batch(states)\n",
    "        next_q_values = self.model.predict_on_batch(next_states)\n",
    "        next_actions = np.argmax(next_q_values, axis=1)\n",
    "        target_q_values = self.target_model.predict_on_batch(next_states)\n",
    "        for i in range(batch_size):\n",
    "            target = rewards[i] if dones[i] else rewards[i] + self.gamma * target_q_values[i, next_actions[i]]\n",
    "            q_values[i, actions[i]] = target\n",
    "        self.model.fit(states, q_values, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "# Clean data before creating labels\n",
    "original_len = len(data)\n",
    "data.dropna(inplace=True)\n",
    "print(f\"Data shape after dropna: {data.shape}\")\n",
    "\n",
    "\n",
    "prices = data['Target'].values\n",
    "trend_labels = y_class\n",
    "# Select features\n",
    "features = ['SMA_crossover', 'SMA_17', 'SMA_9', 'SMA_5', 'ATR_14', 'OBV',\n",
    "            'VWAP', 'Volume', 'BB_bandWidth', 'MACD', 'Volatility', 'RSI_14', 'MACD_Histogram']\n",
    "feature_data = data[features].values\n",
    "\n",
    "# Debug: Verify shapes\n",
    "print(\"Feature data shape:\", feature_data.shape)\n",
    "print(\"Prices shape:\", prices.shape)\n",
    "print(\"Trend labels shape:\", trend_labels.shape)\n",
    "\n",
    "# Train-test split\n",
    "num_timesteps = len(data)\n",
    "train_size = int(0.8 * num_timesteps)\n",
    "train_data = feature_data[:train_size]\n",
    "test_data = feature_data[train_size:]\n",
    "train_prices = prices[:train_size]\n",
    "test_prices = prices[train_size:]\n",
    "train_labels = trend_labels[:train_size]\n",
    "test_labels = trend_labels[train_size:]\n",
    "\n",
    "# Initialize environments\n",
    "train_env = TradingEnvironment(train_data, features, train_prices, train_labels, max_steps=1000)\n",
    "test_env = TradingEnvironment(test_data, features, test_prices, test_labels, max_steps=1000)\n",
    "\n",
    "# Initialize agent\n",
    "agent = DQNAgent(state_shape=train_env.observation_space.shape, action_size=train_env.action_space.n)\n",
    "\n",
    "# Training phase\n",
    "episodes = 100\n",
    "batch_size = 32\n",
    "target_update_freq = 50\n",
    "best_reward = -np.inf\n",
    "patience = 15\n",
    "patience_counter = 0\n",
    "early_stop_threshold = 0.02\n",
    "\n",
    "print(\"Training...\")\n",
    "for e in range(episodes):\n",
    "    state = train_env.reset()\n",
    "    total_reward = 0\n",
    "    start_time = time.time()\n",
    "    step = 0\n",
    "    while True:\n",
    "        action = agent.act(state, explore=True)\n",
    "        next_state, reward, done, info = train_env.step(action)\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        step += 1\n",
    "        if step % 10 == 0:\n",
    "            agent.replay(batch_size)\n",
    "        if step % target_update_freq == 0:\n",
    "            agent.update_target_model()\n",
    "        if done:\n",
    "            break\n",
    "    metrics = train_env.get_metrics()\n",
    "    print(f\"Train Episode {e+1}/{episodes}, Total Reward: {total_reward:.2f}, \"\n",
    "          f\"Portfolio Value: {metrics['final_portfolio_value']:.2f}, \"\n",
    "          f\"Profit/Loss: {metrics['profit_loss']:.2f}, \"\n",
    "          f\"Sharpe: {metrics['sharpe_ratio']:.2f}, \"\n",
    "          f\"Accuracy: {metrics['action_accuracy']:.2f}, \"\n",
    "          f\"Time: {time.time() - start_time:.2f}s\")\n",
    "    if total_reward > best_reward + early_stop_threshold:\n",
    "        best_reward = total_reward\n",
    "        patience_counter = 0\n",
    "        agent.model.save('dqn_model.keras')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Testing phase\n",
    "print(\"\\nTesting...\")\n",
    "state = test_env.reset()\n",
    "total_reward = 0\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    action = agent.act(state, explore=False)\n",
    "    next_state, reward, done, info = test_env.step(action)\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "    if done:\n",
    "        break\n",
    "metrics = test_env.get_metrics()\n",
    "print(f\"Test Results: Total Reward: {total_reward:.2f}, \"\n",
    "      f\"Portfolio Value: {metrics['final_portfolio_value']:.2f}, \"\n",
    "      f\"Profit/Loss: {metrics['profit_loss']:.2f}, \"\n",
    "      f\"Sharpe: {metrics['sharpe_ratio']:.2f}, \"\n",
    "      f\"Accuracy: {metrics['action_accuracy']:.2f}, \"\n",
    "      f\"Time: {time.time() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5524fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data date range: 2010-01-04 00:00:00 to 2025-04-11 00:00:00\n",
      "Raw data shape: (3843, 5)\n",
      "MultiIndex detected, flattening columns...\n",
      "Initial columns: ['Close', 'High', 'Low', 'Open', 'Volume']\n",
      "Data shape after dropna: (3644, 20)\n",
      "Data shape after upsampling: (5290, 20)\n",
      "Feature data shape: (5290, 13)\n",
      "Prices shape: (5290,)\n",
      "Trend labels shape: (5290, 3)\n",
      "Data shape: (4232, 13), Prices shape: (4232,), Labels shape: (4232, 3)\n",
      "Observation space shape: (16,)\n",
      "Data shape: (1058, 13), Prices shape: (1058,), Labels shape: (1058, 3)\n",
      "Observation space shape: (16,)\n",
      "DQN model input shape: (16,)\n",
      "Training...\n",
      "Train Episode 1/200, Total Reward: -50.00, Portfolio Value: 10001.16, Profit/Loss: 1.16, Sharpe: 2.14, Accuracy: 0.36, Time: 5.14s\n",
      "Train Episode 2/200, Total Reward: -49.99, Portfolio Value: 10002.89, Profit/Loss: 2.89, Sharpe: 1.09, Accuracy: 0.56, Time: 5.30s\n",
      "Train Episode 3/200, Total Reward: -49.94, Portfolio Value: 10013.92, Profit/Loss: 13.92, Sharpe: 2.67, Accuracy: 0.31, Time: 5.90s\n",
      "Train Episode 4/200, Total Reward: -49.92, Portfolio Value: 10017.58, Profit/Loss: 17.58, Sharpe: 2.35, Accuracy: 0.38, Time: 5.44s\n",
      "Train Episode 5/200, Total Reward: -49.83, Portfolio Value: 10040.79, Profit/Loss: 40.79, Sharpe: 2.34, Accuracy: 0.28, Time: 5.44s\n",
      "Train Episode 6/200, Total Reward: -49.82, Portfolio Value: 10042.17, Profit/Loss: 42.17, Sharpe: 2.40, Accuracy: 0.28, Time: 5.44s\n",
      "Train Episode 7/200, Total Reward: -49.83, Portfolio Value: 10040.55, Profit/Loss: 40.55, Sharpe: 2.38, Accuracy: 0.25, Time: 5.41s\n",
      "Train Episode 8/200, Total Reward: -49.85, Portfolio Value: 10035.68, Profit/Loss: 35.68, Sharpe: 2.42, Accuracy: 0.38, Time: 5.42s\n",
      "Train Episode 9/200, Total Reward: -50.00, Portfolio Value: 10000.03, Profit/Loss: 0.03, Sharpe: 1.49, Accuracy: 0.99, Time: 5.40s\n",
      "Train Episode 10/200, Total Reward: -50.00, Portfolio Value: 10000.11, Profit/Loss: 0.11, Sharpe: 1.67, Accuracy: 0.99, Time: 5.44s\n",
      "Train Episode 11/200, Total Reward: -49.95, Portfolio Value: 10010.66, Profit/Loss: 10.66, Sharpe: 2.34, Accuracy: 0.50, Time: 5.43s\n",
      "Train Episode 12/200, Total Reward: -49.96, Portfolio Value: 10008.34, Profit/Loss: 8.34, Sharpe: 2.80, Accuracy: 0.69, Time: 5.43s\n",
      "Train Episode 13/200, Total Reward: -49.97, Portfolio Value: 10008.66, Profit/Loss: 8.66, Sharpe: 1.84, Accuracy: 0.24, Time: 5.49s\n",
      "Train Episode 14/200, Total Reward: -49.97, Portfolio Value: 10007.69, Profit/Loss: 7.69, Sharpe: 1.80, Accuracy: 0.49, Time: 5.80s\n",
      "Train Episode 15/200, Total Reward: -49.97, Portfolio Value: 10006.67, Profit/Loss: 6.67, Sharpe: 1.89, Accuracy: 0.28, Time: 5.84s\n",
      "Train Episode 16/200, Total Reward: -49.98, Portfolio Value: 10005.76, Profit/Loss: 5.76, Sharpe: 1.77, Accuracy: 0.38, Time: 5.67s\n",
      "Train Episode 17/200, Total Reward: -49.98, Portfolio Value: 10005.33, Profit/Loss: 5.33, Sharpe: 1.95, Accuracy: 0.41, Time: 5.49s\n",
      "Train Episode 18/200, Total Reward: -49.98, Portfolio Value: 10004.04, Profit/Loss: 4.04, Sharpe: 1.82, Accuracy: 0.39, Time: 5.52s\n",
      "Train Episode 19/200, Total Reward: -49.99, Portfolio Value: 10003.65, Profit/Loss: 3.65, Sharpe: 2.20, Accuracy: 0.43, Time: 5.82s\n",
      "Train Episode 20/200, Total Reward: -49.99, Portfolio Value: 10002.76, Profit/Loss: 2.76, Sharpe: 1.79, Accuracy: 0.26, Time: 5.53s\n",
      "Train Episode 21/200, Total Reward: -49.99, Portfolio Value: 10001.99, Profit/Loss: 1.99, Sharpe: 1.82, Accuracy: 0.51, Time: 5.54s\n",
      "Train Episode 22/200, Total Reward: -49.99, Portfolio Value: 10001.64, Profit/Loss: 1.64, Sharpe: 1.87, Accuracy: 0.29, Time: 5.89s\n",
      "Train Episode 23/200, Total Reward: -49.99, Portfolio Value: 10001.29, Profit/Loss: 1.29, Sharpe: 2.04, Accuracy: 0.59, Time: 5.60s\n",
      "Train Episode 24/200, Total Reward: -50.00, Portfolio Value: 10000.98, Profit/Loss: 0.98, Sharpe: 1.87, Accuracy: 0.08, Time: 5.63s\n",
      "Train Episode 25/200, Total Reward: -50.00, Portfolio Value: 10000.68, Profit/Loss: 0.68, Sharpe: 1.94, Accuracy: 0.60, Time: 5.56s\n",
      "Early stopping triggered.\n",
      "\n",
      "Testing...\n",
      "Test Results: Total Reward: -49.99, Portfolio Value: 10002.73, Profit/Loss: 2.73, Sharpe: 2.62, Accuracy: 0.95, Buy Count: 9, Sell Count: 0, Hold Count: 991, Time: 0.92s\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "import yfinance as yf\n",
    "from ta.volatility import BollingerBands\n",
    "from pykalman import KalmanFilter\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# TradingEnvironment\n",
    "class TradingEnvironment(gym.Env):\n",
    "    def __init__(self, data, features, prices, trend_labels, initial_cash=10000, max_steps=1000):\n",
    "        super(TradingEnvironment, self).__init__()\n",
    "        self.data = data\n",
    "        self.features = features\n",
    "        self.prices = prices\n",
    "        self.trend_labels = np.array(trend_labels)  # One-hot labels\n",
    "        self.initial_cash = initial_cash\n",
    "        self.max_steps = max_steps\n",
    "        self.current_step = 0\n",
    "        self.cash = initial_cash\n",
    "        self.holdings = 0\n",
    "        self.portfolio_values = []\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf,\n",
    "            shape=(len(features) + 3,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        self.feature_means = np.mean(data, axis=0)\n",
    "        self.feature_stds = np.std(data, axis=0) + 1e-6\n",
    "        self.normalized_data = (data - self.feature_means) / self.feature_stds\n",
    "        self.actions = []\n",
    "        self.buy_count = 0\n",
    "        self.sell_count = 0\n",
    "        self.hold_count = 0\n",
    "        self.prev_action = 2\n",
    "        print(f\"Data shape: {self.data.shape}, Prices shape: {self.prices.shape}, Labels shape: {self.trend_labels.shape}\")\n",
    "        print(f\"Observation space shape: {self.observation_space.shape}\")\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.cash = self.initial_cash\n",
    "        self.holdings = 0\n",
    "        self.portfolio_values = []\n",
    "        self.actions = []\n",
    "        self.buy_count = 0\n",
    "        self.sell_count = 0\n",
    "        self.hold_count = 0\n",
    "        self.prev_action = 2\n",
    "        return self._get_observation()\n",
    "\n",
    "    def _get_observation(self):\n",
    "        if self.current_step >= len(self.data):\n",
    "            return np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        features = self.normalized_data[self.current_step]\n",
    "        state = np.concatenate([\n",
    "            features,\n",
    "            [(self.cash / self.initial_cash) - 1],\n",
    "            [self.holdings],\n",
    "            [self.prev_action]\n",
    "        ])\n",
    "        assert state.shape == (len(self.features) + 3,), f\"Invalid state shape: {state.shape}\"\n",
    "        return state.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        current_price = self.prices[self.current_step]\n",
    "        reward = 0\n",
    "        if action == 0 and self.cash >= current_price:\n",
    "            self.holdings += 1\n",
    "            self.cash -= current_price\n",
    "            self.buy_count += 1\n",
    "            reward -= 0.05\n",
    "        elif action == 1 and self.holdings > 0:\n",
    "            self.holdings -= 1\n",
    "            self.cash += current_price\n",
    "            self.sell_count += 1\n",
    "            reward -= 0.05\n",
    "        else:\n",
    "            self.hold_count += 1\n",
    "            reward -= 0.05  # Stronger HOLD penalty\n",
    "        self.actions.append(action)\n",
    "        self.prev_action = action\n",
    "        portfolio_value = self.cash + self.holdings * current_price\n",
    "        self.portfolio_values.append(portfolio_value)\n",
    "        if len(self.portfolio_values) > 1:\n",
    "            portfolio_change = (portfolio_value - self.portfolio_values[-2]) / self.initial_cash\n",
    "            reward += portfolio_change * 50  # Increased weight\n",
    "            if len(self.portfolio_values) >= 11:\n",
    "                returns = np.diff(self.portfolio_values[-10:]) / self.portfolio_values[-11:-2]\n",
    "                volatility = np.std(returns)\n",
    "                reward -= volatility * 2  # Reduced penalty\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= min(len(self.data), self.max_steps)\n",
    "        next_state = self._get_observation() if not done else np.zeros_like(self._get_observation())\n",
    "        info = {'portfolio_value': portfolio_value}\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def get_metrics(self):\n",
    "        total_steps = len(self.actions)\n",
    "        predicted_labels = np.array(self.actions)\n",
    "        true_labels = np.argmax(self.trend_labels[:total_steps], axis=1)\n",
    "        accuracy = np.mean(predicted_labels == true_labels) if total_steps > 0 else 0\n",
    "        final_value = self.portfolio_values[-1] if self.portfolio_values else self.initial_cash\n",
    "        sharpe = 0\n",
    "        if len(self.portfolio_values) > 1:\n",
    "            returns = np.diff(self.portfolio_values) / np.array(self.portfolio_values[:-1])\n",
    "            if len(returns) > 0 and np.std(returns) > 0:\n",
    "                sharpe = np.mean(returns) / np.std(returns) * np.sqrt(252)\n",
    "        return {\n",
    "            'final_portfolio_value': final_value,\n",
    "            'profit_loss': final_value - self.initial_cash,\n",
    "            'sharpe_ratio': sharpe,\n",
    "            'buy_count': self.buy_count,\n",
    "            'sell_count': self.sell_count,\n",
    "            'hold_count': self.hold_count,\n",
    "            'action_accuracy': accuracy\n",
    "        }\n",
    "\n",
    "# DQNAgent with Double DQN\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_shape, action_size, learning_rate=0.0005, gamma=0.99):\n",
    "        self.state_shape = state_shape\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.99\n",
    "        self.learning_rate = learning_rate\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.priorities = deque(maxlen=2000)\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "        print(f\"DQN model input shape: {self.state_shape}\")\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=self.state_shape),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(self.action_size, activation='linear')\n",
    "        ])\n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        priority = abs(reward) + 0.1\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        self.priorities.append(priority)\n",
    "\n",
    "    def act(self, state, explore=True):\n",
    "        if explore and np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.action_size)\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        q_values = self.model.predict_on_batch(state)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        priorities = np.array(self.priorities)\n",
    "        probs = priorities / priorities.sum()\n",
    "        indices = np.random.choice(len(self.memory), batch_size, p=probs)\n",
    "        minibatch = [self.memory[i] for i in indices]\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "        states = np.array(states)\n",
    "        next_states = np.array(next_states)\n",
    "        actions = np.array(actions)\n",
    "        rewards = np.array(rewards)\n",
    "        dones = np.array(dones)\n",
    "        #print(f\"Replay states shape: {states.shape}, next_states shape: {next_states.shape}\")\n",
    "        q_values = self.model.predict_on_batch(states)\n",
    "        next_q_values = self.model.predict_on_batch(next_states)\n",
    "        next_actions = np.argmax(next_q_values, axis=1)\n",
    "        target_q_values = self.target_model.predict_on_batch(next_states)\n",
    "        for i in range(batch_size):\n",
    "            target = rewards[i] if dones[i] else rewards[i] + self.gamma * target_q_values[i, next_actions[i]]\n",
    "            q_values[i, actions[i]] = target\n",
    "        self.model.fit(states, q_values, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Data preparation\n",
    "data = yf.download('^GSPC', start='2010-01-01', end='2025-04-12')\n",
    "\n",
    "# Validate date range\n",
    "print(f\"Data date range: {data.index[0]} to {data.index[-1]}\")\n",
    "print(f\"Raw data shape: {data.shape}\")\n",
    "\n",
    "# Flatten MultiIndex columns\n",
    "if isinstance(data.columns, pd.MultiIndex):\n",
    "    print(\"MultiIndex detected, flattening columns...\")\n",
    "    data.columns = [col[0] for col in data.columns]\n",
    "print(\"Initial columns:\", data.columns.tolist())\n",
    "\n",
    "# Apply Kalman Filter to smooth Close prices (no shift)\n",
    "kf = KalmanFilter(\n",
    "    transition_matrices=[1],\n",
    "    observation_matrices=[1],\n",
    "    initial_state_mean=data['Close'].iloc[0],\n",
    "    initial_state_covariance=1.0,\n",
    "    observation_covariance=1.0,\n",
    "    transition_covariance=0.1\n",
    ")\n",
    "smoothed_prices, _ = kf.filter(data['Close'].values)\n",
    "data['Target'] = smoothed_prices\n",
    "\n",
    "# Compute indicators\n",
    "data['SMA_crossover'] = data['Close'].rolling(window=200).mean() - data['Close'].rolling(window=50).mean()\n",
    "\n",
    "def average_true_range(df, window=14):\n",
    "    high_low = df['High'] - df['Low']\n",
    "    high_close = abs(df['High'] - df['Close'].shift(1))\n",
    "    low_close = abs(df['Low'] - df['Close'].shift(1))\n",
    "    true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    df['ATR_14'] = true_range.rolling(window).mean()\n",
    "    return df\n",
    "\n",
    "data = average_true_range(data)\n",
    "\n",
    "def on_balance_volume(df):\n",
    "    df['OBV'] = (np.sign(df['Close'].diff()) * df['Volume']).cumsum()\n",
    "    return df\n",
    "\n",
    "data = on_balance_volume(data)\n",
    "\n",
    "def vwap(df):\n",
    "    df['VWAP'] = (df['Close'] * df['Volume']).cumsum() / df['Volume'].cumsum()\n",
    "    return df\n",
    "\n",
    "data = vwap(data)\n",
    "\n",
    "delta = data['Close'].diff()\n",
    "gain = delta.where(delta > 0, 0)\n",
    "loss = -delta.where(delta < 0, 0)\n",
    "avg_gain = gain.rolling(window=14).mean()\n",
    "avg_loss = loss.rolling(window=14).mean()\n",
    "rs = avg_gain / avg_loss\n",
    "data['RSI_14'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "ema_12 = data['Close'].ewm(span=12, adjust=False).mean()\n",
    "ema_26 = data['Close'].ewm(span=26, adjust=False).mean()\n",
    "data['MACD'] = ema_12 - ema_26\n",
    "\n",
    "data['Return'] = data['Close'].pct_change()\n",
    "data['Volatility'] = data['Return'].rolling(window=10).std()\n",
    "\n",
    "bb_indicator = BollingerBands(close=data['Close'], window=20, window_dev=2)\n",
    "data['BB_bandWidth'] = bb_indicator.bollinger_hband() - bb_indicator.bollinger_lband()\n",
    "\n",
    "data['SMA_17'] = data['Close'].rolling(window=17).mean()\n",
    "data['SMA_5'] = data['Close'].rolling(window=5).mean()\n",
    "data['SMA_9'] = data['Close'].rolling(window=9).mean()\n",
    "\n",
    "data['MACD_Signal'] = data['MACD'].ewm(span=9, adjust=False).mean()\n",
    "data['MACD_Histogram'] = data['MACD'] - data['MACD_Signal']\n",
    "\n",
    "# Normalize features and target\n",
    "features = ['SMA_crossover', 'SMA_17', 'SMA_9', 'SMA_5', 'ATR_14', 'OBV',\n",
    "            'VWAP', 'Volume', 'BB_bandWidth', 'MACD', 'RSI_14', 'MACD_Histogram', 'Volatility']\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "data[features] = scaler_X.fit_transform(data[features])\n",
    "data['Target'] = scaler_y.fit_transform(data['Target'].values.reshape(-1, 1))\n",
    "\n",
    "# Create labels\n",
    "def create_labels(y):\n",
    "    labels = np.zeros((len(y), 3))\n",
    "    for i in range(len(y) - 1):\n",
    "        diff = y[i + 1] - y[i]\n",
    "        if diff > 0.006:  # UP\n",
    "            labels[i, 0] = 1\n",
    "        elif diff < -0.006:  # DOWN\n",
    "            labels[i, 1] = 1\n",
    "        else:  # NEUTRAL\n",
    "            labels[i, 2] = 1\n",
    "    labels[-1, 2] = 1\n",
    "    return labels\n",
    "\n",
    "# Clean data\n",
    "data.dropna(inplace=True)\n",
    "print(f\"Data shape after dropna: {data.shape}\")\n",
    "\n",
    "# Upsample to 6915 rows (placeholder, replace with your method)\n",
    "if len(data) < 6915:\n",
    "    data = data.reindex(pd.date_range(start=data.index[0], end=data.index[-1], freq='D'))\n",
    "    data = data.interpolate(method='linear')\n",
    "    data = data.dropna()\n",
    "    print(f\"Data shape after upsampling: {data.shape}\")\n",
    "\n",
    "X = data[features].values\n",
    "y = data['Target'].values\n",
    "y_class = create_labels(y)\n",
    "\n",
    "# Debug: Verify shapes\n",
    "print(\"Feature data shape:\", X.shape)\n",
    "print(\"Prices shape:\", y.shape)\n",
    "print(\"Trend labels shape:\", y_class.shape)\n",
    "\n",
    "# Train-test split\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "y_train_class, y_test_class = y_class[:split], y_class[split:]\n",
    "\n",
    "# Initialize environments\n",
    "train_env = TradingEnvironment(X_train, features, y_train, y_train_class, max_steps=1000)\n",
    "test_env = TradingEnvironment(X_test, features, y_test, y_test_class, max_steps=1000)\n",
    "\n",
    "# Initialize agent\n",
    "agent = DQNAgent(state_shape=train_env.observation_space.shape, action_size=train_env.action_space.n)\n",
    "\n",
    "# Training phase\n",
    "episodes = 200\n",
    "batch_size = 64\n",
    "target_update_freq = 50\n",
    "best_reward = -np.inf\n",
    "patience = 20\n",
    "patience_counter = 0\n",
    "early_stop_threshold = 0.01\n",
    "\n",
    "print(\"Training...\")\n",
    "for e in range(episodes):\n",
    "    state = train_env.reset()\n",
    "    total_reward = 0\n",
    "    start_time = time.time()\n",
    "    step = 0\n",
    "    while True:\n",
    "        action = agent.act(state, explore=True)\n",
    "        next_state, reward, done, info = train_env.step(action)\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        step += 1\n",
    "        if step % 10 == 0:\n",
    "            agent.replay(batch_size)\n",
    "        if step % target_update_freq == 0:\n",
    "            agent.update_target_model()\n",
    "        if done:\n",
    "            break\n",
    "    metrics = train_env.get_metrics()\n",
    "    print(f\"Train Episode {e+1}/{episodes}, Total Reward: {total_reward:.2f}, \"\n",
    "          f\"Portfolio Value: {metrics['final_portfolio_value']:.2f}, \"\n",
    "          f\"Profit/Loss: {metrics['profit_loss']:.2f}, \"\n",
    "          f\"Sharpe: {metrics['sharpe_ratio']:.2f}, \"\n",
    "          f\"Accuracy: {metrics['action_accuracy']:.2f}, \"\n",
    "          f\"Time: {time.time() - start_time:.2f}s\")\n",
    "    if total_reward > best_reward + early_stop_threshold:\n",
    "        best_reward = total_reward\n",
    "        patience_counter = 0\n",
    "        agent.model.save('dqn_model.keras')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Testing phase\n",
    "print(\"\\nTesting...\")\n",
    "state = test_env.reset()\n",
    "total_reward = 0\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    action = agent.act(state, explore=False)\n",
    "    next_state, reward, done, info = test_env.step(action)\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "    if done:\n",
    "        break\n",
    "metrics = test_env.get_metrics()\n",
    "print(f\"Test Results: Total Reward: {total_reward:.2f}, \"\n",
    "      f\"Portfolio Value: {metrics['final_portfolio_value']:.2f}, \"\n",
    "      f\"Profit/Loss: {metrics['profit_loss']:.2f}, \"\n",
    "      f\"Sharpe: {metrics['sharpe_ratio']:.2f}, \"\n",
    "      f\"Accuracy: {metrics['action_accuracy']:.2f}, \"\n",
    "      f\"Buy Count: {metrics['buy_count']}, \"\n",
    "      f\"Sell Count: {metrics['sell_count']}, \"\n",
    "      f\"Hold Count: {metrics['hold_count']}, \"\n",
    "      f\"Time: {time.time() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b398bfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data date range: 2010-01-04 00:00:00 to 2025-04-11 00:00:00\n",
      "Raw data shape: (3843, 5)\n",
      "MultiIndex detected, flattening columns...\n",
      "Initial columns: ['Close', 'High', 'Low', 'Open', 'Volume']\n",
      "Data shape after dropna: (3644, 20)\n",
      "Data shape after upsampling: (5290, 20)\n",
      "Feature data shape: (5290, 13)\n",
      "Prices shape: (5290,)\n",
      "Trend labels shape: (5290, 3)\n",
      "Data shape: (4232, 13), Prices shape: (4232,), Labels shape: (4232, 3)\n",
      "Observation space shape: (16,)\n",
      "Data shape: (1058, 13), Prices shape: (1058,), Labels shape: (1058, 3)\n",
      "Observation space shape: (16,)\n",
      "DQN model input shape: (16,)\n",
      "Training...\n",
      "Train Episode 1/200, Total Reward: -50.00, Portfolio Value: 10000.39, Profit/Loss: 0.39, Sharpe: 2.23, Accuracy: 0.38, Time: 5.43s\n",
      "Train Episode 2/200, Total Reward: -50.00, Portfolio Value: 10000.46, Profit/Loss: 0.46, Sharpe: 2.19, Accuracy: 0.46, Time: 5.35s\n",
      "Train Episode 3/200, Total Reward: -49.98, Portfolio Value: 10004.69, Profit/Loss: 4.69, Sharpe: 2.17, Accuracy: 0.51, Time: 5.48s\n",
      "Train Episode 4/200, Total Reward: -49.96, Portfolio Value: 10010.36, Profit/Loss: 10.36, Sharpe: 2.38, Accuracy: 0.44, Time: 5.52s\n",
      "Train Episode 5/200, Total Reward: -49.98, Portfolio Value: 10005.36, Profit/Loss: 5.36, Sharpe: 2.56, Accuracy: 0.83, Time: 5.48s\n",
      "Train Episode 6/200, Total Reward: -49.98, Portfolio Value: 10004.12, Profit/Loss: 4.12, Sharpe: 2.64, Accuracy: 0.77, Time: 5.61s\n",
      "Train Episode 7/200, Total Reward: -49.98, Portfolio Value: 10005.72, Profit/Loss: 5.72, Sharpe: 1.75, Accuracy: 0.71, Time: 5.62s\n",
      "Train Episode 8/200, Total Reward: -49.97, Portfolio Value: 10006.65, Profit/Loss: 6.65, Sharpe: 2.09, Accuracy: 0.35, Time: 6.18s\n",
      "Train Episode 9/200, Total Reward: -49.93, Portfolio Value: 10015.70, Profit/Loss: 15.70, Sharpe: 2.55, Accuracy: 0.34, Time: 6.26s\n",
      "Train Episode 10/200, Total Reward: -49.94, Portfolio Value: 10016.60, Profit/Loss: 16.60, Sharpe: 1.69, Accuracy: 0.32, Time: 5.55s\n",
      "Train Episode 11/200, Total Reward: -49.95, Portfolio Value: 10012.46, Profit/Loss: 12.46, Sharpe: 1.65, Accuracy: 0.17, Time: 5.47s\n",
      "Train Episode 12/200, Total Reward: -49.96, Portfolio Value: 10011.72, Profit/Loss: 11.72, Sharpe: 1.75, Accuracy: 0.44, Time: 5.56s\n",
      "Train Episode 13/200, Total Reward: -49.97, Portfolio Value: 10008.46, Profit/Loss: 8.46, Sharpe: 1.57, Accuracy: 0.14, Time: 6.37s\n",
      "Train Episode 14/200, Total Reward: -49.97, Portfolio Value: 10007.35, Profit/Loss: 7.35, Sharpe: 1.54, Accuracy: 0.37, Time: 5.75s\n",
      "Train Episode 15/200, Total Reward: -49.97, Portfolio Value: 10006.87, Profit/Loss: 6.87, Sharpe: 1.75, Accuracy: 0.21, Time: 5.56s\n",
      "Train Episode 16/200, Total Reward: -49.98, Portfolio Value: 10005.79, Profit/Loss: 5.79, Sharpe: 1.85, Accuracy: 0.27, Time: 5.50s\n",
      "Train Episode 17/200, Total Reward: -49.98, Portfolio Value: 10005.00, Profit/Loss: 5.00, Sharpe: 1.62, Accuracy: 0.36, Time: 5.50s\n",
      "Train Episode 18/200, Total Reward: -49.98, Portfolio Value: 10004.31, Profit/Loss: 4.31, Sharpe: 1.91, Accuracy: 0.17, Time: 5.50s\n",
      "Train Episode 19/200, Total Reward: -49.99, Portfolio Value: 10003.42, Profit/Loss: 3.42, Sharpe: 1.92, Accuracy: 0.40, Time: 5.50s\n",
      "Train Episode 20/200, Total Reward: -49.99, Portfolio Value: 10002.40, Profit/Loss: 2.40, Sharpe: 1.75, Accuracy: 0.10, Time: 5.54s\n",
      "Train Episode 21/200, Total Reward: -49.99, Portfolio Value: 10001.73, Profit/Loss: 1.73, Sharpe: 1.86, Accuracy: 0.42, Time: 5.87s\n",
      "Train Episode 22/200, Total Reward: -50.00, Portfolio Value: 10001.11, Profit/Loss: 1.11, Sharpe: 1.68, Accuracy: 0.16, Time: 5.75s\n",
      "Train Episode 23/200, Total Reward: -50.00, Portfolio Value: 10000.60, Profit/Loss: 0.60, Sharpe: 1.68, Accuracy: 0.05, Time: 5.63s\n",
      "Train Episode 24/200, Total Reward: -50.00, Portfolio Value: 10000.35, Profit/Loss: 0.35, Sharpe: 1.48, Accuracy: 0.54, Time: 5.63s\n",
      "Train Episode 25/200, Total Reward: -50.00, Portfolio Value: 10000.08, Profit/Loss: 0.08, Sharpe: 1.41, Accuracy: 0.00, Time: 5.63s\n",
      "Train Episode 26/200, Total Reward: -50.00, Portfolio Value: 10000.00, Profit/Loss: 0.00, Sharpe: 0.69, Accuracy: 0.48, Time: 5.65s\n",
      "Train Episode 27/200, Total Reward: -50.00, Portfolio Value: 10000.11, Profit/Loss: 0.11, Sharpe: 2.84, Accuracy: 0.55, Time: 5.68s\n",
      "Train Episode 28/200, Total Reward: -50.00, Portfolio Value: 10000.06, Profit/Loss: 0.06, Sharpe: 1.15, Accuracy: 0.00, Time: 5.79s\n",
      "Train Episode 29/200, Total Reward: -50.00, Portfolio Value: 10000.04, Profit/Loss: 0.04, Sharpe: 1.79, Accuracy: 0.51, Time: 5.72s\n",
      "Early stopping triggered.\n",
      "\n",
      "Testing...\n",
      "Test Results: Total Reward: -50.00, Portfolio Value: 10000.84, Profit/Loss: 0.84, Sharpe: 2.13, Accuracy: 0.95, Buy Count: 5, Sell Count: 0, Hold Count: 995, Time: 0.93s\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 1058 but corresponding boolean dimension is 1000",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 434\u001b[39m\n\u001b[32m    432\u001b[39m \u001b[38;5;66;03m# Plot prices with buy/sell signals\u001b[39;00m\n\u001b[32m    433\u001b[39m test_indices = np.arange(\u001b[38;5;28mlen\u001b[39m(y_test))\n\u001b[32m--> \u001b[39m\u001b[32m434\u001b[39m \u001b[43mplot_prices\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpredicted_labels\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_indices\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 215\u001b[39m, in \u001b[36mplot_prices\u001b[39m\u001b[34m(prices, actions, indices)\u001b[39m\n\u001b[32m    212\u001b[39m plt.plot(indices, prices, label=\u001b[33m'\u001b[39m\u001b[33mNormalized Price (Target)\u001b[39m\u001b[33m'\u001b[39m, color=\u001b[33m'\u001b[39m\u001b[33mblue\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    214\u001b[39m \u001b[38;5;66;03m# Overlay buy/sell signals\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m buy_signals = \u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    216\u001b[39m sell_signals = indices[actions == \u001b[32m1\u001b[39m]\n\u001b[32m    217\u001b[39m buy_prices = prices[actions == \u001b[32m0\u001b[39m]\n",
      "\u001b[31mIndexError\u001b[39m: boolean index did not match indexed array along dimension 0; dimension is 1058 but corresponding boolean dimension is 1000"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "import yfinance as yf\n",
    "from ta.volatility import BollingerBands\n",
    "from pykalman import KalmanFilter\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# TradingEnvironment\n",
    "class TradingEnvironment(gym.Env):\n",
    "    def __init__(self, data, features, prices, trend_labels, initial_cash=10000, max_steps=1000):\n",
    "        super(TradingEnvironment, self).__init__()\n",
    "        self.data = data\n",
    "        self.features = features\n",
    "        self.prices = prices\n",
    "        self.trend_labels = np.array(trend_labels)  # One-hot labels\n",
    "        self.initial_cash = initial_cash\n",
    "        self.max_steps = max_steps\n",
    "        self.current_step = 0\n",
    "        self.cash = initial_cash\n",
    "        self.holdings = 0\n",
    "        self.portfolio_values = []\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf,\n",
    "            shape=(len(features) + 3,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        self.feature_means = np.mean(data, axis=0)\n",
    "        self.feature_stds = np.std(data, axis=0) + 1e-6\n",
    "        self.normalized_data = (data - self.feature_means) / self.feature_stds\n",
    "        self.actions = []\n",
    "        self.buy_count = 0\n",
    "        self.sell_count = 0\n",
    "        self.hold_count = 0\n",
    "        self.prev_action = 2\n",
    "        print(f\"Data shape: {self.data.shape}, Prices shape: {self.prices.shape}, Labels shape: {self.trend_labels.shape}\")\n",
    "        print(f\"Observation space shape: {self.observation_space.shape}\")\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.cash = self.initial_cash\n",
    "        self.holdings = 0\n",
    "        self.portfolio_values = []\n",
    "        self.actions = []\n",
    "        self.buy_count = 0\n",
    "        self.sell_count = 0\n",
    "        self.hold_count = 0\n",
    "        self.prev_action = 2\n",
    "        return self._get_observation()\n",
    "\n",
    "    def _get_observation(self):\n",
    "        if self.current_step >= len(self.data):\n",
    "            return np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        features = self.normalized_data[self.current_step]\n",
    "        state = np.concatenate([\n",
    "            features,\n",
    "            [(self.cash / self.initial_cash) - 1],\n",
    "            [self.holdings],\n",
    "            [self.prev_action]\n",
    "        ])\n",
    "        assert state.shape == (len(self.features) + 3,), f\"Invalid state shape: {state.shape}\"\n",
    "        return state.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        current_price = self.prices[self.current_step]\n",
    "        reward = 0\n",
    "        if action == 0 and self.cash >= current_price:\n",
    "            self.holdings += 1\n",
    "            self.cash -= current_price\n",
    "            self.buy_count += 1\n",
    "            reward -= 0.05\n",
    "        elif action == 1 and self.holdings > 0:\n",
    "            self.holdings -= 1\n",
    "            self.cash += current_price\n",
    "            self.sell_count += 1\n",
    "            reward -= 0.05\n",
    "        else:\n",
    "            self.hold_count += 1\n",
    "            reward -= 0.05  # Stronger HOLD penalty\n",
    "        self.actions.append(action)\n",
    "        self.prev_action = action\n",
    "        portfolio_value = self.cash + self.holdings * current_price\n",
    "        self.portfolio_values.append(portfolio_value)\n",
    "        if len(self.portfolio_values) > 1:\n",
    "            portfolio_change = (portfolio_value - self.portfolio_values[-2]) / self.initial_cash\n",
    "            reward += portfolio_change * 50  # Increased weight\n",
    "            if len(self.portfolio_values) >= 11:\n",
    "                returns = np.diff(self.portfolio_values[-10:]) / self.portfolio_values[-11:-2]\n",
    "                volatility = np.std(returns)\n",
    "                reward -= volatility * 2  # Reduced penalty\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= min(len(self.data), self.max_steps)\n",
    "        next_state = self._get_observation() if not done else np.zeros_like(self._get_observation())\n",
    "        info = {'portfolio_value': portfolio_value}\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def get_metrics(self):\n",
    "        total_steps = len(self.actions)\n",
    "        predicted_labels = np.array(self.actions)\n",
    "        true_labels = np.argmax(self.trend_labels[:total_steps], axis=1)\n",
    "        accuracy = np.mean(predicted_labels == true_labels) if total_steps > 0 else 0\n",
    "        final_value = self.portfolio_values[-1] if self.portfolio_values else self.initial_cash\n",
    "        sharpe = 0\n",
    "        if len(self.portfolio_values) > 1:\n",
    "            returns = np.diff(self.portfolio_values) / np.array(self.portfolio_values[:-1])\n",
    "            if len(returns) > 0 and np.std(returns) > 0:\n",
    "                sharpe = np.mean(returns) / np.std(returns) * np.sqrt(252)\n",
    "        return {\n",
    "            'final_portfolio_value': final_value,\n",
    "            'profit_loss': final_value - self.initial_cash,\n",
    "            'sharpe_ratio': sharpe,\n",
    "            'buy_count': self.buy_count,\n",
    "            'sell_count': self.sell_count,\n",
    "            'hold_count': self.hold_count,\n",
    "            'action_accuracy': accuracy,\n",
    "            'predicted_labels': predicted_labels,\n",
    "            'true_labels': true_labels\n",
    "        }\n",
    "\n",
    "# DQNAgent with Double DQN\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_shape, action_size, learning_rate=0.0005, gamma=0.99):\n",
    "        self.state_shape = state_shape\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.99\n",
    "        self.learning_rate = learning_rate\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.priorities = deque(maxlen=2000)\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "        print(f\"DQN model input shape: {self.state_shape}\")\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=self.state_shape),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(self.action_size, activation='linear')\n",
    "        ])\n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        priority = abs(reward) + 0.1\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        self.priorities.append(priority)\n",
    "\n",
    "    def act(self, state, explore=True):\n",
    "        if explore and np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.action_size)\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        q_values = self.model.predict_on_batch(state)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        priorities = np.array(self.priorities)\n",
    "        probs = priorities / priorities.sum()\n",
    "        indices = np.random.choice(len(self.memory), batch_size, p=probs)\n",
    "        minibatch = [self.memory[i] for i in indices]\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "        states = np.array(states)\n",
    "        next_states = np.array(next_states)\n",
    "        actions = np.array(actions)\n",
    "        rewards = np.array(rewards)\n",
    "        dones = np.array(dones)\n",
    "        #print(f\"Replay states shape: {states.shape}, next_states shape: {next_states.shape}\")\n",
    "        q_values = self.model.predict_on_batch(states)\n",
    "        next_q_values = self.model.predict_on_batch(next_states)\n",
    "        next_actions = np.argmax(next_q_values, axis=1)\n",
    "        target_q_values = self.target_model.predict_on_batch(next_states)\n",
    "        for i in range(batch_size):\n",
    "            target = rewards[i] if dones[i] else rewards[i] + self.gamma * target_q_values[i, next_actions[i]]\n",
    "            q_values[i, actions[i]] = target\n",
    "        self.model.fit(states, q_values, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Plotting functions\n",
    "def plot_confusion_matrix(true_labels, predicted_labels, classes=['UP', 'DOWN', 'NEUTRAL']):\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_prices(prices, actions, indices):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(indices, prices, label='Normalized Price (Target)', color='blue')\n",
    "    \n",
    "    # Overlay buy/sell signals\n",
    "    buy_signals = indices[actions == 0]\n",
    "    sell_signals = indices[actions == 1]\n",
    "    buy_prices = prices[actions == 0]\n",
    "    sell_prices = prices[actions == 1]\n",
    "    \n",
    "    plt.scatter(buy_signals, buy_prices, color='green', marker='^', label='Buy', alpha=0.6)\n",
    "    plt.scatter(sell_signals, sell_prices, color='red', marker='v', label='Sell', alpha=0.6)\n",
    "    \n",
    "    plt.title('Normalized Price with Buy/Sell Signals')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Normalized Price')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('price_plot.png')\n",
    "    plt.close()\n",
    "\n",
    "# Data preparation\n",
    "data = yf.download('^GSPC', start='2010-01-01', end='2025-04-12')\n",
    "\n",
    "# Validate date range\n",
    "print(f\"Data date range: {data.index[0]} to {data.index[-1]}\")\n",
    "print(f\"Raw data shape: {data.shape}\")\n",
    "\n",
    "# Flatten MultiIndex columns\n",
    "if isinstance(data.columns, pd.MultiIndex):\n",
    "    print(\"MultiIndex detected, flattening columns...\")\n",
    "    data.columns = [col[0] for col in data.columns]\n",
    "print(\"Initial columns:\", data.columns.tolist())\n",
    "\n",
    "# Apply Kalman Filter to smooth Close prices (no shift)\n",
    "kf = KalmanFilter(\n",
    "    transition_matrices=[1],\n",
    "    observation_matrices=[1],\n",
    "    initial_state_mean=data['Close'].iloc[0],\n",
    "    initial_state_covariance=1.0,\n",
    "    observation_covariance=1.0,\n",
    "    transition_covariance=0.1\n",
    ")\n",
    "smoothed_prices, _ = kf.filter(data['Close'].values)\n",
    "data['Target'] = smoothed_prices\n",
    "\n",
    "# Compute indicators\n",
    "data['SMA_crossover'] = data['Close'].rolling(window=200).mean() - data['Close'].rolling(window=50).mean()\n",
    "\n",
    "def average_true_range(df, window=14):\n",
    "    high_low = df['High'] - df['Low']\n",
    "    high_close = abs(df['High'] - df['Close'].shift(1))\n",
    "    low_close = abs(df['Low'] - df['Close'].shift(1))\n",
    "    true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    df['ATR_14'] = true_range.rolling(window).mean()\n",
    "    return df\n",
    "\n",
    "data = average_true_range(data)\n",
    "\n",
    "def on_balance_volume(df):\n",
    "    df['OBV'] = (np.sign(df['Close'].diff()) * df['Volume']).cumsum()\n",
    "    return df\n",
    "\n",
    "data = on_balance_volume(data)\n",
    "\n",
    "def vwap(df):\n",
    "    df['VWAP'] = (df['Close'] * df['Volume']).cumsum() / df['Volume'].cumsum()\n",
    "    return df\n",
    "\n",
    "data = vwap(data)\n",
    "\n",
    "delta = data['Close'].diff()\n",
    "gain = delta.where(delta > 0, 0)\n",
    "loss = -delta.where(delta < 0, 0)\n",
    "avg_gain = gain.rolling(window=14).mean()\n",
    "avg_loss = loss.rolling(window=14).mean()\n",
    "rs = avg_gain / avg_loss\n",
    "data['RSI_14'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "ema_12 = data['Close'].ewm(span=12, adjust=False).mean()\n",
    "ema_26 = data['Close'].ewm(span=26, adjust=False).mean()\n",
    "data['MACD'] = ema_12 - ema_26\n",
    "\n",
    "data['Return'] = data['Close'].pct_change()\n",
    "data['Volatility'] = data['Return'].rolling(window=10).std()\n",
    "\n",
    "bb_indicator = BollingerBands(close=data['Close'], window=20, window_dev=2)\n",
    "data['BB_bandWidth'] = bb_indicator.bollinger_hband() - bb_indicator.bollinger_lband()\n",
    "\n",
    "data['SMA_17'] = data['Close'].rolling(window=17).mean()\n",
    "data['SMA_5'] = data['Close'].rolling(window=5).mean()\n",
    "data['SMA_9'] = data['Close'].rolling(window=9).mean()\n",
    "\n",
    "data['MACD_Signal'] = data['MACD'].ewm(span=9, adjust=False).mean()\n",
    "data['MACD_Histogram'] = data['MACD'] - data['MACD_Signal']\n",
    "\n",
    "# Normalize features and target\n",
    "features = ['SMA_crossover', 'SMA_17', 'SMA_9', 'SMA_5', 'ATR_14', 'OBV',\n",
    "            'VWAP', 'Volume', 'BB_bandWidth', 'MACD', 'RSI_14', 'MACD_Histogram', 'Volatility']\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "data[features] = scaler_X.fit_transform(data[features])\n",
    "data['Target'] = scaler_y.fit_transform(data['Target'].values.reshape(-1, 1))\n",
    "\n",
    "# Create labels\n",
    "def create_labels(y):\n",
    "    labels = np.zeros((len(y), 3))\n",
    "    for i in range(len(y) - 1):\n",
    "        diff = y[i + 1] - y[i]\n",
    "        if diff > 0.006:  # UP\n",
    "            labels[i, 0] = 1\n",
    "        elif diff < -0.006:  # DOWN\n",
    "            labels[i, 1] = 1\n",
    "        else:  # NEUTRAL\n",
    "            labels[i, 2] = 1\n",
    "    labels[-1, 2] = 1\n",
    "    return labels\n",
    "\n",
    "# Clean data\n",
    "data.dropna(inplace=True)\n",
    "print(f\"Data shape after dropna: {data.shape}\")\n",
    "\n",
    "# Upsample to 6915 rows (placeholder, replace with your method)\n",
    "if len(data) < 6915:\n",
    "    data = data.reindex(pd.date_range(start=data.index[0], end=data.index[-1], freq='D'))\n",
    "    data = data.interpolate(method='linear')\n",
    "    data = data.dropna()\n",
    "    print(f\"Data shape after upsampling: {data.shape}\")\n",
    "\n",
    "X = data[features].values\n",
    "y = data['Target'].values\n",
    "y_class = create_labels(y)\n",
    "\n",
    "# Debug: Verify shapes\n",
    "print(\"Feature data shape:\", X.shape)\n",
    "print(\"Prices shape:\", y.shape)\n",
    "print(\"Trend labels shape:\", y_class.shape)\n",
    "\n",
    "# Train-test split\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "y_train_class, y_test_class = y_class[:split], y_class[split:]\n",
    "\n",
    "# Initialize environments\n",
    "train_env = TradingEnvironment(X_train, features, y_train, y_train_class, max_steps=1000)\n",
    "test_env = TradingEnvironment(X_test, features, y_test, y_test_class, max_steps=1000)\n",
    "\n",
    "# Initialize agent\n",
    "agent = DQNAgent(state_shape=train_env.observation_space.shape, action_size=train_env.action_space.n)\n",
    "\n",
    "# Training phase\n",
    "episodes = 200\n",
    "batch_size = 64\n",
    "target_update_freq = 50\n",
    "best_reward = -np.inf\n",
    "patience = 20\n",
    "patience_counter = 0\n",
    "early_stop_threshold = 0.01\n",
    "\n",
    "print(\"Training...\")\n",
    "for e in range(episodes):\n",
    "    state = train_env.reset()\n",
    "    total_reward = 0\n",
    "    start_time = time.time()\n",
    "    step = 0\n",
    "    while True:\n",
    "        action = agent.act(state, explore=True)\n",
    "        next_state, reward, done, info = train_env.step(action)\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        step += 1\n",
    "        if step % 10 == 0:\n",
    "            agent.replay(batch_size)\n",
    "        if step % target_update_freq == 0:\n",
    "            agent.update_target_model()\n",
    "        if done:\n",
    "            break\n",
    "    metrics = train_env.get_metrics()\n",
    "    print(f\"Train Episode {e+1}/{episodes}, Total Reward: {total_reward:.2f}, \"\n",
    "          f\"Portfolio Value: {metrics['final_portfolio_value']:.2f}, \"\n",
    "          f\"Profit/Loss: {metrics['profit_loss']:.2f}, \"\n",
    "          f\"Sharpe: {metrics['sharpe_ratio']:.2f}, \"\n",
    "          f\"Accuracy: {metrics['action_accuracy']:.2f}, \"\n",
    "          f\"Time: {time.time() - start_time:.2f}s\")\n",
    "    if total_reward > best_reward + early_stop_threshold:\n",
    "        best_reward = total_reward\n",
    "        patience_counter = 0\n",
    "        agent.model.save('dqn_model.keras')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Testing phase with plotting\n",
    "print(\"\\nTesting...\")\n",
    "state = test_env.reset()\n",
    "total_reward = 0\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    action = agent.act(state, explore=False)\n",
    "    next_state, reward, done, info = test_env.step(action)\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "    if done:\n",
    "        break\n",
    "metrics = test_env.get_metrics()\n",
    "print(f\"Test Results: Total Reward: {total_reward:.2f}, \"\n",
    "      f\"Portfolio Value: {metrics['final_portfolio_value']:.2f}, \"\n",
    "      f\"Profit/Loss: {metrics['profit_loss']:.2f}, \"\n",
    "      f\"Sharpe: {metrics['sharpe_ratio']:.2f}, \"\n",
    "      f\"Accuracy: {metrics['action_accuracy']:.2f}, \"\n",
    "      f\"Buy Count: {metrics['buy_count']}, \"\n",
    "      f\"Sell Count: {metrics['sell_count']}, \"\n",
    "      f\"Hold Count: {metrics['hold_count']}, \"\n",
    "      f\"Time: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(metrics['true_labels'], metrics['predicted_labels'])\n",
    "\n",
    "# Plot prices with buy/sell signals\n",
    "test_indices = np.arange(len(y_test))\n",
    "plot_prices(y_test, metrics['predicted_labels'], test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "583174ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data date range: 2010-01-04 00:00:00 to 2025-04-11 00:00:00\n",
      "Raw data shape: (3843, 5)\n",
      "MultiIndex detected, flattening columns...\n",
      "Initial columns: ['Close', 'High', 'Low', 'Open', 'Volume']\n",
      "Data shape after dropna: (3644, 20)\n",
      "Data shape after upsampling: (5290, 20)\n",
      "Feature data shape: (5290, 13)\n",
      "Prices shape: (5290,)\n",
      "Trend labels shape: (5290, 3)\n",
      "Class distribution (UP, DOWN, NEUTRAL): [ 222  230 4838]\n",
      "Resampled train feature shape: (12054, 13)\n",
      "Resampled train prices shape: (8464,)\n",
      "Resampled train labels shape: (12054, 3)\n",
      "Resampled class distribution: [4018 4018 4018]\n",
      "Data shape: (12054, 13), Prices shape: (8464,), Labels shape: (12054, 3)\n",
      "Observation space shape: (16,)\n",
      "Data shape: (1058, 13), Prices shape: (1058,), Labels shape: (1058, 3)\n",
      "Observation space shape: (16,)\n",
      "DQN model input shape: (16,)\n",
      "Training...\n",
      "Train Episode 1/200, Total Reward: -658.21, Portfolio Value: 10207.38, Profit/Loss: 207.38, Sharpe: 0.78, Accuracy: 0.53, Time: 47.12s\n",
      "Train Episode 2/200, Total Reward: -515.69, Portfolio Value: 10414.53, Profit/Loss: 414.53, Sharpe: 0.79, Accuracy: 0.36, Time: 46.82s\n",
      "Train Episode 3/200, Total Reward: -484.24, Portfolio Value: 10265.79, Profit/Loss: 265.79, Sharpe: 0.99, Accuracy: 0.29, Time: 46.34s\n",
      "Train Episode 4/200, Total Reward: -492.05, Portfolio Value: 10142.32, Profit/Loss: 142.32, Sharpe: 1.23, Accuracy: 0.29, Time: 47.18s\n",
      "Train Episode 5/200, Total Reward: -493.39, Portfolio Value: 10034.24, Profit/Loss: 34.24, Sharpe: 1.21, Accuracy: 0.31, Time: 47.65s\n",
      "Train Episode 6/200, Total Reward: -495.56, Portfolio Value: 10004.22, Profit/Loss: 4.22, Sharpe: 1.16, Accuracy: 0.31, Time: 47.54s\n",
      "Train Episode 7/200, Total Reward: -535.99, Portfolio Value: 10000.58, Profit/Loss: 0.58, Sharpe: 0.71, Accuracy: 0.24, Time: 46.49s\n",
      "Train Episode 8/200, Total Reward: -549.24, Portfolio Value: 10000.82, Profit/Loss: 0.82, Sharpe: 0.80, Accuracy: 0.26, Time: 47.30s\n",
      "Train Episode 9/200, Total Reward: -558.69, Portfolio Value: 10000.74, Profit/Loss: 0.74, Sharpe: 0.90, Accuracy: 0.27, Time: 48.37s\n",
      "Train Episode 10/200, Total Reward: -594.99, Portfolio Value: 10001.00, Profit/Loss: 1.00, Sharpe: 0.81, Accuracy: 0.25, Time: 53.50s\n",
      "Train Episode 11/200, Total Reward: -600.05, Portfolio Value: 10000.35, Profit/Loss: 0.35, Sharpe: 0.53, Accuracy: 0.23, Time: 50.27s\n",
      "Train Episode 12/200, Total Reward: -588.54, Portfolio Value: 10000.75, Profit/Loss: 0.75, Sharpe: 1.00, Accuracy: 0.28, Time: 49.10s\n",
      "Train Episode 13/200, Total Reward: -645.00, Portfolio Value: 10000.50, Profit/Loss: 0.50, Sharpe: 1.09, Accuracy: 0.20, Time: 58.26s\n",
      "Train Episode 14/200, Total Reward: -640.55, Portfolio Value: 10000.27, Profit/Loss: 0.27, Sharpe: 0.36, Accuracy: 0.25, Time: 63.80s\n",
      "Train Episode 15/200, Total Reward: -671.70, Portfolio Value: 10000.04, Profit/Loss: 0.04, Sharpe: 0.07, Accuracy: 0.21, Time: 50.90s\n",
      "Train Episode 16/200, Total Reward: -635.60, Portfolio Value: 10000.17, Profit/Loss: 0.17, Sharpe: 0.40, Accuracy: 0.18, Time: 52.61s\n",
      "Train Episode 17/200, Total Reward: -659.90, Portfolio Value: 10000.38, Profit/Loss: 0.38, Sharpe: 0.72, Accuracy: 0.21, Time: 47.26s\n",
      "Train Episode 18/200, Total Reward: -646.65, Portfolio Value: 10000.19, Profit/Loss: 0.19, Sharpe: 0.38, Accuracy: 0.14, Time: 49.57s\n",
      "Train Episode 19/200, Total Reward: -623.55, Portfolio Value: 10000.12, Profit/Loss: 0.12, Sharpe: 0.22, Accuracy: 0.21, Time: 55.84s\n",
      "Train Episode 20/200, Total Reward: -620.25, Portfolio Value: 10000.28, Profit/Loss: 0.28, Sharpe: 0.50, Accuracy: 0.20, Time: 46.37s\n",
      "Train Episode 21/200, Total Reward: -567.30, Portfolio Value: 10000.31, Profit/Loss: 0.31, Sharpe: 0.50, Accuracy: 0.21, Time: 45.77s\n",
      "Train Episode 22/200, Total Reward: -529.75, Portfolio Value: 9999.92, Profit/Loss: -0.08, Sharpe: -0.03, Accuracy: 0.25, Time: 48.60s\n",
      "Train Episode 23/200, Total Reward: -540.33, Portfolio Value: 10069.42, Profit/Loss: 69.42, Sharpe: 0.81, Accuracy: 0.38, Time: 55.67s\n",
      "Early stopping triggered.\n",
      "\n",
      "Testing...\n",
      "Test Results: Total Reward: -51.82, Portfolio Value: 10118.40, Profit/Loss: 118.40, Sharpe: 0.87, Accuracy: 0.12, Buy Count: 1058, Sell Count: 0, Hold Count: 0, Time: 1.10s\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "import yfinance as yf\n",
    "from ta.volatility import BollingerBands\n",
    "from pykalman import KalmanFilter\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# TradingEnvironment\n",
    "class TradingEnvironment(gym.Env):\n",
    "    def __init__(self, data, features, prices, trend_labels, initial_cash=10000, max_steps=1000):\n",
    "        super(TradingEnvironment, self).__init__()\n",
    "        self.data = data\n",
    "        self.features = features\n",
    "        self.prices = prices\n",
    "        self.trend_labels = np.array(trend_labels)  # One-hot labels\n",
    "        self.initial_cash = initial_cash\n",
    "        self.max_steps = max_steps\n",
    "        self.current_step = 0\n",
    "        self.cash = initial_cash\n",
    "        self.holdings = 0\n",
    "        self.portfolio_values = []\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf,\n",
    "            shape=(len(features) + 3,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        self.feature_means = np.mean(data, axis=0)\n",
    "        self.feature_stds = np.std(data, axis=0) + 1e-6\n",
    "        self.normalized_data = (data - self.feature_means) / self.feature_stds\n",
    "        self.actions = []\n",
    "        self.buy_count = 0\n",
    "        self.sell_count = 0\n",
    "        self.hold_count = 0\n",
    "        self.prev_action = 2\n",
    "        print(f\"Data shape: {self.data.shape}, Prices shape: {self.prices.shape}, Labels shape: {self.trend_labels.shape}\")\n",
    "        print(f\"Observation space shape: {self.observation_space.shape}\")\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.cash = self.initial_cash\n",
    "        self.holdings = 0\n",
    "        self.portfolio_values = []\n",
    "        self.actions = []\n",
    "        self.buy_count = 0\n",
    "        self.sell_count = 0\n",
    "        self.hold_count = 0\n",
    "        self.prev_action = 2\n",
    "        return self._get_observation()\n",
    "\n",
    "    def _get_observation(self):\n",
    "        if self.current_step >= len(self.data):\n",
    "            return np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        features = self.normalized_data[self.current_step]\n",
    "        state = np.concatenate([\n",
    "            features,\n",
    "            [(self.cash / self.initial_cash) - 1],\n",
    "            [self.holdings],\n",
    "            [self.prev_action]\n",
    "        ])\n",
    "        assert state.shape == (len(self.features) + 3,), f\"Invalid state shape: {state.shape}\"\n",
    "        return state.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        current_price = self.prices[self.current_step]\n",
    "        reward = 0\n",
    "        if action == 0 and self.cash >= current_price:\n",
    "            self.holdings += 1\n",
    "            self.cash -= current_price\n",
    "            self.buy_count += 1\n",
    "            reward -= 0.05\n",
    "        elif action == 1 and self.holdings > 0:\n",
    "            self.holdings -= 1\n",
    "            self.cash += current_price\n",
    "            self.sell_count += 1\n",
    "            reward -= 0.05\n",
    "        else:\n",
    "            self.hold_count += 1\n",
    "            reward -= 0.1  # Stronger HOLD penalty\n",
    "        self.actions.append(action)\n",
    "        self.prev_action = action\n",
    "        portfolio_value = self.cash + self.holdings * current_price\n",
    "        self.portfolio_values.append(portfolio_value)\n",
    "        if len(self.portfolio_values) > 1:\n",
    "            portfolio_change = (portfolio_value - self.portfolio_values[-2]) / self.initial_cash\n",
    "            reward += portfolio_change * 100  # Increased weight\n",
    "            if len(self.portfolio_values) >= 11:\n",
    "                returns = np.diff(self.portfolio_values[-10:]) / self.portfolio_values[-11:-2]\n",
    "                volatility = np.std(returns)\n",
    "                reward -= volatility * 1  # Reduced penalty\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= min(len(self.data), self.max_steps)\n",
    "        next_state = self._get_observation() if not done else np.zeros_like(self._get_observation())\n",
    "        info = {'portfolio_value': portfolio_value}\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def get_metrics(self):\n",
    "        total_steps = len(self.actions)\n",
    "        predicted_labels = np.array(self.actions)\n",
    "        true_labels = np.argmax(self.trend_labels[:total_steps], axis=1)\n",
    "        accuracy = np.mean(predicted_labels == true_labels) if total_steps > 0 else 0\n",
    "        final_value = self.portfolio_values[-1] if self.portfolio_values else self.initial_cash\n",
    "        sharpe = 0\n",
    "        if len(self.portfolio_values) > 1:\n",
    "            returns = np.diff(self.portfolio_values) / np.array(self.portfolio_values[:-1])\n",
    "            if len(returns) > 0 and np.std(returns) > 0:\n",
    "                sharpe = np.mean(returns) / np.std(returns) * np.sqrt(252)\n",
    "        return {\n",
    "            'final_portfolio_value': final_value,\n",
    "            'profit_loss': final_value - self.initial_cash,\n",
    "            'sharpe_ratio': sharpe,\n",
    "            'buy_count': self.buy_count,\n",
    "            'sell_count': self.sell_count,\n",
    "            'hold_count': self.hold_count,\n",
    "            'action_accuracy': accuracy,\n",
    "            'predicted_labels': predicted_labels,\n",
    "            'true_labels': true_labels\n",
    "        }\n",
    "\n",
    "# DQNAgent with Double DQN\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_shape, action_size, learning_rate=0.0005, gamma=0.99):\n",
    "        self.state_shape = state_shape\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.99\n",
    "        self.learning_rate = learning_rate\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.priorities = deque(maxlen=2000)\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "        print(f\"DQN model input shape: {self.state_shape}\")\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=self.state_shape),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(self.action_size, activation='linear')\n",
    "        ])\n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        priority = abs(reward) + 0.1\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        self.priorities.append(priority)\n",
    "\n",
    "    def act(self, state, explore=True):\n",
    "        if explore and np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.action_size)\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        q_values = self.model.predict_on_batch(state)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        priorities = np.array(self.priorities)\n",
    "        probs = priorities / priorities.sum()\n",
    "        indices = np.random.choice(len(self.memory), batch_size, p=probs)\n",
    "        minibatch = [self.memory[i] for i in indices]\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "        states = np.array(states)\n",
    "        next_states = np.array(next_states)\n",
    "        actions = np.array(actions)\n",
    "        rewards = np.array(rewards)\n",
    "        dones = np.array(dones)\n",
    "        #print(f\"Replay states shape: {states.shape}, next_states shape: {next_states.shape}\")\n",
    "        q_values = self.model.predict_on_batch(states)\n",
    "        next_q_values = self.model.predict_on_batch(next_states)\n",
    "        next_actions = np.argmax(next_q_values, axis=1)\n",
    "        target_q_values = self.target_model.predict_on_batch(next_states)\n",
    "        for i in range(batch_size):\n",
    "            target = rewards[i] if dones[i] else rewards[i] + self.gamma * target_q_values[i, next_actions[i]]\n",
    "            q_values[i, actions[i]] = target\n",
    "        self.model.fit(states, q_values, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Plotting functions\n",
    "def plot_confusion_matrix(true_labels, predicted_labels, classes=['UP', 'DOWN', 'NEUTRAL']):\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_prices(prices, actions, indices):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(indices, prices, label='Normalized Price (Target)', color='blue')\n",
    "    \n",
    "    # Overlay buy/sell signals\n",
    "    buy_signals = indices[actions == 0]\n",
    "    sell_signals = indices[actions == 1]\n",
    "    buy_prices = prices[actions == 0]\n",
    "    sell_prices = prices[actions == 1]\n",
    "    \n",
    "    plt.scatter(buy_signals, buy_prices, color='green', marker='^', label='Buy', alpha=0.6)\n",
    "    plt.scatter(sell_signals, sell_prices, color='red', marker='v', label='Sell', alpha=0.6)\n",
    "    \n",
    "    plt.title('Normalized Price with Buy/Sell Signals')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Normalized Price')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('price_plot.png')\n",
    "    plt.close()\n",
    "\n",
    "# Data preparation\n",
    "data = yf.download('^GSPC', start='2010-01-01', end='2025-04-12')\n",
    "\n",
    "# Validate date range\n",
    "print(f\"Data date range: {data.index[0]} to {data.index[-1]}\")\n",
    "print(f\"Raw data shape: {data.shape}\")\n",
    "\n",
    "# Flatten MultiIndex columns\n",
    "if isinstance(data.columns, pd.MultiIndex):\n",
    "    print(\"MultiIndex detected, flattening columns...\")\n",
    "    data.columns = [col[0] for col in data.columns]\n",
    "print(\"Initial columns:\", data.columns.tolist())\n",
    "\n",
    "# Apply Kalman Filter to smooth Close prices (no shift)\n",
    "kf = KalmanFilter(\n",
    "    transition_matrices=[1],\n",
    "    observation_matrices=[1],\n",
    "    initial_state_mean=data['Close'].iloc[0],\n",
    "    initial_state_covariance=1.0,\n",
    "    observation_covariance=1.0,\n",
    "    transition_covariance=0.1\n",
    ")\n",
    "smoothed_prices, _ = kf.filter(data['Close'].values)\n",
    "data['Target'] = smoothed_prices\n",
    "\n",
    "# Compute indicators\n",
    "data['SMA_crossover'] = data['Close'].rolling(window=200).mean() - data['Close'].rolling(window=50).mean()\n",
    "\n",
    "def average_true_range(df, window=14):\n",
    "    high_low = df['High'] - df['Low']\n",
    "    high_close = abs(df['High'] - df['Close'].shift(1))\n",
    "    low_close = abs(df['Low'] - df['Close'].shift(1))\n",
    "    true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    df['ATR_14'] = true_range.rolling(window).mean()\n",
    "    return df\n",
    "\n",
    "data = average_true_range(data)\n",
    "\n",
    "def on_balance_volume(df):\n",
    "    df['OBV'] = (np.sign(df['Close'].diff()) * df['Volume']).cumsum()\n",
    "    return df\n",
    "\n",
    "data = on_balance_volume(data)\n",
    "\n",
    "def vwap(df):\n",
    "    df['VWAP'] = (df['Close'] * df['Volume']).cumsum() / df['Volume'].cumsum()\n",
    "    return df\n",
    "\n",
    "data = vwap(data)\n",
    "\n",
    "delta = data['Close'].diff()\n",
    "gain = delta.where(delta > 0, 0)\n",
    "loss = -delta.where(delta < 0, 0)\n",
    "avg_gain = gain.rolling(window=14).mean()\n",
    "avg_loss = loss.rolling(window=14).mean()\n",
    "rs = avg_gain / avg_loss\n",
    "data['RSI_14'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "ema_12 = data['Close'].ewm(span=12, adjust=False).mean()\n",
    "ema_26 = data['Close'].ewm(span=26, adjust=False).mean()\n",
    "data['MACD'] = ema_12 - ema_26\n",
    "\n",
    "data['Return'] = data['Close'].pct_change()\n",
    "data['Volatility'] = data['Return'].rolling(window=10).std()\n",
    "\n",
    "bb_indicator = BollingerBands(close=data['Close'], window=20, window_dev=2)\n",
    "data['BB_bandWidth'] = bb_indicator.bollinger_hband() - bb_indicator.bollinger_lband()\n",
    "\n",
    "data['SMA_17'] = data['Close'].rolling(window=17).mean()\n",
    "data['SMA_5'] = data['Close'].rolling(window=5).mean()\n",
    "data['SMA_9'] = data['Close'].rolling(window=9).mean()\n",
    "\n",
    "data['MACD_Signal'] = data['MACD'].ewm(span=9, adjust=False).mean()\n",
    "data['MACD_Histogram'] = data['MACD'] - data['MACD_Signal']\n",
    "\n",
    "# Normalize features and target\n",
    "features = ['SMA_crossover', 'SMA_17', 'SMA_9', 'SMA_5', 'ATR_14', 'OBV',\n",
    "            'VWAP', 'Volume', 'BB_bandWidth', 'MACD', 'RSI_14', 'MACD_Histogram', 'Volatility']\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "data[features] = scaler_X.fit_transform(data[features])\n",
    "data['Target'] = scaler_y.fit_transform(data['Target'].values.reshape(-1, 1))\n",
    "\n",
    "# Create labels\n",
    "def create_labels(y):\n",
    "    labels = np.zeros((len(y), 3))\n",
    "    for i in range(len(y) - 1):\n",
    "        diff = y[i + 1] - y[i]\n",
    "        if diff > 0.003:  # Reduced threshold\n",
    "            labels[i, 0] = 1\n",
    "        elif diff < -0.003:  # Reduced threshold\n",
    "            labels[i, 1] = 1\n",
    "        else:\n",
    "            labels[i, 2] = 1\n",
    "    labels[-1, 2] = 1\n",
    "    return labels\n",
    "\n",
    "# Clean data\n",
    "data.dropna(inplace=True)\n",
    "print(f\"Data shape after dropna: {data.shape}\")\n",
    "\n",
    "# Upsample to 6915 rows (placeholder, replace with your method)\n",
    "if len(data) < 6915:\n",
    "    data = data.reindex(pd.date_range(start=data.index[0], end=data.index[-1], freq='D'))\n",
    "    data = data.interpolate(method='linear')\n",
    "    data = data.dropna()\n",
    "    print(f\"Data shape after upsampling: {data.shape}\")\n",
    "\n",
    "X = data[features].values\n",
    "y = data['Target'].values\n",
    "y_class = create_labels(y)\n",
    "\n",
    "# Debug: Verify shapes and class distribution\n",
    "print(\"Feature data shape:\", X.shape)\n",
    "print(\"Prices shape:\", y.shape)\n",
    "print(\"Trend labels shape:\", y_class.shape)\n",
    "print(\"Class distribution (UP, DOWN, NEUTRAL):\", np.bincount(np.argmax(y_class, axis=1)))\n",
    "\n",
    "# Train-test split\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "y_train_class, y_test_class = y_class[:split], y_class[split:]\n",
    "\n",
    "# Apply SMOTE to training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_class_resampled = smote.fit_resample(X_train, np.argmax(y_train_class, axis=1))\n",
    "y_train_class_resampled = np.eye(3)[y_train_class_resampled]  # Convert back to one-hot\n",
    "# Adjust y_train to match resampled size (repeat or truncate)\n",
    "y_train_resampled = np.repeat(y_train[:len(X_train)], int(len(X_train_resampled)/len(X_train)), axis=0)[:len(X_train_resampled)]\n",
    "\n",
    "# Debug: Verify resampled shapes and class distribution\n",
    "print(\"Resampled train feature shape:\", X_train_resampled.shape)\n",
    "print(\"Resampled train prices shape:\", y_train_resampled.shape)\n",
    "print(\"Resampled train labels shape:\", y_train_class_resampled.shape)\n",
    "print(\"Resampled class distribution:\", np.bincount(np.argmax(y_train_class_resampled, axis=1)))\n",
    "\n",
    "# Initialize environments\n",
    "train_env = TradingEnvironment(X_train_resampled, features, y_train_resampled, y_train_class_resampled, max_steps=len(y_train_resampled))\n",
    "test_env = TradingEnvironment(X_test, features, y_test, y_test_class, max_steps=len(y_test))\n",
    "\n",
    "# Initialize agent\n",
    "agent = DQNAgent(state_shape=train_env.observation_space.shape, action_size=train_env.action_space.n)\n",
    "\n",
    "# Training phase\n",
    "episodes = 200\n",
    "batch_size = 32\n",
    "target_update_freq = 50\n",
    "best_reward = -np.inf\n",
    "patience = 20\n",
    "patience_counter = 0\n",
    "early_stop_threshold = 0.01\n",
    "\n",
    "print(\"Training...\")\n",
    "for e in range(episodes):\n",
    "    state = train_env.reset()\n",
    "    total_reward = 0\n",
    "    start_time = time.time()\n",
    "    step = 0\n",
    "    while True:\n",
    "        action = agent.act(state, explore=True)\n",
    "        next_state, reward, done, info = train_env.step(action)\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        step += 1\n",
    "        if step % 10 == 0:\n",
    "            agent.replay(batch_size)\n",
    "        if step % target_update_freq == 0:\n",
    "            agent.update_target_model()\n",
    "        if done:\n",
    "            break\n",
    "    metrics = train_env.get_metrics()\n",
    "    print(f\"Train Episode {e+1}/{episodes}, Total Reward: {total_reward:.2f}, \"\n",
    "          f\"Portfolio Value: {metrics['final_portfolio_value']:.2f}, \"\n",
    "          f\"Profit/Loss: {metrics['profit_loss']:.2f}, \"\n",
    "          f\"Sharpe: {metrics['sharpe_ratio']:.2f}, \"\n",
    "          f\"Accuracy: {metrics['action_accuracy']:.2f}, \"\n",
    "          f\"Time: {time.time() - start_time:.2f}s\")\n",
    "    if total_reward > best_reward + early_stop_threshold:\n",
    "        best_reward = total_reward\n",
    "        patience_counter = 0\n",
    "        agent.model.save('dqn_model.keras')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Testing phase with plotting\n",
    "print(\"\\nTesting...\")\n",
    "state = test_env.reset()\n",
    "total_reward = 0\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    action = agent.act(state, explore=False)\n",
    "    next_state, reward, done, info = test_env.step(action)\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "    if done:\n",
    "        break\n",
    "metrics = test_env.get_metrics()\n",
    "print(f\"Test Results: Total Reward: {total_reward:.2f}, \"\n",
    "      f\"Portfolio Value: {metrics['final_portfolio_value']:.2f}, \"\n",
    "      f\"Profit/Loss: {metrics['profit_loss']:.2f}, \"\n",
    "      f\"Sharpe: {metrics['sharpe_ratio']:.2f}, \"\n",
    "      f\"Accuracy: {metrics['action_accuracy']:.2f}, \"\n",
    "      f\"Buy Count: {metrics['buy_count']}, \"\n",
    "      f\"Sell Count: {metrics['sell_count']}, \"\n",
    "      f\"Hold Count: {metrics['hold_count']}, \"\n",
    "      f\"Time: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(metrics['true_labels'], metrics['predicted_labels'])\n",
    "\n",
    "# Plot prices with buy/sell signals\n",
    "test_indices = np.arange(len(metrics['predicted_labels']))\n",
    "plot_prices(y_test[:len(metrics['predicted_labels'])], metrics['predicted_labels'], test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c57b795f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  34,   72, 5184])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.bincount(np.argmax(y_class, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db5d5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data date range: 2010-01-04 00:00:00 to 2025-04-11 00:00:00\n",
      "Raw data shape: (3843, 5)\n",
      "MultiIndex detected, flattening columns...\n",
      "Initial columns: ['Close', 'High', 'Low', 'Open', 'Volume']\n",
      "Data shape after dropna: (3644, 23)\n",
      "Data shape after upsampling: (5290, 23)\n",
      "Feature data shape: (5290, 16)\n",
      "Prices shape: (5290,)\n",
      "Trend labels shape: (5290, 3)\n",
      "Class distribution (UP, DOWN, NEUTRAL): [ 222  230 4838]\n",
      "X_train_resampled shape: (12054, 16)\n",
      "y_train_class_resampled shape: (12054, 3)\n",
      "y_train_resampled shape: (12054,)\n",
      "Resampled class distribution: [4018 4018 4018]\n",
      "Data shape: (12054, 16), Prices shape: (12054,), Labels shape: (12054, 3)\n",
      "Observation space shape: (16,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/PPOAgent/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN model input shape: (16,)\n",
      "Training...\n",
      "Train Episode 1/500, Total Reward: -63.59, Portfolio Value: 9993.01, Profit/Loss: -6.99, Sharpe: -0.02, Accuracy: 0.32, Time: 3798.10s\n",
      "Train Episode 2/500, Total Reward: -60.02, Portfolio Value: 9996.75, Profit/Loss: -3.25, Sharpe: -0.02, Accuracy: 0.36, Time: 2911.71s\n",
      "Train Episode 3/500, Total Reward: -62.55, Portfolio Value: 9995.37, Profit/Loss: -4.63, Sharpe: -0.03, Accuracy: 0.31, Time: 3533.84s\n",
      "Train Episode 4/500, Total Reward: -60.16, Portfolio Value: 10004.35, Profit/Loss: 4.35, Sharpe: 0.02, Accuracy: 0.34, Time: 2947.76s\n",
      "Train Episode 5/500, Total Reward: -60.09, Portfolio Value: 10006.01, Profit/Loss: 6.01, Sharpe: 0.05, Accuracy: 0.32, Time: 4465.10s\n",
      "Train Episode 6/500, Total Reward: -61.51, Portfolio Value: 9994.82, Profit/Loss: -5.18, Sharpe: -0.02, Accuracy: 0.33, Time: 7274.88s\n",
      "Train Episode 7/500, Total Reward: -60.02, Portfolio Value: 9994.94, Profit/Loss: -5.06, Sharpe: -0.08, Accuracy: 0.31, Time: 6006.67s\n",
      "Train Episode 8/500, Total Reward: -60.25, Portfolio Value: 9990.97, Profit/Loss: -9.03, Sharpe: -0.04, Accuracy: 0.35, Time: 7119.47s\n",
      "Train Episode 9/500, Total Reward: -61.37, Portfolio Value: 9991.95, Profit/Loss: -8.05, Sharpe: -0.03, Accuracy: 0.32, Time: 2922.48s\n",
      "Train Episode 10/500, Total Reward: -61.37, Portfolio Value: 9998.25, Profit/Loss: -1.75, Sharpe: -0.01, Accuracy: 0.32, Time: 2909.46s\n",
      "Train Episode 11/500, Total Reward: -61.42, Portfolio Value: 9995.96, Profit/Loss: -4.04, Sharpe: -0.02, Accuracy: 0.35, Time: 33747.31s\n",
      "Train Episode 12/500, Total Reward: -61.28, Portfolio Value: 9993.69, Profit/Loss: -6.31, Sharpe: -0.02, Accuracy: 0.36, Time: 3070.33s\n",
      "Train Episode 13/500, Total Reward: -62.23, Portfolio Value: 10003.63, Profit/Loss: 3.63, Sharpe: 0.01, Accuracy: 0.34, Time: 2996.99s\n",
      "Train Episode 14/500, Total Reward: -60.94, Portfolio Value: 9995.77, Profit/Loss: -4.23, Sharpe: -0.02, Accuracy: 0.34, Time: 6476.21s\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import deque\n",
    "import yfinance as yf\n",
    "from ta.volatility import BollingerBands\n",
    "from pykalman import KalmanFilter\n",
    "import pywt\n",
    "import time\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# TradingEnvironment\n",
    "class TradingEnvironment(gym.Env):\n",
    "    def __init__(self, data, features, prices, trend_labels, initial_cash=10000, max_steps=1000):\n",
    "        super(TradingEnvironment, self).__init__()\n",
    "        self.data = data\n",
    "        self.features = features\n",
    "        self.prices = prices\n",
    "        self.trend_labels = np.array(trend_labels)\n",
    "        self.initial_cash = initial_cash\n",
    "        self.max_steps = max_steps\n",
    "        self.current_step = 0\n",
    "        self.cash = initial_cash\n",
    "        self.holdings = 0\n",
    "        self.portfolio_values = []\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf,\n",
    "            shape=(len(features),),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        self.feature_means = np.mean(data, axis=0)\n",
    "        self.feature_stds = np.std(data, axis=0) + 1e-6\n",
    "        self.normalized_data = (data - self.feature_means) / self.feature_stds\n",
    "        self.actions = []\n",
    "        self.buy_count = 0\n",
    "        self.sell_count = 0\n",
    "        self.hold_count = 0\n",
    "        self.prev_action = 2\n",
    "        print(f\"Data shape: {self.data.shape}, Prices shape: {self.prices.shape}, Labels shape: {self.trend_labels.shape}\")\n",
    "        print(f\"Observation space shape: {self.observation_space.shape}\")\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.cash = self.initial_cash\n",
    "        self.holdings = 0\n",
    "        self.portfolio_values = []\n",
    "        self.actions = []\n",
    "        self.buy_count = 0\n",
    "        self.sell_count = 0\n",
    "        self.hold_count = 0\n",
    "        self.prev_action = 2\n",
    "        return self._get_observation()\n",
    "\n",
    "    def _get_observation(self):\n",
    "        if self.current_step >= len(self.data):\n",
    "            return np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        state = self.normalized_data[self.current_step]\n",
    "        return state.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        current_price = self.prices[self.current_step]\n",
    "        reward = 0\n",
    "        if action == 0 and self.cash >= current_price:\n",
    "            self.holdings += 1\n",
    "            self.cash -= current_price\n",
    "            self.buy_count += 1\n",
    "            reward -= 0.05\n",
    "            if self.prev_action == 0:  # Penalize consecutive buys\n",
    "                reward -= 0.1\n",
    "        elif action == 1 and self.holdings > 0:\n",
    "            self.holdings -= 1\n",
    "            self.cash += current_price\n",
    "            self.sell_count += 1\n",
    "            reward -= 0.05\n",
    "        else:\n",
    "            self.hold_count += 1\n",
    "            reward -= 0.05  # Reduced from -0.1\n",
    "        self.actions.append(action)\n",
    "        self.prev_action = action\n",
    "        portfolio_value = self.cash + self.holdings * current_price\n",
    "        self.portfolio_values.append(portfolio_value)\n",
    "        if len(self.portfolio_values) > 1:\n",
    "            portfolio_change = (portfolio_value - self.portfolio_values[-2]) / self.initial_cash\n",
    "            reward += portfolio_change * 100\n",
    "            if len(self.portfolio_values) >= 11:\n",
    "                returns = np.diff(self.portfolio_values[-10:]) / self.portfolio_values[-11:-2]\n",
    "                volatility = np.std(returns)\n",
    "                reward -= volatility * 1\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= min(len(self.data), self.max_steps)\n",
    "        next_state = self._get_observation() if not done else np.zeros_like(self._get_observation())\n",
    "        info = {'portfolio_value': portfolio_value}\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def get_metrics(self):\n",
    "        total_steps = len(self.actions)\n",
    "        predicted_labels = np.array(self.actions)\n",
    "        true_labels = np.argmax(self.trend_labels[:total_steps], axis=1)\n",
    "        accuracy = np.mean(predicted_labels == true_labels) if total_steps > 0 else 0\n",
    "        final_value = self.portfolio_values[-1] if self.portfolio_values else self.initial_cash\n",
    "        sharpe = 0\n",
    "        if len(self.portfolio_values) > 1:\n",
    "            returns = np.diff(self.portfolio_values) / np.array(self.portfolio_values[:-1])\n",
    "            if len(returns) > 0 and np.std(returns) > 0:\n",
    "                sharpe = np.mean(returns) / np.std(returns) * np.sqrt(252)\n",
    "        return {\n",
    "            'final_portfolio_value': final_value,\n",
    "            'profit_loss': final_value - self.initial_cash,\n",
    "            'sharpe_ratio': sharpe,\n",
    "            'buy_count': self.buy_count,\n",
    "            'sell_count': self.sell_count,\n",
    "            'hold_count': self.hold_count,\n",
    "            'action_accuracy': accuracy,\n",
    "            'predicted_labels': predicted_labels,\n",
    "            'true_labels': true_labels\n",
    "        }\n",
    "\n",
    "# DQN Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 150  # Slower decay\n",
    "        self.learning_rate = 0.005\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential([\n",
    "            Dense(128, activation='relu', input_shape=(self.state_size,)),\n",
    "            Dropout(0.2),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(0.2),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(self.action_size, activation='linear')\n",
    "        ])\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.action_size)\n",
    "        state = np.reshape(state, [1, self.state_size])\n",
    "        act_values = self.model.predict(state, verbose=0)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = np.random.choice(len(self.memory), size=batch_size, replace=False)\n",
    "        minibatch = [self.memory[i] for i in minibatch]\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "            next_state = np.reshape(next_state, [1, self.state_size])\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.target_model.predict(next_state, verbose=0)[0])\n",
    "            target_f = self.model.predict(state, verbose=0)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "\n",
    "# Plotting functions\n",
    "def plot_confusion_matrix(true_labels, predicted_labels, classes=['UP', 'DOWN', 'NEUTRAL']):\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.title('Confusion Matrix (DQN)')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_prices(prices, actions, indices):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(indices, prices, label='Normalized Price (Target)', color='blue')\n",
    "    buy_signals = indices[actions == 0]\n",
    "    sell_signals = indices[actions == 1]\n",
    "    buy_prices = prices[actions == 0]\n",
    "    sell_prices = prices[actions == 1]\n",
    "    plt.scatter(buy_signals, buy_prices, color='green', marker='^', label='Buy', alpha=0.6)\n",
    "    plt.scatter(sell_signals, sell_prices, color='red', marker='v', label='Sell', alpha=0.6)\n",
    "    plt.title('Normalized Price with Buy/Sell Signals (DQN)')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Normalized Price')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('price_plot.png')\n",
    "    plt.close()\n",
    "\n",
    "# Data preparation\n",
    "data = yf.download('^GSPC', start='2010-01-01', end='2025-04-12')\n",
    "\n",
    "# Validate date range\n",
    "print(f\"Data date range: {data.index[0]} to {data.index[-1]}\")\n",
    "print(f\"Raw data shape: {data.shape}\")\n",
    "\n",
    "# Flatten MultiIndex columns\n",
    "if isinstance(data.columns, pd.MultiIndex):\n",
    "    print(\"MultiIndex detected, flattening columns...\")\n",
    "    data.columns = [col[0] for col in data.columns]\n",
    "print(\"Initial columns:\", data.columns.tolist())\n",
    "\n",
    "# Apply Kalman Filter to smooth Close prices\n",
    "kf = KalmanFilter(\n",
    "    transition_matrices=[1],\n",
    "    observation_matrices=[1],\n",
    "    initial_state_mean=data['Close'].iloc[0],\n",
    "    initial_state_covariance=1.0,\n",
    "    observation_covariance=1.0,\n",
    "    transition_covariance=0.1\n",
    ")\n",
    "smoothed_prices, _ = kf.filter(data['Close'].values)\n",
    "data['Target'] = smoothed_prices\n",
    "\n",
    "# Compute indicators\n",
    "data['SMA_crossover'] = data['Close'].rolling(window=200).mean() - data['Close'].rolling(window=50).mean()\n",
    "\n",
    "def average_true_range(df, window=14):\n",
    "    high_low = df['High'] - df['Low']\n",
    "    high_close = abs(df['High'] - df['Close'].shift(1))\n",
    "    low_close = abs(df['Low'] - df['Close'].shift(1))\n",
    "    true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    df['ATR_14'] = true_range.rolling(window).mean()\n",
    "    return df\n",
    "\n",
    "data = average_true_range(data)\n",
    "\n",
    "def on_balance_volume(df):\n",
    "    df['OBV'] = (np.sign(df['Close'].diff()) * df['Volume']).cumsum()\n",
    "    return df\n",
    "\n",
    "data = on_balance_volume(data)\n",
    "\n",
    "def vwap(df):\n",
    "    df['VWAP'] = (df['Close'] * df['Volume']).cumsum() / df['Volume'].cumsum()\n",
    "    return df\n",
    "\n",
    "data = vwap(data)\n",
    "\n",
    "delta = data['Close'].diff()\n",
    "gain = delta.where(delta > 0, 0)\n",
    "loss = -delta.where(delta < 0, 0)\n",
    "avg_gain = gain.rolling(window=14).mean()\n",
    "avg_loss = loss.rolling(window=14).mean()\n",
    "rs = avg_gain / avg_loss\n",
    "data['RSI_14'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "ema_12 = data['Close'].ewm(span=12, adjust=False).mean()\n",
    "ema_26 = data['Close'].ewm(span=26, adjust=False).mean()\n",
    "data['MACD'] = ema_12 - ema_26\n",
    "\n",
    "data['Return'] = data['Close'].pct_change()\n",
    "data['Volatility'] = data['Return'].rolling(window=10).std()\n",
    "\n",
    "bb_indicator = BollingerBands(close=data['Close'], window=20, window_dev=2)\n",
    "data['BB_bandWidth'] = bb_indicator.bollinger_hband() - bb_indicator.bollinger_lband()\n",
    "\n",
    "data['SMA_17'] = data['Close'].rolling(window=17).mean()\n",
    "data['SMA_5'] = data['Close'].rolling(window=5).mean()\n",
    "data['SMA_9'] = data['Close'].rolling(window=9).mean()\n",
    "\n",
    "data['MACD_Signal'] = data['MACD'].ewm(span=9, adjust=False).mean()\n",
    "data['MACD_Histogram'] = data['MACD'] - data['MACD_Signal']\n",
    "\n",
    "# Wavelet features (3 features for DQN)\n",
    "def compute_wavelet_features(prices, wavelet='db4', levels=2):\n",
    "    coeffs = pywt.wavedec(prices, wavelet, level=levels, mode='smooth')\n",
    "    cA2, cD2, cD1 = coeffs[0], coeffs[1], coeffs[2]\n",
    "    min_len = min(len(cA2), len(cD2), len(cD1), len(prices))\n",
    "    return [np.pad(c[:min_len], (0, len(prices) - min_len), mode='constant') for c in [cA2, cD2, cD1]]\n",
    "\n",
    "wavelet_features = compute_wavelet_features(data['Target'].values)\n",
    "for i, name in enumerate(['Wavelet_cA2', 'Wavelet_cD2', 'Wavelet_cD1']):\n",
    "    data[name] = wavelet_features[i]\n",
    "\n",
    "# Normalize features and target\n",
    "features = ['SMA_crossover', 'SMA_17', 'SMA_9', 'SMA_5', 'ATR_14', 'OBV',\n",
    "            'VWAP', 'Volume', 'BB_bandWidth', 'MACD', 'RSI_14', 'MACD_Histogram', 'Volatility',\n",
    "            'Wavelet_cA2', 'Wavelet_cD2', 'Wavelet_cD1']\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "data[features] = scaler_X.fit_transform(data[features])\n",
    "data['Target'] = scaler_y.fit_transform(data['Target'].values.reshape(-1, 1))\n",
    "\n",
    "# Create labels\n",
    "def create_labels(y):\n",
    "    labels = np.zeros((len(y), 3))\n",
    "    for i in range(len(y) - 1):\n",
    "        diff = y[i + 1] - y[i]\n",
    "        if diff > 0.003:\n",
    "            labels[i, 0] = 1\n",
    "        elif diff < -0.003:\n",
    "            labels[i, 1] = 1\n",
    "        else:\n",
    "            labels[i, 2] = 1\n",
    "    labels[-1, 2] = 1\n",
    "    return labels\n",
    "\n",
    "# Clean data\n",
    "data.dropna(inplace=True)\n",
    "print(f\"Data shape after dropna: {data.shape}\")\n",
    "\n",
    "# Upsample to 6915 rows (placeholder)\n",
    "if len(data) < 6915:\n",
    "    data = data.reindex(pd.date_range(start=data.index[0], end=data.index[-1], freq='D'))\n",
    "    data = data.interpolate(method='linear')\n",
    "    data = data.dropna()\n",
    "    print(f\"Data shape after upsampling: {data.shape}\")\n",
    "\n",
    "X = data[features].values\n",
    "y = data['Target'].values\n",
    "y_class = create_labels(y)\n",
    "\n",
    "# Debug: Verify shapes and class distribution\n",
    "print(\"Feature data shape:\", X.shape)\n",
    "print(\"Prices shape:\", y.shape)\n",
    "print(\"Trend labels shape:\", y_class.shape)\n",
    "print(\"Class distribution (UP, DOWN, NEUTRAL):\", np.bincount(np.argmax(y_class, axis=1)))\n",
    "\n",
    "# Train-test split\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "y_train_class, y_test_class = y_class[:split], y_class[split:]\n",
    "\n",
    "# Apply SMOTE to training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_flat = X_train\n",
    "y_train_class_flat = np.argmax(y_train_class, axis=1)\n",
    "X_train_resampled, y_train_class_resampled = smote.fit_resample(X_train_flat, y_train_class_flat)\n",
    "y_train_class_resampled = np.eye(3)[y_train_class_resampled]\n",
    "n_samples = len(X_train_resampled)\n",
    "indices = np.arange(len(y_train))\n",
    "sampled_indices = np.random.choice(indices, size=n_samples, replace=True)\n",
    "y_train_resampled = y_train[sampled_indices]\n",
    "\n",
    "# Debug: Verify resampled shapes\n",
    "print(\"X_train_resampled shape:\", X_train_resampled.shape)\n",
    "print(\"y_train_class_resampled shape:\", y_train_class_resampled.shape)\n",
    "print(\"y_train_resampled shape:\", y_train_resampled.shape)\n",
    "print(\"Resampled class distribution:\", np.bincount(np.argmax(y_train_class_resampled, axis=1)))\n",
    "\n",
    "# Initialize environment and agent\n",
    "env = TradingEnvironment(X_train_resampled, features, y_train_resampled, y_train_class_resampled)\n",
    "agent = DQNAgent(state_size=len(features), action_size=3)\n",
    "print(f\"DQN model input shape: {len(features),}\")\n",
    "\n",
    "# Training phase\n",
    "print(\"Training...\")\n",
    "batch_size = 32\n",
    "no_improvement = 0\n",
    "best_profit = -float('inf')\n",
    "for episode in range(500):  # Increased episodes\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    start_time = time.time()\n",
    "    for step in range(len(X_train_resampled)):\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)\n",
    "    agent.update_target_model()\n",
    "    metrics = env.get_metrics()\n",
    "    print(f\"Train Episode {episode+1}/500, Total Reward: {total_reward:.2f}, \"\n",
    "          f\"Portfolio Value: {metrics['final_portfolio_value']:.2f}, \"\n",
    "          f\"Profit/Loss: {metrics['profit_loss']:.2f}, \"\n",
    "          f\"Sharpe: {metrics['sharpe_ratio']:.2f}, \"\n",
    "          f\"Accuracy: {metrics['action_accuracy']:.2f}, \"\n",
    "          f\"Time: {time.time() - start_time:.2f}s\")\n",
    "    if metrics['profit_loss'] > best_profit:\n",
    "        best_profit = metrics['profit_loss']\n",
    "        no_improvement = 0\n",
    "    else:\n",
    "        no_improvement += 1\n",
    "    if no_improvement > 20:  # Increased patience\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "    agent.epsilon = max(agent.epsilon_min, 1.0 - episode / agent.epsilon_decay)  # Slower decay\n",
    "\n",
    "# Testing phase\n",
    "print(\"\\nTesting...\")\n",
    "test_env = TradingEnvironment(X_test, features, y_test, y_test_class, max_steps=len(y_test))\n",
    "test_rewards = []\n",
    "test_metrics = []\n",
    "for _ in range(10):  # Multiple test episodes\n",
    "    state = test_env.reset()\n",
    "    total_reward = 0\n",
    "    start_time = time.time()\n",
    "    for step in range(len(X_test)):\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, info = test_env.step(action)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    metrics = test_env.get_metrics()\n",
    "    test_rewards.append(total_reward)\n",
    "    test_metrics.append(metrics)\n",
    "avg_reward = np.mean(test_rewards)\n",
    "avg_metrics = {\n",
    "    'profit_loss': np.mean([m['profit_loss'] for m in test_metrics]),\n",
    "    'sharpe_ratio': np.mean([m['sharpe_ratio'] for m in test_metrics]),\n",
    "    'action_accuracy': np.mean([m['action_accuracy'] for m in test_metrics]),\n",
    "    'buy_count': np.mean([m['buy_count'] for m in test_metrics]),\n",
    "    'sell_count': np.mean([m['sell_count'] for m in test_metrics]),\n",
    "    'hold_count': np.mean([m['hold_count'] for m in test_metrics]),\n",
    "    'final_portfolio_value': np.mean([m['final_portfolio_value'] for m in test_metrics])\n",
    "}\n",
    "print(f\"Average Test Results: Total Reward: {avg_reward:.2f}, \"\n",
    "      f\"Portfolio Value: {avg_metrics['final_portfolio_value']:.2f}, \"\n",
    "      f\"Profit/Loss: {avg_metrics['profit_loss']:.2f}, \"\n",
    "      f\"Sharpe: {avg_metrics['sharpe_ratio']:.2f}, \"\n",
    "      f\"Accuracy: {avg_metrics['action_accuracy']:.2f}, \"\n",
    "      f\"Buy Count: {avg_metrics['buy_count']:.2f}, \"\n",
    "      f\"Sell Count: {avg_metrics['sell_count']:.2f}, \"\n",
    "      f\"Hold Count: {avg_metrics['hold_count']:.2f}, \"\n",
    "      f\"Time: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "# Plot confusion matrix and prices for the last test episode\n",
    "plot_confusion_matrix(metrics['true_labels'], metrics['predicted_labels'])\n",
    "plot_prices(y_test[:len(metrics['predicted_labels'])], metrics['predicted_labels'], np.arange(len(metrics['predicted_labels'])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PPOAgent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
