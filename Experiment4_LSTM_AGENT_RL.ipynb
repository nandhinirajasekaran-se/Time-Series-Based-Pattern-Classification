{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dcb83a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data date range: 2010-01-04 00:00:00 to 2025-04-11 00:00:00\n",
      "Raw data shape: (3843, 5)\n",
      "MultiIndex detected, flattening columns...\n",
      "Initial columns: ['Close', 'High', 'Low', 'Open', 'Volume']\n",
      "Data shape after dropna: (3644, 25)\n",
      "Data shape after upsampling: (5290, 25)\n",
      "Feature data shape: (5290, 18)\n",
      "Prices shape: (5290,)\n",
      "Trend labels shape: (5290, 3)\n",
      "Class distribution (UP, DOWN, NEUTRAL): [ 222  230 4838]\n",
      "X_train_resampled shape: (12054, 18)\n",
      "y_train_class_resampled shape: (12054, 3)\n",
      "y_train_resampled shape: (12054,)\n",
      "Train sequence shape: (12044, 10, 18)\n",
      "Train labels shape: (12044, 3)\n",
      "Train prices shape: (12044,)\n",
      "Test sequence shape: (1048, 10, 18)\n",
      "Test labels shape: (1048, 3)\n",
      "Test prices shape: (1048,)\n",
      "Resampled class distribution: [4018 4018 4008]\n",
      "Creating LSTM model with units: 64 and dropout rate: 0.3 for sequence length: 10 and num features: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 13:20:04.827435: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2025-04-20 13:20:04.827637: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2025-04-20 13:20:04.827995: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2025-04-20 13:20:04.828302: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-04-20 13:20:04.829038: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 13:20:06.952639: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 136ms/step - class_accuracy: 0.5840 - class_loss: 0.0557 - loss: 0.3265 - price_loss: 0.2707 - price_mae: 0.1569 - val_class_accuracy: 0.9149 - val_class_loss: 0.0431 - val_loss: 0.3342 - val_price_loss: 0.2904 - val_price_mae: 0.1745 - learning_rate: 5.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 126ms/step - class_accuracy: 0.8690 - class_loss: 0.0335 - loss: 0.2734 - price_loss: 0.2399 - price_mae: 0.1492 - val_class_accuracy: 0.9099 - val_class_loss: 0.0320 - val_loss: 0.2716 - val_price_loss: 0.2385 - val_price_mae: 0.1480 - learning_rate: 5.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 129ms/step - class_accuracy: 0.9114 - class_loss: 0.0252 - loss: 0.2714 - price_loss: 0.2462 - price_mae: 0.1506 - val_class_accuracy: 0.9103 - val_class_loss: 0.0314 - val_loss: 0.2847 - val_price_loss: 0.2521 - val_price_mae: 0.1571 - learning_rate: 5.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 130ms/step - class_accuracy: 0.9153 - class_loss: 0.0233 - loss: 0.2497 - price_loss: 0.2264 - price_mae: 0.1440 - val_class_accuracy: 0.9983 - val_class_loss: 0.0093 - val_loss: 0.2614 - val_price_loss: 0.2506 - val_price_mae: 0.1570 - learning_rate: 5.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 130ms/step - class_accuracy: 0.9278 - class_loss: 0.0198 - loss: 0.2543 - price_loss: 0.2346 - price_mae: 0.1472 - val_class_accuracy: 0.9884 - val_class_loss: 0.0146 - val_loss: 0.2523 - val_price_loss: 0.2355 - val_price_mae: 0.1480 - learning_rate: 5.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 130ms/step - class_accuracy: 0.9405 - class_loss: 0.0181 - loss: 0.2469 - price_loss: 0.2288 - price_mae: 0.1457 - val_class_accuracy: 0.9975 - val_class_loss: 0.0110 - val_loss: 0.2591 - val_price_loss: 0.2467 - val_price_mae: 0.1533 - learning_rate: 5.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 129ms/step - class_accuracy: 0.9409 - class_loss: 0.0163 - loss: 0.2447 - price_loss: 0.2284 - price_mae: 0.1454 - val_class_accuracy: 0.9954 - val_class_loss: 0.0116 - val_loss: 0.2536 - val_price_loss: 0.2404 - val_price_mae: 0.1499 - learning_rate: 5.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 129ms/step - class_accuracy: 0.9462 - class_loss: 0.0156 - loss: 0.2366 - price_loss: 0.2210 - price_mae: 0.1431 - val_class_accuracy: 0.9718 - val_class_loss: 0.0123 - val_loss: 0.2547 - val_price_loss: 0.2409 - val_price_mae: 0.1525 - learning_rate: 5.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 130ms/step - class_accuracy: 0.9424 - class_loss: 0.0150 - loss: 0.2424 - price_loss: 0.2274 - price_mae: 0.1439 - val_class_accuracy: 0.9950 - val_class_loss: 0.0064 - val_loss: 0.2556 - val_price_loss: 0.2479 - val_price_mae: 0.1541 - learning_rate: 5.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 131ms/step - class_accuracy: 0.9433 - class_loss: 0.0143 - loss: 0.2446 - price_loss: 0.2303 - price_mae: 0.1457 - val_class_accuracy: 0.9797 - val_class_loss: 0.0122 - val_loss: 0.2530 - val_price_loss: 0.2392 - val_price_mae: 0.1499 - learning_rate: 5.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 131ms/step - class_accuracy: 0.9570 - class_loss: 0.0117 - loss: 0.2344 - price_loss: 0.2227 - price_mae: 0.1420 - val_class_accuracy: 0.9942 - val_class_loss: 0.0084 - val_loss: 0.2468 - val_price_loss: 0.2367 - val_price_mae: 0.1481 - learning_rate: 2.5000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 131ms/step - class_accuracy: 0.9547 - class_loss: 0.0118 - loss: 0.2394 - price_loss: 0.2275 - price_mae: 0.1445 - val_class_accuracy: 0.9909 - val_class_loss: 0.0074 - val_loss: 0.2471 - val_price_loss: 0.2378 - val_price_mae: 0.1480 - learning_rate: 2.5000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 131ms/step - class_accuracy: 0.9603 - class_loss: 0.0111 - loss: 0.2384 - price_loss: 0.2273 - price_mae: 0.1442 - val_class_accuracy: 0.9763 - val_class_loss: 0.0093 - val_loss: 0.2485 - val_price_loss: 0.2374 - val_price_mae: 0.1491 - learning_rate: 2.5000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 150ms/step - class_accuracy: 0.9611 - class_loss: 0.0104 - loss: 0.2328 - price_loss: 0.2224 - price_mae: 0.1412 - val_class_accuracy: 0.9992 - val_class_loss: 0.0026 - val_loss: 0.2457 - val_price_loss: 0.2417 - val_price_mae: 0.1517 - learning_rate: 2.5000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 138ms/step - class_accuracy: 0.9603 - class_loss: 0.0096 - loss: 0.2370 - price_loss: 0.2275 - price_mae: 0.1441 - val_class_accuracy: 0.9938 - val_class_loss: 0.0038 - val_loss: 0.2427 - val_price_loss: 0.2374 - val_price_mae: 0.1480 - learning_rate: 2.5000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 139ms/step - class_accuracy: 0.9634 - class_loss: 0.0087 - loss: 0.2337 - price_loss: 0.2251 - price_mae: 0.1443 - val_class_accuracy: 0.9958 - val_class_loss: 0.0038 - val_loss: 0.2448 - val_price_loss: 0.2395 - val_price_mae: 0.1503 - learning_rate: 2.5000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 156ms/step - class_accuracy: 0.9627 - class_loss: 0.0092 - loss: 0.2358 - price_loss: 0.2266 - price_mae: 0.1427 - val_class_accuracy: 0.9685 - val_class_loss: 0.0111 - val_loss: 0.2528 - val_price_loss: 0.2399 - val_price_mae: 0.1501 - learning_rate: 2.5000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 195ms/step - class_accuracy: 0.9682 - class_loss: 0.0080 - loss: 0.2253 - price_loss: 0.2173 - price_mae: 0.1405 - val_class_accuracy: 0.9983 - val_class_loss: 0.0021 - val_loss: 0.2422 - val_price_loss: 0.2386 - val_price_mae: 0.1479 - learning_rate: 2.5000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 136ms/step - class_accuracy: 0.9683 - class_loss: 0.0073 - loss: 0.2303 - price_loss: 0.2230 - price_mae: 0.1422 - val_class_accuracy: 0.9958 - val_class_loss: 0.0035 - val_loss: 0.2436 - val_price_loss: 0.2389 - val_price_mae: 0.1480 - learning_rate: 2.5000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 142ms/step - class_accuracy: 0.9633 - class_loss: 0.0079 - loss: 0.2284 - price_loss: 0.2205 - price_mae: 0.1418 - val_class_accuracy: 0.9988 - val_class_loss: 7.8366e-04 - val_loss: 0.2405 - val_price_loss: 0.2383 - val_price_mae: 0.1480 - learning_rate: 2.5000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 147ms/step - class_accuracy: 0.9675 - class_loss: 0.0073 - loss: 0.2287 - price_loss: 0.2213 - price_mae: 0.1418 - val_class_accuracy: 0.9967 - val_class_loss: 0.0019 - val_loss: 0.2406 - val_price_loss: 0.2372 - val_price_mae: 0.1480 - learning_rate: 2.5000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 131ms/step - class_accuracy: 0.9709 - class_loss: 0.0068 - loss: 0.2273 - price_loss: 0.2205 - price_mae: 0.1426 - val_class_accuracy: 0.9983 - val_class_loss: 0.0012 - val_loss: 0.2396 - val_price_loss: 0.2370 - val_price_mae: 0.1479 - learning_rate: 2.5000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 138ms/step - class_accuracy: 0.9696 - class_loss: 0.0070 - loss: 0.2332 - price_loss: 0.2262 - price_mae: 0.1440 - val_class_accuracy: 0.9983 - val_class_loss: 0.0011 - val_loss: 0.2442 - val_price_loss: 0.2414 - val_price_mae: 0.1506 - learning_rate: 2.5000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 153ms/step - class_accuracy: 0.9594 - class_loss: 0.0085 - loss: 0.2293 - price_loss: 0.2208 - price_mae: 0.1427 - val_class_accuracy: 0.9942 - val_class_loss: 0.0029 - val_loss: 0.2426 - val_price_loss: 0.2381 - val_price_mae: 0.1483 - learning_rate: 2.5000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 140ms/step - class_accuracy: 0.9709 - class_loss: 0.0066 - loss: 0.2287 - price_loss: 0.2221 - price_mae: 0.1419 - val_class_accuracy: 0.9963 - val_class_loss: 0.0018 - val_loss: 0.2414 - val_price_loss: 0.2381 - val_price_mae: 0.1487 - learning_rate: 2.5000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 128ms/step - class_accuracy: 0.9698 - class_loss: 0.0073 - loss: 0.2284 - price_loss: 0.2212 - price_mae: 0.1416 - val_class_accuracy: 0.9992 - val_class_loss: 0.0012 - val_loss: 0.2426 - val_price_loss: 0.2397 - val_price_mae: 0.1497 - learning_rate: 2.5000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 127ms/step - class_accuracy: 0.9701 - class_loss: 0.0073 - loss: 0.2317 - price_loss: 0.2245 - price_mae: 0.1439 - val_class_accuracy: 0.9963 - val_class_loss: 0.0014 - val_loss: 0.2441 - val_price_loss: 0.2411 - val_price_mae: 0.1505 - learning_rate: 2.5000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 136ms/step - class_accuracy: 0.9743 - class_loss: 0.0064 - loss: 0.2301 - price_loss: 0.2238 - price_mae: 0.1431 - val_class_accuracy: 0.9988 - val_class_loss: 0.0011 - val_loss: 0.2432 - val_price_loss: 0.2404 - val_price_mae: 0.1494 - learning_rate: 1.2500e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 159ms/step - class_accuracy: 0.9719 - class_loss: 0.0067 - loss: 0.2290 - price_loss: 0.2222 - price_mae: 0.1424 - val_class_accuracy: 0.9992 - val_class_loss: 5.8768e-04 - val_loss: 0.2407 - val_price_loss: 0.2387 - val_price_mae: 0.1479 - learning_rate: 1.2500e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 124ms/step - class_accuracy: 0.9754 - class_loss: 0.0059 - loss: 0.2305 - price_loss: 0.2246 - price_mae: 0.1433 - val_class_accuracy: 0.9971 - val_class_loss: 0.0015 - val_loss: 0.2406 - val_price_loss: 0.2378 - val_price_mae: 0.1481 - learning_rate: 1.2500e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 121ms/step - class_accuracy: 0.9708 - class_loss: 0.0062 - loss: 0.2298 - price_loss: 0.2236 - price_mae: 0.1434 - val_class_accuracy: 0.9992 - val_class_loss: 8.2840e-04 - val_loss: 0.2396 - val_price_loss: 0.2374 - val_price_mae: 0.1479 - learning_rate: 1.2500e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 130ms/step - class_accuracy: 0.9724 - class_loss: 0.0060 - loss: 0.2362 - price_loss: 0.2302 - price_mae: 0.1453 - val_class_accuracy: 0.9979 - val_class_loss: 9.2987e-04 - val_loss: 0.2404 - val_price_loss: 0.2380 - val_price_mae: 0.1484 - learning_rate: 1.2500e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 152ms/step - class_accuracy: 0.9772 - class_loss: 0.0055 - loss: 0.2243 - price_loss: 0.2188 - price_mae: 0.1411 - val_class_accuracy: 0.9975 - val_class_loss: 0.0013 - val_loss: 0.2405 - val_price_loss: 0.2379 - val_price_mae: 0.1483 - learning_rate: 6.2500e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 146ms/step - class_accuracy: 0.9755 - class_loss: 0.0060 - loss: 0.2251 - price_loss: 0.2191 - price_mae: 0.1407 - val_class_accuracy: 0.9988 - val_class_loss: 8.8675e-04 - val_loss: 0.2396 - val_price_loss: 0.2373 - val_price_mae: 0.1480 - learning_rate: 6.2500e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 128ms/step - class_accuracy: 0.9771 - class_loss: 0.0055 - loss: 0.2245 - price_loss: 0.2190 - price_mae: 0.1410 - val_class_accuracy: 0.9971 - val_class_loss: 0.0014 - val_loss: 0.2404 - val_price_loss: 0.2377 - val_price_mae: 0.1480 - learning_rate: 6.2500e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 135ms/step - class_accuracy: 0.9747 - class_loss: 0.0060 - loss: 0.2323 - price_loss: 0.2263 - price_mae: 0.1443 - val_class_accuracy: 0.9992 - val_class_loss: 8.6947e-04 - val_loss: 0.2397 - val_price_loss: 0.2375 - val_price_mae: 0.1482 - learning_rate: 6.2500e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 124ms/step - class_accuracy: 0.9722 - class_loss: 0.0058 - loss: 0.2315 - price_loss: 0.2257 - price_mae: 0.1430 - val_class_accuracy: 0.9992 - val_class_loss: 9.3340e-04 - val_loss: 0.2398 - val_price_loss: 0.2376 - val_price_mae: 0.1483 - learning_rate: 6.2500e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 121ms/step - class_accuracy: 0.9747 - class_loss: 0.0057 - loss: 0.2326 - price_loss: 0.2270 - price_mae: 0.1426 - val_class_accuracy: 0.9992 - val_class_loss: 6.4275e-04 - val_loss: 0.2417 - val_price_loss: 0.2395 - val_price_mae: 0.1484 - learning_rate: 3.1250e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 122ms/step - class_accuracy: 0.9764 - class_loss: 0.0056 - loss: 0.2229 - price_loss: 0.2173 - price_mae: 0.1398 - val_class_accuracy: 0.9988 - val_class_loss: 6.6595e-04 - val_loss: 0.2398 - val_price_loss: 0.2378 - val_price_mae: 0.1481 - learning_rate: 3.1250e-05\n",
      "Epoch 40/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 120ms/step - class_accuracy: 0.9750 - class_loss: 0.0063 - loss: 0.2327 - price_loss: 0.2264 - price_mae: 0.1447 - val_class_accuracy: 0.9988 - val_class_loss: 7.2699e-04 - val_loss: 0.2409 - val_price_loss: 0.2389 - val_price_mae: 0.1483 - learning_rate: 3.1250e-05\n",
      "Epoch 41/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 122ms/step - class_accuracy: 0.9776 - class_loss: 0.0056 - loss: 0.2291 - price_loss: 0.2236 - price_mae: 0.1425 - val_class_accuracy: 0.9954 - val_class_loss: 0.0015 - val_loss: 0.2408 - val_price_loss: 0.2381 - val_price_mae: 0.1483 - learning_rate: 3.1250e-05\n",
      "Epoch 42/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 133ms/step - class_accuracy: 0.9759 - class_loss: 0.0055 - loss: 0.2290 - price_loss: 0.2235 - price_mae: 0.1418 - val_class_accuracy: 0.9992 - val_class_loss: 7.0925e-04 - val_loss: 0.2409 - val_price_loss: 0.2388 - val_price_mae: 0.1481 - learning_rate: 3.1250e-05\n",
      "Epoch 43/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 145ms/step - class_accuracy: 0.9768 - class_loss: 0.0052 - loss: 0.2292 - price_loss: 0.2241 - price_mae: 0.1427 - val_class_accuracy: 0.9996 - val_class_loss: 6.3967e-04 - val_loss: 0.2404 - val_price_loss: 0.2385 - val_price_mae: 0.1482 - learning_rate: 1.5625e-05\n",
      "Epoch 44/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 157ms/step - class_accuracy: 0.9760 - class_loss: 0.0053 - loss: 0.2323 - price_loss: 0.2270 - price_mae: 0.1435 - val_class_accuracy: 0.9988 - val_class_loss: 7.8186e-04 - val_loss: 0.2405 - val_price_loss: 0.2384 - val_price_mae: 0.1483 - learning_rate: 1.5625e-05\n",
      "Data shape: (1058, 18), Prices shape: (1058,), Labels shape: (1058, 3)\n",
      "Observation space shape: (18,)\n",
      "\n",
      "Testing...\n",
      "Test Results: Total Reward: -98.12, Portfolio Value: 9998.14, Profit/Loss: -1.86, Sharpe: -0.93, Accuracy: 0.66, Buy Count: 79, Sell Count: 55, Hold Count: 914, Time: 63.06s\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, GRU, Dense, Dropout, Bidirectional, Concatenate\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import yfinance as yf\n",
    "from ta.volatility import BollingerBands\n",
    "from pykalman import KalmanFilter\n",
    "import pywt\n",
    "import time\n",
    "\n",
    "# Custom Loss for Regression\n",
    "def custom_loss(y_true, y_pred):\n",
    "    error = tf.abs(y_true - y_pred)\n",
    "    weight = tf.where(error > 0.05, 5.0, 1.0)\n",
    "    weighted_mse = tf.reduce_mean(weight * tf.square(y_true - y_pred))\n",
    "    direction = tf.reduce_mean(tf.sign(y_true[1:] - y_true[:-1]) * tf.sign(y_pred[1:] - y_pred[:-1]))\n",
    "    drop_penalty = tf.reduce_mean(tf.where(y_true < y_pred, 15.0 * tf.square(y_true - y_pred), 0.0))\n",
    "    actual_vol = tf.math.reduce_variance(y_true)\n",
    "    pred_vol = tf.math.reduce_variance(y_pred)\n",
    "    vol_penalty = tf.square(actual_vol - pred_vol)\n",
    "    return weighted_mse - 0.1 * direction + 0.5 * drop_penalty + 0.2 * vol_penalty\n",
    "\n",
    "# Focal Loss for Classification\n",
    "def focal_loss(gamma=2.0, alpha=0.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0 - 1e-7)\n",
    "        pt = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        alpha_t = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "        return -tf.reduce_mean(alpha_t * tf.pow(1.0 - pt, gamma) * tf.math.log(pt))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# Improved Attention Layer\n",
    "class ImprovedAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads=4):\n",
    "        super(ImprovedAttentionLayer, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        assert self.d_model % self.num_heads == 0\n",
    "        self.depth = self.d_model // self.num_heads\n",
    "        self.Wq = self.add_weight(name='Wq', shape=(self.d_model, self.d_model), initializer='glorot_uniform')\n",
    "        self.Wk = self.add_weight(name='Wk', shape=(self.d_model, self.d_model), initializer='glorot_uniform')\n",
    "        self.Wv = self.add_weight(name='Wv', shape=(self.d_model, self.d_model), initializer='glorot_uniform')\n",
    "        self.Wo = self.add_weight(name='Wo', shape=(self.d_model, self.d_model), initializer='glorot_uniform')\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        Q = tf.matmul(inputs, self.Wq)\n",
    "        K = tf.matmul(inputs, self.Wk)\n",
    "        V = tf.matmul(inputs, self.Wv)\n",
    "        Q = self.split_heads(Q, batch_size)\n",
    "        K = self.split_heads(K, batch_size)\n",
    "        V = self.split_heads(V, batch_size)\n",
    "        dk = tf.cast(self.depth, tf.float32)\n",
    "        score = tf.matmul(Q, K, transpose_b=True) / tf.math.sqrt(dk)\n",
    "        attention_weights = tf.nn.softmax(score, axis=-1)\n",
    "        context = tf.matmul(attention_weights, V)\n",
    "        context = tf.transpose(context, perm=[0, 2, 1, 3])\n",
    "        context = tf.reshape(context, (batch_size, -1, self.d_model))\n",
    "        output = tf.matmul(context, self.Wo)\n",
    "        return output\n",
    "\n",
    "# Updated LSTM Model with GRU and Wavelet Branch\n",
    "def create_lstm_model(units, dropout_rate, sequence_length, num_features, num_wavelet_features=5):\n",
    "    print(\"Creating LSTM model with units:\", units, \"and dropout rate:\", dropout_rate,\n",
    "          \"for sequence length:\", sequence_length, \"and num features:\", num_features)\n",
    "    inputs = Input(shape=(sequence_length, num_features))\n",
    "    main_features = inputs[:, :, :-num_wavelet_features]\n",
    "    wavelet_features = inputs[:, :, -num_wavelet_features:]\n",
    "\n",
    "    # Main branch\n",
    "    x_main = Bidirectional(LSTM(units, return_sequences=True))(main_features)\n",
    "    x_main = ImprovedAttentionLayer()(x_main)\n",
    "    x_main = Dropout(dropout_rate)(x_main)\n",
    "    x_main = GRU(64, return_sequences=False)(x_main)\n",
    "    x_main = Dropout(dropout_rate)(x_main)\n",
    "\n",
    "    # Wavelet branch\n",
    "    x_wavelet = Bidirectional(LSTM(units, return_sequences=True))(wavelet_features)\n",
    "    x_wavelet = ImprovedAttentionLayer()(x_wavelet)\n",
    "    x_wavelet = Dropout(dropout_rate)(x_wavelet)\n",
    "    x_wavelet = GRU(64, return_sequences=False)(x_wavelet)\n",
    "    x_wavelet = Dropout(dropout_rate)(x_wavelet)\n",
    "\n",
    "    # Combine branches\n",
    "    x = Concatenate()([x_main, x_wavelet])\n",
    "    price_output = Dense(1, activation='linear', name='price')(x)\n",
    "    class_output = Dense(3, activation='softmax', name='class')(x)\n",
    "    model = Model(inputs, outputs=[price_output, class_output])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005, clipvalue=0.5)\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss={'price': custom_loss, 'class': focal_loss()},\n",
    "                  metrics={'price': 'mae', 'class': 'accuracy'})\n",
    "    return model\n",
    "\n",
    "# TradingEnvironment\n",
    "class TradingEnvironment(gym.Env):\n",
    "    def __init__(self, data, features, prices, trend_labels, initial_cash=10000, max_steps=1000):\n",
    "        super(TradingEnvironment, self).__init__()\n",
    "        self.data = data\n",
    "        self.features = features\n",
    "        self.prices = prices\n",
    "        self.trend_labels = np.array(trend_labels)\n",
    "        self.initial_cash = initial_cash\n",
    "        self.max_steps = max_steps\n",
    "        self.current_step = 0\n",
    "        self.cash = initial_cash\n",
    "        self.holdings = 0\n",
    "        self.portfolio_values = []\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf,\n",
    "            shape=(len(features),),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        self.feature_means = np.mean(data, axis=0)\n",
    "        self.feature_stds = np.std(data, axis=0) + 1e-6\n",
    "        self.normalized_data = (data - self.feature_means) / self.feature_stds\n",
    "        self.actions = []\n",
    "        self.buy_count = 0\n",
    "        self.sell_count = 0\n",
    "        self.hold_count = 0\n",
    "        self.prev_action = 2\n",
    "        print(f\"Data shape: {self.data.shape}, Prices shape: {self.prices.shape}, Labels shape: {self.trend_labels.shape}\")\n",
    "        print(f\"Observation space shape: {self.observation_space.shape}\")\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.cash = self.initial_cash\n",
    "        self.holdings = 0\n",
    "        self.portfolio_values = []\n",
    "        self.actions = []\n",
    "        self.buy_count = 0\n",
    "        self.sell_count = 0\n",
    "        self.hold_count = 0\n",
    "        self.prev_action = 2\n",
    "        return self._get_observation()\n",
    "\n",
    "    def _get_observation(self):\n",
    "        if self.current_step >= len(self.data):\n",
    "            return np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        state = self.normalized_data[self.current_step]\n",
    "        return state.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        current_price = self.prices[self.current_step]\n",
    "        reward = 0\n",
    "        if action == 0 and self.cash >= current_price:\n",
    "            self.holdings += 1\n",
    "            self.cash -= current_price\n",
    "            self.buy_count += 1\n",
    "            reward -= 0.05\n",
    "        elif action == 1 and self.holdings > 0:\n",
    "            self.holdings -= 1\n",
    "            self.cash += current_price\n",
    "            self.sell_count += 1\n",
    "            reward -= 0.05\n",
    "        else:\n",
    "            self.hold_count += 1\n",
    "            reward -= 0.1\n",
    "        self.actions.append(action)\n",
    "        self.prev_action = action\n",
    "        portfolio_value = self.cash + self.holdings * current_price\n",
    "        self.portfolio_values.append(portfolio_value)\n",
    "        if len(self.portfolio_values) > 1:\n",
    "            portfolio_change = (portfolio_value - self.portfolio_values[-2]) / self.initial_cash\n",
    "            reward += portfolio_change * 100\n",
    "            if len(self.portfolio_values) >= 11:\n",
    "                returns = np.diff(self.portfolio_values[-10:]) / self.portfolio_values[-11:-2]\n",
    "                volatility = np.std(returns)\n",
    "                reward -= volatility * 1\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= min(len(self.data), self.max_steps)\n",
    "        next_state = self._get_observation() if not done else np.zeros_like(self._get_observation())\n",
    "        info = {'portfolio_value': portfolio_value}\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def get_metrics(self):\n",
    "        total_steps = len(self.actions)\n",
    "        predicted_labels = np.array(self.actions)\n",
    "        true_labels = np.argmax(self.trend_labels[:total_steps], axis=1)\n",
    "        accuracy = np.mean(predicted_labels == true_labels) if total_steps > 0 else 0\n",
    "        final_value = self.portfolio_values[-1] if self.portfolio_values else self.initial_cash\n",
    "        sharpe = 0\n",
    "        if len(self.portfolio_values) > 1:\n",
    "            returns = np.diff(self.portfolio_values) / np.array(self.portfolio_values[:-1])\n",
    "            if len(returns) > 0 and np.std(returns) > 0:\n",
    "                sharpe = np.mean(returns) / np.std(returns) * np.sqrt(252)\n",
    "        return {\n",
    "            'final_portfolio_value': final_value,\n",
    "            'profit_loss': final_value - self.initial_cash,\n",
    "            'sharpe_ratio': sharpe,\n",
    "            'buy_count': self.buy_count,\n",
    "            'sell_count': self.sell_count,\n",
    "            'hold_count': self.hold_count,\n",
    "            'action_accuracy': accuracy,\n",
    "            'predicted_labels': predicted_labels,\n",
    "            'true_labels': true_labels\n",
    "        }\n",
    "\n",
    "# LSTM Agent\n",
    "class LSTMAgent:\n",
    "    def __init__(self, sequence_length, num_features, num_wavelet_features=5):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.num_features = num_features\n",
    "        self.num_wavelet_features = num_wavelet_features\n",
    "        self.model = create_lstm_model(\n",
    "            units=64,\n",
    "            dropout_rate=0.3,\n",
    "            sequence_length=sequence_length,\n",
    "            num_features=num_features,\n",
    "            num_wavelet_features=num_wavelet_features\n",
    "        )\n",
    "\n",
    "    def prepare_sequence(self, data, current_step):\n",
    "        if current_step < self.sequence_length:\n",
    "            return np.zeros((self.sequence_length, self.num_features), dtype=np.float32)\n",
    "        sequence = data[max(0, current_step - self.sequence_length):current_step]\n",
    "        if len(sequence) < self.sequence_length:\n",
    "            padding = np.zeros((self.sequence_length - len(sequence), self.num_features), dtype=np.float32)\n",
    "            sequence = np.vstack([padding, sequence])\n",
    "        return sequence\n",
    "\n",
    "    def act(self, sequence):\n",
    "        sequence = np.expand_dims(sequence, axis=0)\n",
    "        _, class_probs = self.model.predict(sequence, verbose=0)\n",
    "        return np.argmax(class_probs[0])\n",
    "\n",
    "# Plotting functions\n",
    "def plot_confusion_matrix(true_labels, predicted_labels, classes=['UP', 'DOWN', 'NEUTRAL']):\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.title('Confusion Matrix (LSTM)')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.savefig('confusion_matrix_lstm.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_prices(prices, actions, indices):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(indices, prices, label='Normalized Price (Target)', color='blue')\n",
    "    buy_signals = indices[actions == 0]\n",
    "    sell_signals = indices[actions == 1]\n",
    "    buy_prices = prices[actions == 0]\n",
    "    sell_prices = prices[actions == 1]\n",
    "    plt.scatter(buy_signals, buy_prices, color='green', marker='^', label='Buy', alpha=0.6)\n",
    "    plt.scatter(sell_signals, sell_prices, color='red', marker='v', label='Sell', alpha=0.6)\n",
    "    plt.title('Normalized Price with Buy/Sell Signals (LSTM)')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Normalized Price')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('price_plot_lstm.png')\n",
    "    plt.close()\n",
    "\n",
    "# Data preparation\n",
    "data = yf.download('^GSPC', start='2010-01-01', end='2025-04-12')\n",
    "\n",
    "# Validate date range\n",
    "print(f\"Data date range: {data.index[0]} to {data.index[-1]}\")\n",
    "print(f\"Raw data shape: {data.shape}\")\n",
    "\n",
    "# Flatten MultiIndex columns\n",
    "if isinstance(data.columns, pd.MultiIndex):\n",
    "    print(\"MultiIndex detected, flattening columns...\")\n",
    "    data.columns = [col[0] for col in data.columns]\n",
    "print(\"Initial columns:\", data.columns.tolist())\n",
    "\n",
    "# Apply Kalman Filter to smooth Close prices (no shift)\n",
    "kf = KalmanFilter(\n",
    "    transition_matrices=[1],\n",
    "    observation_matrices=[1],\n",
    "    initial_state_mean=data['Close'].iloc[0],\n",
    "    initial_state_covariance=1.0,\n",
    "    observation_covariance=1.0,\n",
    "    transition_covariance=0.1\n",
    ")\n",
    "smoothed_prices, _ = kf.filter(data['Close'].values)\n",
    "data['Target'] = smoothed_prices\n",
    "\n",
    "# Compute indicators\n",
    "data['SMA_crossover'] = data['Close'].rolling(window=200).mean() - data['Close'].rolling(window=50).mean()\n",
    "\n",
    "def average_true_range(df, window=14):\n",
    "    high_low = df['High'] - df['Low']\n",
    "    high_close = abs(df['High'] - df['Close'].shift(1))\n",
    "    low_close = abs(df['Low'] - df['Close'].shift(1))\n",
    "    true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    df['ATR_14'] = true_range.rolling(window).mean()\n",
    "    return df\n",
    "\n",
    "data = average_true_range(data)\n",
    "\n",
    "def on_balance_volume(df):\n",
    "    df['OBV'] = (np.sign(df['Close'].diff()) * df['Volume']).cumsum()\n",
    "    return df\n",
    "\n",
    "data = on_balance_volume(data)\n",
    "\n",
    "def vwap(df):\n",
    "    df['VWAP'] = (df['Close'] * df['Volume']).cumsum() / df['Volume'].cumsum()\n",
    "    return df\n",
    "\n",
    "data = vwap(data)\n",
    "\n",
    "delta = data['Close'].diff()\n",
    "gain = delta.where(delta > 0, 0)\n",
    "loss = -delta.where(delta < 0, 0)\n",
    "avg_gain = gain.rolling(window=14).mean()\n",
    "avg_loss = loss.rolling(window=14).mean()\n",
    "rs = avg_gain / avg_loss\n",
    "data['RSI_14'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "ema_12 = data['Close'].ewm(span=12, adjust=False).mean()\n",
    "ema_26 = data['Close'].ewm(span=26, adjust=False).mean()\n",
    "data['MACD'] = ema_12 - ema_26\n",
    "\n",
    "data['Return'] = data['Close'].pct_change()\n",
    "data['Volatility'] = data['Return'].rolling(window=10).std()\n",
    "\n",
    "bb_indicator = BollingerBands(close=data['Close'], window=20, window_dev=2)\n",
    "data['BB_bandWidth'] = bb_indicator.bollinger_hband() - bb_indicator.bollinger_lband()\n",
    "\n",
    "data['SMA_17'] = data['Close'].rolling(window=17).mean()\n",
    "data['SMA_5'] = data['Close'].rolling(window=5).mean()\n",
    "data['SMA_9'] = data['Close'].rolling(window=9).mean()\n",
    "\n",
    "data['MACD_Signal'] = data['MACD'].ewm(span=9, adjust=False).mean()\n",
    "data['MACD_Histogram'] = data['MACD'] - data['MACD_Signal']\n",
    "\n",
    "# Wavelet features (5 features)\n",
    "def compute_wavelet_features(prices, wavelet='db4', levels=3):\n",
    "    coeffs = pywt.wavedec(prices, wavelet, level=levels, mode='smooth')\n",
    "    # Extract cA3, cD3, cD2, cD1, cA2\n",
    "    cA3, cD3, cD2, cD1 = coeffs[0], coeffs[1], coeffs[2], coeffs[3]\n",
    "    cA2 = pywt.waverec([coeffs[0], coeffs[1], None, None], wavelet)[:len(prices)]\n",
    "    # Truncate/pad to match length\n",
    "    min_len = min(len(cA3), len(cD3), len(cD2), len(cD1), len(cA2), len(prices))\n",
    "    return [np.pad(c[:min_len], (0, len(prices) - min_len), mode='constant') for c in [cA3, cD3, cD2, cD1, cA2]]\n",
    "\n",
    "wavelet_features = compute_wavelet_features(data['Target'].values)\n",
    "for i, name in enumerate(['Wavelet_cA3', 'Wavelet_cD3', 'Wavelet_cD2', 'Wavelet_cD1', 'Wavelet_cA2']):\n",
    "    data[name] = wavelet_features[i]\n",
    "\n",
    "# Normalize features and target\n",
    "features = ['SMA_crossover', 'SMA_17', 'SMA_9', 'SMA_5', 'ATR_14', 'OBV',\n",
    "            'VWAP', 'Volume', 'BB_bandWidth', 'MACD', 'RSI_14', 'MACD_Histogram', 'Volatility',\n",
    "            'Wavelet_cA3', 'Wavelet_cD3', 'Wavelet_cD2', 'Wavelet_cD1', 'Wavelet_cA2']\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "data[features] = scaler_X.fit_transform(data[features])\n",
    "data['Target'] = scaler_y.fit_transform(data['Target'].values.reshape(-1, 1))\n",
    "\n",
    "# Create labels\n",
    "def create_labels(y):\n",
    "    labels = np.zeros((len(y), 3))\n",
    "    for i in range(len(y) - 1):\n",
    "        diff = y[i + 1] - y[i]\n",
    "        if diff > 0.003:\n",
    "            labels[i, 0] = 1\n",
    "        elif diff < -0.003:\n",
    "            labels[i, 1] = 1\n",
    "        else:\n",
    "            labels[i, 2] = 1\n",
    "    labels[-1, 2] = 1\n",
    "    return labels\n",
    "\n",
    "# Clean data\n",
    "data.dropna(inplace=True)\n",
    "print(f\"Data shape after dropna: {data.shape}\")\n",
    "\n",
    "# Upsample to 6915 rows (placeholder)\n",
    "if len(data) < 6915:\n",
    "    data = data.reindex(pd.date_range(start=data.index[0], end=data.index[-1], freq='D'))\n",
    "    data = data.interpolate(method='linear')\n",
    "    data = data.dropna()\n",
    "    print(f\"Data shape after upsampling: {data.shape}\")\n",
    "\n",
    "X = data[features].values\n",
    "y = data['Target'].values\n",
    "y_class = create_labels(y)\n",
    "\n",
    "# Debug: Verify shapes and class distribution\n",
    "print(\"Feature data shape:\", X.shape)\n",
    "print(\"Prices shape:\", y.shape)\n",
    "print(\"Trend labels shape:\", y_class.shape)\n",
    "print(\"Class distribution (UP, DOWN, NEUTRAL):\", np.bincount(np.argmax(y_class, axis=1)))\n",
    "\n",
    "# Train-test split\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "y_train_class, y_test_class = y_class[:split], y_class[split:]\n",
    "\n",
    "# Apply SMOTE to training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_flat = X_train\n",
    "y_train_class_flat = np.argmax(y_train_class, axis=1)\n",
    "X_train_resampled, y_train_class_resampled = smote.fit_resample(X_train_flat, y_train_class_flat)\n",
    "y_train_class_resampled = np.eye(3)[y_train_class_resampled]\n",
    "\n",
    "# Align y_train_resampled with X_train_resampled\n",
    "n_samples = len(X_train_resampled)\n",
    "y_train_resampled = np.zeros(n_samples)\n",
    "indices = np.arange(len(y_train))\n",
    "sampled_indices = np.random.choice(indices, size=n_samples, replace=True)\n",
    "y_train_resampled = y_train[sampled_indices]\n",
    "\n",
    "# Debug: Verify resampled shapes\n",
    "print(\"X_train_resampled shape:\", X_train_resampled.shape)\n",
    "print(\"y_train_class_resampled shape:\", y_train_class_resampled.shape)\n",
    "print(\"y_train_resampled shape:\", y_train_resampled.shape)\n",
    "\n",
    "# Prepare sequences for LSTM\n",
    "lookback = 10\n",
    "def create_sequences(data, labels, prices, lookback):\n",
    "    if not (len(data) == len(labels) == len(prices)):\n",
    "        raise ValueError(f\"Length mismatch: data={len(data)}, labels={len(labels)}, prices={len(prices)}\")\n",
    "    n_samples = min(len(data), len(labels), len(prices))\n",
    "    sequences = []\n",
    "    seq_labels = []\n",
    "    seq_prices = []\n",
    "    for i in range(lookback, n_samples):\n",
    "        sequences.append(data[i-lookback:i])\n",
    "        seq_labels.append(labels[i])\n",
    "        seq_prices.append(prices[i])\n",
    "    return np.array(sequences), np.array(seq_labels), np.array(seq_prices)\n",
    "\n",
    "# Create sequences\n",
    "X_train_seq, y_train_seq, y_train_prices = create_sequences(X_train_resampled, y_train_class_resampled, y_train_resampled, lookback)\n",
    "X_test_seq, y_test_seq, y_test_prices = create_sequences(X_test, y_test_class, y_test, lookback)\n",
    "\n",
    "# Debug: Verify sequence shapes\n",
    "print(\"Train sequence shape:\", X_train_seq.shape)\n",
    "print(\"Train labels shape:\", y_train_seq.shape)\n",
    "print(\"Train prices shape:\", y_train_prices.shape)\n",
    "print(\"Test sequence shape:\", X_test_seq.shape)\n",
    "print(\"Test labels shape:\", y_test_seq.shape)\n",
    "print(\"Test prices shape:\", y_test_prices.shape)\n",
    "print(\"Resampled class distribution:\", np.bincount(np.argmax(y_train_seq, axis=1)))\n",
    "\n",
    "# Initialize LSTM agent\n",
    "agent = LSTMAgent(sequence_length=lookback, num_features=len(features), num_wavelet_features=5)\n",
    "\n",
    "# Train LSTM model\n",
    "agent.model.fit(\n",
    "    X_train_seq, {'price': y_train_prices, 'class': y_train_seq},\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Initialize test environment\n",
    "test_env = TradingEnvironment(X_test, features, y_test, y_test_class, max_steps=len(y_test))\n",
    "\n",
    "# Testing phase\n",
    "print(\"\\nTesting...\")\n",
    "state = test_env.reset()\n",
    "total_reward = 0\n",
    "start_time = time.time()\n",
    "for step in range(lookback, len(X_test)):\n",
    "    sequence = test_env.normalized_data[step-lookback:step]\n",
    "    if len(sequence) < lookback:\n",
    "        sequence = np.pad(sequence, ((lookback-len(sequence), 0), (0, 0)), mode='constant')\n",
    "    action = agent.act(sequence)\n",
    "    next_state, reward, done, info = test_env.step(action)\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "    if done:\n",
    "        break\n",
    "metrics = test_env.get_metrics()\n",
    "print(f\"Test Results: Total Reward: {total_reward:.2f}, \"\n",
    "      f\"Portfolio Value: {metrics['final_portfolio_value']:.2f}, \"\n",
    "      f\"Profit/Loss: {metrics['profit_loss']:.2f}, \"\n",
    "      f\"Sharpe: {metrics['sharpe_ratio']:.2f}, \"\n",
    "      f\"Accuracy: {metrics['action_accuracy']:.2f}, \"\n",
    "      f\"Buy Count: {metrics['buy_count']}, \"\n",
    "      f\"Sell Count: {metrics['sell_count']}, \"\n",
    "      f\"Hold Count: {metrics['hold_count']}, \"\n",
    "      f\"Time: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(metrics['true_labels'], metrics['predicted_labels'])\n",
    "\n",
    "# Plot prices with buy/sell signals\n",
    "test_indices = np.arange(len(metrics['predicted_labels']))\n",
    "plot_prices(y_test[lookback:lookback+len(metrics['predicted_labels'])], metrics['predicted_labels'], test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763939d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808a4e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data date range: 2010-01-04 00:00:00 to 2025-04-11 00:00:00\n",
      "Raw data shape: (3843, 5)\n",
      "MultiIndex detected, flattening columns...\n",
      "Initial columns: ['Close', 'High', 'Low', 'Open', 'Volume']\n",
      "Data shape after dropna: (3644, 25)\n",
      "Data shape after upsampling: (5290, 25)\n",
      "Feature data shape: (5290, 18)\n",
      "Prices shape: (5290,)\n",
      "Trend labels shape: (5290, 3)\n",
      "Class distribution (UP, DOWN, NEUTRAL): [ 222  230 4838]\n",
      "X_train_resampled shape: (12054, 18)\n",
      "y_train_class_resampled shape: (12054, 3)\n",
      "y_train_resampled shape: (12054,)\n",
      "Train sequence shape: (12044, 10, 18)\n",
      "Train labels shape: (12044, 3)\n",
      "Train prices shape: (12044,)\n",
      "Test sequence shape: (1048, 10, 18)\n",
      "Test labels shape: (1048, 3)\n",
      "Test prices shape: (1048,)\n",
      "Resampled class distribution: [4018 4018 4008]\n",
      "Creating LSTM model with units: 64 and dropout rate: 0.3 for sequence length: 10 and num features: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 13:20:04.827435: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2025-04-20 13:20:04.827637: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2025-04-20 13:20:04.827995: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2025-04-20 13:20:04.828302: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-04-20 13:20:04.829038: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 13:20:06.952639: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 136ms/step - class_accuracy: 0.5840 - class_loss: 0.0557 - loss: 0.3265 - price_loss: 0.2707 - price_mae: 0.1569 - val_class_accuracy: 0.9149 - val_class_loss: 0.0431 - val_loss: 0.3342 - val_price_loss: 0.2904 - val_price_mae: 0.1745 - learning_rate: 5.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 126ms/step - class_accuracy: 0.8690 - class_loss: 0.0335 - loss: 0.2734 - price_loss: 0.2399 - price_mae: 0.1492 - val_class_accuracy: 0.9099 - val_class_loss: 0.0320 - val_loss: 0.2716 - val_price_loss: 0.2385 - val_price_mae: 0.1480 - learning_rate: 5.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 129ms/step - class_accuracy: 0.9114 - class_loss: 0.0252 - loss: 0.2714 - price_loss: 0.2462 - price_mae: 0.1506 - val_class_accuracy: 0.9103 - val_class_loss: 0.0314 - val_loss: 0.2847 - val_price_loss: 0.2521 - val_price_mae: 0.1571 - learning_rate: 5.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 130ms/step - class_accuracy: 0.9153 - class_loss: 0.0233 - loss: 0.2497 - price_loss: 0.2264 - price_mae: 0.1440 - val_class_accuracy: 0.9983 - val_class_loss: 0.0093 - val_loss: 0.2614 - val_price_loss: 0.2506 - val_price_mae: 0.1570 - learning_rate: 5.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 130ms/step - class_accuracy: 0.9278 - class_loss: 0.0198 - loss: 0.2543 - price_loss: 0.2346 - price_mae: 0.1472 - val_class_accuracy: 0.9884 - val_class_loss: 0.0146 - val_loss: 0.2523 - val_price_loss: 0.2355 - val_price_mae: 0.1480 - learning_rate: 5.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 130ms/step - class_accuracy: 0.9405 - class_loss: 0.0181 - loss: 0.2469 - price_loss: 0.2288 - price_mae: 0.1457 - val_class_accuracy: 0.9975 - val_class_loss: 0.0110 - val_loss: 0.2591 - val_price_loss: 0.2467 - val_price_mae: 0.1533 - learning_rate: 5.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 129ms/step - class_accuracy: 0.9409 - class_loss: 0.0163 - loss: 0.2447 - price_loss: 0.2284 - price_mae: 0.1454 - val_class_accuracy: 0.9954 - val_class_loss: 0.0116 - val_loss: 0.2536 - val_price_loss: 0.2404 - val_price_mae: 0.1499 - learning_rate: 5.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 129ms/step - class_accuracy: 0.9462 - class_loss: 0.0156 - loss: 0.2366 - price_loss: 0.2210 - price_mae: 0.1431 - val_class_accuracy: 0.9718 - val_class_loss: 0.0123 - val_loss: 0.2547 - val_price_loss: 0.2409 - val_price_mae: 0.1525 - learning_rate: 5.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 130ms/step - class_accuracy: 0.9424 - class_loss: 0.0150 - loss: 0.2424 - price_loss: 0.2274 - price_mae: 0.1439 - val_class_accuracy: 0.9950 - val_class_loss: 0.0064 - val_loss: 0.2556 - val_price_loss: 0.2479 - val_price_mae: 0.1541 - learning_rate: 5.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 131ms/step - class_accuracy: 0.9433 - class_loss: 0.0143 - loss: 0.2446 - price_loss: 0.2303 - price_mae: 0.1457 - val_class_accuracy: 0.9797 - val_class_loss: 0.0122 - val_loss: 0.2530 - val_price_loss: 0.2392 - val_price_mae: 0.1499 - learning_rate: 5.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 131ms/step - class_accuracy: 0.9570 - class_loss: 0.0117 - loss: 0.2344 - price_loss: 0.2227 - price_mae: 0.1420 - val_class_accuracy: 0.9942 - val_class_loss: 0.0084 - val_loss: 0.2468 - val_price_loss: 0.2367 - val_price_mae: 0.1481 - learning_rate: 2.5000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 131ms/step - class_accuracy: 0.9547 - class_loss: 0.0118 - loss: 0.2394 - price_loss: 0.2275 - price_mae: 0.1445 - val_class_accuracy: 0.9909 - val_class_loss: 0.0074 - val_loss: 0.2471 - val_price_loss: 0.2378 - val_price_mae: 0.1480 - learning_rate: 2.5000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 131ms/step - class_accuracy: 0.9603 - class_loss: 0.0111 - loss: 0.2384 - price_loss: 0.2273 - price_mae: 0.1442 - val_class_accuracy: 0.9763 - val_class_loss: 0.0093 - val_loss: 0.2485 - val_price_loss: 0.2374 - val_price_mae: 0.1491 - learning_rate: 2.5000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 150ms/step - class_accuracy: 0.9611 - class_loss: 0.0104 - loss: 0.2328 - price_loss: 0.2224 - price_mae: 0.1412 - val_class_accuracy: 0.9992 - val_class_loss: 0.0026 - val_loss: 0.2457 - val_price_loss: 0.2417 - val_price_mae: 0.1517 - learning_rate: 2.5000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 138ms/step - class_accuracy: 0.9603 - class_loss: 0.0096 - loss: 0.2370 - price_loss: 0.2275 - price_mae: 0.1441 - val_class_accuracy: 0.9938 - val_class_loss: 0.0038 - val_loss: 0.2427 - val_price_loss: 0.2374 - val_price_mae: 0.1480 - learning_rate: 2.5000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 139ms/step - class_accuracy: 0.9634 - class_loss: 0.0087 - loss: 0.2337 - price_loss: 0.2251 - price_mae: 0.1443 - val_class_accuracy: 0.9958 - val_class_loss: 0.0038 - val_loss: 0.2448 - val_price_loss: 0.2395 - val_price_mae: 0.1503 - learning_rate: 2.5000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 156ms/step - class_accuracy: 0.9627 - class_loss: 0.0092 - loss: 0.2358 - price_loss: 0.2266 - price_mae: 0.1427 - val_class_accuracy: 0.9685 - val_class_loss: 0.0111 - val_loss: 0.2528 - val_price_loss: 0.2399 - val_price_mae: 0.1501 - learning_rate: 2.5000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 195ms/step - class_accuracy: 0.9682 - class_loss: 0.0080 - loss: 0.2253 - price_loss: 0.2173 - price_mae: 0.1405 - val_class_accuracy: 0.9983 - val_class_loss: 0.0021 - val_loss: 0.2422 - val_price_loss: 0.2386 - val_price_mae: 0.1479 - learning_rate: 2.5000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 136ms/step - class_accuracy: 0.9683 - class_loss: 0.0073 - loss: 0.2303 - price_loss: 0.2230 - price_mae: 0.1422 - val_class_accuracy: 0.9958 - val_class_loss: 0.0035 - val_loss: 0.2436 - val_price_loss: 0.2389 - val_price_mae: 0.1480 - learning_rate: 2.5000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 142ms/step - class_accuracy: 0.9633 - class_loss: 0.0079 - loss: 0.2284 - price_loss: 0.2205 - price_mae: 0.1418 - val_class_accuracy: 0.9988 - val_class_loss: 7.8366e-04 - val_loss: 0.2405 - val_price_loss: 0.2383 - val_price_mae: 0.1480 - learning_rate: 2.5000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 147ms/step - class_accuracy: 0.9675 - class_loss: 0.0073 - loss: 0.2287 - price_loss: 0.2213 - price_mae: 0.1418 - val_class_accuracy: 0.9967 - val_class_loss: 0.0019 - val_loss: 0.2406 - val_price_loss: 0.2372 - val_price_mae: 0.1480 - learning_rate: 2.5000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 131ms/step - class_accuracy: 0.9709 - class_loss: 0.0068 - loss: 0.2273 - price_loss: 0.2205 - price_mae: 0.1426 - val_class_accuracy: 0.9983 - val_class_loss: 0.0012 - val_loss: 0.2396 - val_price_loss: 0.2370 - val_price_mae: 0.1479 - learning_rate: 2.5000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 138ms/step - class_accuracy: 0.9696 - class_loss: 0.0070 - loss: 0.2332 - price_loss: 0.2262 - price_mae: 0.1440 - val_class_accuracy: 0.9983 - val_class_loss: 0.0011 - val_loss: 0.2442 - val_price_loss: 0.2414 - val_price_mae: 0.1506 - learning_rate: 2.5000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 153ms/step - class_accuracy: 0.9594 - class_loss: 0.0085 - loss: 0.2293 - price_loss: 0.2208 - price_mae: 0.1427 - val_class_accuracy: 0.9942 - val_class_loss: 0.0029 - val_loss: 0.2426 - val_price_loss: 0.2381 - val_price_mae: 0.1483 - learning_rate: 2.5000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 140ms/step - class_accuracy: 0.9709 - class_loss: 0.0066 - loss: 0.2287 - price_loss: 0.2221 - price_mae: 0.1419 - val_class_accuracy: 0.9963 - val_class_loss: 0.0018 - val_loss: 0.2414 - val_price_loss: 0.2381 - val_price_mae: 0.1487 - learning_rate: 2.5000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 128ms/step - class_accuracy: 0.9698 - class_loss: 0.0073 - loss: 0.2284 - price_loss: 0.2212 - price_mae: 0.1416 - val_class_accuracy: 0.9992 - val_class_loss: 0.0012 - val_loss: 0.2426 - val_price_loss: 0.2397 - val_price_mae: 0.1497 - learning_rate: 2.5000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 127ms/step - class_accuracy: 0.9701 - class_loss: 0.0073 - loss: 0.2317 - price_loss: 0.2245 - price_mae: 0.1439 - val_class_accuracy: 0.9963 - val_class_loss: 0.0014 - val_loss: 0.2441 - val_price_loss: 0.2411 - val_price_mae: 0.1505 - learning_rate: 2.5000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 136ms/step - class_accuracy: 0.9743 - class_loss: 0.0064 - loss: 0.2301 - price_loss: 0.2238 - price_mae: 0.1431 - val_class_accuracy: 0.9988 - val_class_loss: 0.0011 - val_loss: 0.2432 - val_price_loss: 0.2404 - val_price_mae: 0.1494 - learning_rate: 1.2500e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 159ms/step - class_accuracy: 0.9719 - class_loss: 0.0067 - loss: 0.2290 - price_loss: 0.2222 - price_mae: 0.1424 - val_class_accuracy: 0.9992 - val_class_loss: 5.8768e-04 - val_loss: 0.2407 - val_price_loss: 0.2387 - val_price_mae: 0.1479 - learning_rate: 1.2500e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 124ms/step - class_accuracy: 0.9754 - class_loss: 0.0059 - loss: 0.2305 - price_loss: 0.2246 - price_mae: 0.1433 - val_class_accuracy: 0.9971 - val_class_loss: 0.0015 - val_loss: 0.2406 - val_price_loss: 0.2378 - val_price_mae: 0.1481 - learning_rate: 1.2500e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 121ms/step - class_accuracy: 0.9708 - class_loss: 0.0062 - loss: 0.2298 - price_loss: 0.2236 - price_mae: 0.1434 - val_class_accuracy: 0.9992 - val_class_loss: 8.2840e-04 - val_loss: 0.2396 - val_price_loss: 0.2374 - val_price_mae: 0.1479 - learning_rate: 1.2500e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 130ms/step - class_accuracy: 0.9724 - class_loss: 0.0060 - loss: 0.2362 - price_loss: 0.2302 - price_mae: 0.1453 - val_class_accuracy: 0.9979 - val_class_loss: 9.2987e-04 - val_loss: 0.2404 - val_price_loss: 0.2380 - val_price_mae: 0.1484 - learning_rate: 1.2500e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 152ms/step - class_accuracy: 0.9772 - class_loss: 0.0055 - loss: 0.2243 - price_loss: 0.2188 - price_mae: 0.1411 - val_class_accuracy: 0.9975 - val_class_loss: 0.0013 - val_loss: 0.2405 - val_price_loss: 0.2379 - val_price_mae: 0.1483 - learning_rate: 6.2500e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 146ms/step - class_accuracy: 0.9755 - class_loss: 0.0060 - loss: 0.2251 - price_loss: 0.2191 - price_mae: 0.1407 - val_class_accuracy: 0.9988 - val_class_loss: 8.8675e-04 - val_loss: 0.2396 - val_price_loss: 0.2373 - val_price_mae: 0.1480 - learning_rate: 6.2500e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 128ms/step - class_accuracy: 0.9771 - class_loss: 0.0055 - loss: 0.2245 - price_loss: 0.2190 - price_mae: 0.1410 - val_class_accuracy: 0.9971 - val_class_loss: 0.0014 - val_loss: 0.2404 - val_price_loss: 0.2377 - val_price_mae: 0.1480 - learning_rate: 6.2500e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 135ms/step - class_accuracy: 0.9747 - class_loss: 0.0060 - loss: 0.2323 - price_loss: 0.2263 - price_mae: 0.1443 - val_class_accuracy: 0.9992 - val_class_loss: 8.6947e-04 - val_loss: 0.2397 - val_price_loss: 0.2375 - val_price_mae: 0.1482 - learning_rate: 6.2500e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 124ms/step - class_accuracy: 0.9722 - class_loss: 0.0058 - loss: 0.2315 - price_loss: 0.2257 - price_mae: 0.1430 - val_class_accuracy: 0.9992 - val_class_loss: 9.3340e-04 - val_loss: 0.2398 - val_price_loss: 0.2376 - val_price_mae: 0.1483 - learning_rate: 6.2500e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 121ms/step - class_accuracy: 0.9747 - class_loss: 0.0057 - loss: 0.2326 - price_loss: 0.2270 - price_mae: 0.1426 - val_class_accuracy: 0.9992 - val_class_loss: 6.4275e-04 - val_loss: 0.2417 - val_price_loss: 0.2395 - val_price_mae: 0.1484 - learning_rate: 3.1250e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 122ms/step - class_accuracy: 0.9764 - class_loss: 0.0056 - loss: 0.2229 - price_loss: 0.2173 - price_mae: 0.1398 - val_class_accuracy: 0.9988 - val_class_loss: 6.6595e-04 - val_loss: 0.2398 - val_price_loss: 0.2378 - val_price_mae: 0.1481 - learning_rate: 3.1250e-05\n",
      "Epoch 40/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 120ms/step - class_accuracy: 0.9750 - class_loss: 0.0063 - loss: 0.2327 - price_loss: 0.2264 - price_mae: 0.1447 - val_class_accuracy: 0.9988 - val_class_loss: 7.2699e-04 - val_loss: 0.2409 - val_price_loss: 0.2389 - val_price_mae: 0.1483 - learning_rate: 3.1250e-05\n",
      "Epoch 41/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 122ms/step - class_accuracy: 0.9776 - class_loss: 0.0056 - loss: 0.2291 - price_loss: 0.2236 - price_mae: 0.1425 - val_class_accuracy: 0.9954 - val_class_loss: 0.0015 - val_loss: 0.2408 - val_price_loss: 0.2381 - val_price_mae: 0.1483 - learning_rate: 3.1250e-05\n",
      "Epoch 42/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 133ms/step - class_accuracy: 0.9759 - class_loss: 0.0055 - loss: 0.2290 - price_loss: 0.2235 - price_mae: 0.1418 - val_class_accuracy: 0.9992 - val_class_loss: 7.0925e-04 - val_loss: 0.2409 - val_price_loss: 0.2388 - val_price_mae: 0.1481 - learning_rate: 3.1250e-05\n",
      "Epoch 43/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 145ms/step - class_accuracy: 0.9768 - class_loss: 0.0052 - loss: 0.2292 - price_loss: 0.2241 - price_mae: 0.1427 - val_class_accuracy: 0.9996 - val_class_loss: 6.3967e-04 - val_loss: 0.2404 - val_price_loss: 0.2385 - val_price_mae: 0.1482 - learning_rate: 1.5625e-05\n",
      "Epoch 44/50\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 157ms/step - class_accuracy: 0.9760 - class_loss: 0.0053 - loss: 0.2323 - price_loss: 0.2270 - price_mae: 0.1435 - val_class_accuracy: 0.9988 - val_class_loss: 7.8186e-04 - val_loss: 0.2405 - val_price_loss: 0.2384 - val_price_mae: 0.1483 - learning_rate: 1.5625e-05\n",
      "Data shape: (1058, 18), Prices shape: (1058,), Labels shape: (1058, 3)\n",
      "Observation space shape: (18,)\n",
      "\n",
      "Testing...\n",
      "Test Results: Total Reward: -98.12, Portfolio Value: 9998.14, Profit/Loss: -1.86, Sharpe: -0.93, Accuracy: 0.66, Buy Count: 79, Sell Count: 55, Hold Count: 914, Time: 63.06s\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, GRU, Dense, Dropout, Bidirectional, Concatenate\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import yfinance as yf\n",
    "from ta.volatility import BollingerBands\n",
    "from pykalman import KalmanFilter\n",
    "import pywt\n",
    "import time\n",
    "\n",
    "# Custom Loss for Regression\n",
    "def custom_loss(y_true, y_pred):\n",
    "    error = tf.abs(y_true - y_pred)\n",
    "    weight = tf.where(error > 0.05, 5.0, 1.0)\n",
    "    weighted_mse = tf.reduce_mean(weight * tf.square(y_true - y_pred))\n",
    "    direction = tf.reduce_mean(tf.sign(y_true[1:] - y_true[:-1]) * tf.sign(y_pred[1:] - y_pred[:-1]))\n",
    "    drop_penalty = tf.reduce_mean(tf.where(y_true < y_pred, 15.0 * tf.square(y_true - y_pred), 0.0))\n",
    "    actual_vol = tf.math.reduce_variance(y_true)\n",
    "    pred_vol = tf.math.reduce_variance(y_pred)\n",
    "    vol_penalty = tf.square(actual_vol - pred_vol)\n",
    "    return weighted_mse - 0.1 * direction + 0.5 * drop_penalty + 0.2 * vol_penalty\n",
    "\n",
    "# Focal Loss for Classification\n",
    "def focal_loss(gamma=2.0, alpha=0.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0 - 1e-7)\n",
    "        pt = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        alpha_t = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "        return -tf.reduce_mean(alpha_t * tf.pow(1.0 - pt, gamma) * tf.math.log(pt))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# Improved Attention Layer\n",
    "class ImprovedAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads=4):\n",
    "        super(ImprovedAttentionLayer, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        assert self.d_model % self.num_heads == 0\n",
    "        self.depth = self.d_model // self.num_heads\n",
    "        self.Wq = self.add_weight(name='Wq', shape=(self.d_model, self.d_model), initializer='glorot_uniform')\n",
    "        self.Wk = self.add_weight(name='Wk', shape=(self.d_model, self.d_model), initializer='glorot_uniform')\n",
    "        self.Wv = self.add_weight(name='Wv', shape=(self.d_model, self.d_model), initializer='glorot_uniform')\n",
    "        self.Wo = self.add_weight(name='Wo', shape=(self.d_model, self.d_model), initializer='glorot_uniform')\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        Q = tf.matmul(inputs, self.Wq)\n",
    "        K = tf.matmul(inputs, self.Wk)\n",
    "        V = tf.matmul(inputs, self.Wv)\n",
    "        Q = self.split_heads(Q, batch_size)\n",
    "        K = self.split_heads(K, batch_size)\n",
    "        V = self.split_heads(V, batch_size)\n",
    "        dk = tf.cast(self.depth, tf.float32)\n",
    "        score = tf.matmul(Q, K, transpose_b=True) / tf.math.sqrt(dk)\n",
    "        attention_weights = tf.nn.softmax(score, axis=-1)\n",
    "        context = tf.matmul(attention_weights, V)\n",
    "        context = tf.transpose(context, perm=[0, 2, 1, 3])\n",
    "        context = tf.reshape(context, (batch_size, -1, self.d_model))\n",
    "        output = tf.matmul(context, self.Wo)\n",
    "        return output\n",
    "\n",
    "# Updated LSTM Model with GRU and Wavelet Branch\n",
    "def create_lstm_model(units, dropout_rate, sequence_length, num_features, num_wavelet_features=5):\n",
    "    print(\"Creating LSTM model with units:\", units, \"and dropout rate:\", dropout_rate,\n",
    "          \"for sequence length:\", sequence_length, \"and num features:\", num_features)\n",
    "    inputs = Input(shape=(sequence_length, num_features))\n",
    "    main_features = inputs[:, :, :-num_wavelet_features]\n",
    "    wavelet_features = inputs[:, :, -num_wavelet_features:]\n",
    "\n",
    "    # Main branch\n",
    "    x_main = Bidirectional(LSTM(units, return_sequences=True))(main_features)\n",
    "    x_main = ImprovedAttentionLayer()(x_main)\n",
    "    x_main = Dropout(dropout_rate)(x_main)\n",
    "    x_main = GRU(64, return_sequences=False)(x_main)\n",
    "    x_main = Dropout(dropout_rate)(x_main)\n",
    "\n",
    "    # Wavelet branch\n",
    "    x_wavelet = Bidirectional(LSTM(units, return_sequences=True))(wavelet_features)\n",
    "    x_wavelet = ImprovedAttentionLayer()(x_wavelet)\n",
    "    x_wavelet = Dropout(dropout_rate)(x_wavelet)\n",
    "    x_wavelet = GRU(64, return_sequences=False)(x_wavelet)\n",
    "    x_wavelet = Dropout(dropout_rate)(x_wavelet)\n",
    "\n",
    "    # Combine branches\n",
    "    x = Concatenate()([x_main, x_wavelet])\n",
    "    price_output = Dense(1, activation='linear', name='price')(x)\n",
    "    class_output = Dense(3, activation='softmax', name='class')(x)\n",
    "    model = Model(inputs, outputs=[price_output, class_output])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005, clipvalue=0.5)\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss={'price': custom_loss, 'class': focal_loss()},\n",
    "                  metrics={'price': 'mae', 'class': 'accuracy'})\n",
    "    return model\n",
    "\n",
    "# TradingEnvironment\n",
    "class TradingEnvironment(gym.Env):\n",
    "    def __init__(self, data, features, prices, trend_labels, initial_cash=10000, max_steps=1000):\n",
    "        super(TradingEnvironment, self).__init__()\n",
    "        self.data = data\n",
    "        self.features = features\n",
    "        self.prices = prices\n",
    "        self.trend_labels = np.array(trend_labels)\n",
    "        self.initial_cash = initial_cash\n",
    "        self.max_steps = max_steps\n",
    "        self.current_step = 0\n",
    "        self.cash = initial_cash\n",
    "        self.holdings = 0\n",
    "        self.portfolio_values = []\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf,\n",
    "            shape=(len(features),),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        self.feature_means = np.mean(data, axis=0)\n",
    "        self.feature_stds = np.std(data, axis=0) + 1e-6\n",
    "        self.normalized_data = (data - self.feature_means) / self.feature_stds\n",
    "        self.actions = []\n",
    "        self.buy_count = 0\n",
    "        self.sell_count = 0\n",
    "        self.hold_count = 0\n",
    "        self.prev_action = 2\n",
    "        print(f\"Data shape: {self.data.shape}, Prices shape: {self.prices.shape}, Labels shape: {self.trend_labels.shape}\")\n",
    "        print(f\"Observation space shape: {self.observation_space.shape}\")\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.cash = self.initial_cash\n",
    "        self.holdings = 0\n",
    "        self.portfolio_values = []\n",
    "        self.actions = []\n",
    "        self.buy_count = 0\n",
    "        self.sell_count = 0\n",
    "        self.hold_count = 0\n",
    "        self.prev_action = 2\n",
    "        return self._get_observation()\n",
    "\n",
    "    def _get_observation(self):\n",
    "        if self.current_step >= len(self.data):\n",
    "            return np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        state = self.normalized_data[self.current_step]\n",
    "        return state.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        current_price = self.prices[self.current_step]\n",
    "        reward = 0\n",
    "        if action == 0 and self.cash >= current_price:\n",
    "            self.holdings += 1\n",
    "            self.cash -= current_price\n",
    "            self.buy_count += 1\n",
    "            reward -= 0.05\n",
    "        elif action == 1 and self.holdings > 0:\n",
    "            self.holdings -= 1\n",
    "            self.cash += current_price\n",
    "            self.sell_count += 1\n",
    "            reward -= 0.05\n",
    "        else:\n",
    "            self.hold_count += 1\n",
    "            reward -= 0.1\n",
    "        self.actions.append(action)\n",
    "        self.prev_action = action\n",
    "        portfolio_value = self.cash + self.holdings * current_price\n",
    "        self.portfolio_values.append(portfolio_value)\n",
    "        if len(self.portfolio_values) > 1:\n",
    "            portfolio_change = (portfolio_value - self.portfolio_values[-2]) / self.initial_cash\n",
    "            reward += portfolio_change * 100\n",
    "            if len(self.portfolio_values) >= 11:\n",
    "                returns = np.diff(self.portfolio_values[-10:]) / self.portfolio_values[-11:-2]\n",
    "                volatility = np.std(returns)\n",
    "                reward -= volatility * 1\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= min(len(self.data), self.max_steps)\n",
    "        next_state = self._get_observation() if not done else np.zeros_like(self._get_observation())\n",
    "        info = {'portfolio_value': portfolio_value}\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def get_metrics(self):\n",
    "        total_steps = len(self.actions)\n",
    "        predicted_labels = np.array(self.actions)\n",
    "        true_labels = np.argmax(self.trend_labels[:total_steps], axis=1)\n",
    "        accuracy = np.mean(predicted_labels == true_labels) if total_steps > 0 else 0\n",
    "        final_value = self.portfolio_values[-1] if self.portfolio_values else self.initial_cash\n",
    "        sharpe = 0\n",
    "        if len(self.portfolio_values) > 1:\n",
    "            returns = np.diff(self.portfolio_values) / np.array(self.portfolio_values[:-1])\n",
    "            if len(returns) > 0 and np.std(returns) > 0:\n",
    "                sharpe = np.mean(returns) / np.std(returns) * np.sqrt(252)\n",
    "        return {\n",
    "            'final_portfolio_value': final_value,\n",
    "            'profit_loss': final_value - self.initial_cash,\n",
    "            'sharpe_ratio': sharpe,\n",
    "            'buy_count': self.buy_count,\n",
    "            'sell_count': self.sell_count,\n",
    "            'hold_count': self.hold_count,\n",
    "            'action_accuracy': accuracy,\n",
    "            'predicted_labels': predicted_labels,\n",
    "            'true_labels': true_labels\n",
    "        }\n",
    "\n",
    "# LSTM Agent\n",
    "class LSTMAgent:\n",
    "    def __init__(self, sequence_length, num_features, num_wavelet_features=5):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.num_features = num_features\n",
    "        self.num_wavelet_features = num_wavelet_features\n",
    "        self.model = create_lstm_model(\n",
    "            units=64,\n",
    "            dropout_rate=0.3,\n",
    "            sequence_length=sequence_length,\n",
    "            num_features=num_features,\n",
    "            num_wavelet_features=num_wavelet_features\n",
    "        )\n",
    "\n",
    "    def prepare_sequence(self, data, current_step):\n",
    "        if current_step < self.sequence_length:\n",
    "            return np.zeros((self.sequence_length, self.num_features), dtype=np.float32)\n",
    "        sequence = data[max(0, current_step - self.sequence_length):current_step]\n",
    "        if len(sequence) < self.sequence_length:\n",
    "            padding = np.zeros((self.sequence_length - len(sequence), self.num_features), dtype=np.float32)\n",
    "            sequence = np.vstack([padding, sequence])\n",
    "        return sequence\n",
    "\n",
    "    def act(self, sequence):\n",
    "        sequence = np.expand_dims(sequence, axis=0)\n",
    "        _, class_probs = self.model.predict(sequence, verbose=0)\n",
    "        return np.argmax(class_probs[0])\n",
    "\n",
    "# Plotting functions\n",
    "def plot_confusion_matrix(true_labels, predicted_labels, classes=['UP', 'DOWN', 'NEUTRAL']):\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.title('Confusion Matrix (LSTM)')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.savefig('confusion_matrix_lstm.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_prices(prices, actions, indices):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(indices, prices, label='Normalized Price (Target)', color='blue')\n",
    "    buy_signals = indices[actions == 0]\n",
    "    sell_signals = indices[actions == 1]\n",
    "    buy_prices = prices[actions == 0]\n",
    "    sell_prices = prices[actions == 1]\n",
    "    plt.scatter(buy_signals, buy_prices, color='green', marker='^', label='Buy', alpha=0.6)\n",
    "    plt.scatter(sell_signals, sell_prices, color='red', marker='v', label='Sell', alpha=0.6)\n",
    "    plt.title('Normalized Price with Buy/Sell Signals (LSTM)')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Normalized Price')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('price_plot_lstm.png')\n",
    "    plt.close()\n",
    "\n",
    "# Data preparation\n",
    "data = yf.download('^GSPC', start='2010-01-01', end='2025-04-12')\n",
    "\n",
    "# Validate date range\n",
    "print(f\"Data date range: {data.index[0]} to {data.index[-1]}\")\n",
    "print(f\"Raw data shape: {data.shape}\")\n",
    "\n",
    "# Flatten MultiIndex columns\n",
    "if isinstance(data.columns, pd.MultiIndex):\n",
    "    print(\"MultiIndex detected, flattening columns...\")\n",
    "    data.columns = [col[0] for col in data.columns]\n",
    "print(\"Initial columns:\", data.columns.tolist())\n",
    "\n",
    "# Apply Kalman Filter to smooth Close prices (no shift)\n",
    "kf = KalmanFilter(\n",
    "    transition_matrices=[1],\n",
    "    observation_matrices=[1],\n",
    "    initial_state_mean=data['Close'].iloc[0],\n",
    "    initial_state_covariance=1.0,\n",
    "    observation_covariance=1.0,\n",
    "    transition_covariance=0.1\n",
    ")\n",
    "smoothed_prices, _ = kf.filter(data['Close'].values)\n",
    "data['Target'] = smoothed_prices\n",
    "\n",
    "# Compute indicators\n",
    "data['SMA_crossover'] = data['Close'].rolling(window=200).mean() - data['Close'].rolling(window=50).mean()\n",
    "\n",
    "def average_true_range(df, window=14):\n",
    "    high_low = df['High'] - df['Low']\n",
    "    high_close = abs(df['High'] - df['Close'].shift(1))\n",
    "    low_close = abs(df['Low'] - df['Close'].shift(1))\n",
    "    true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    df['ATR_14'] = true_range.rolling(window).mean()\n",
    "    return df\n",
    "\n",
    "data = average_true_range(data)\n",
    "\n",
    "def on_balance_volume(df):\n",
    "    df['OBV'] = (np.sign(df['Close'].diff()) * df['Volume']).cumsum()\n",
    "    return df\n",
    "\n",
    "data = on_balance_volume(data)\n",
    "\n",
    "def vwap(df):\n",
    "    df['VWAP'] = (df['Close'] * df['Volume']).cumsum() / df['Volume'].cumsum()\n",
    "    return df\n",
    "\n",
    "data = vwap(data)\n",
    "\n",
    "delta = data['Close'].diff()\n",
    "gain = delta.where(delta > 0, 0)\n",
    "loss = -delta.where(delta < 0, 0)\n",
    "avg_gain = gain.rolling(window=14).mean()\n",
    "avg_loss = loss.rolling(window=14).mean()\n",
    "rs = avg_gain / avg_loss\n",
    "data['RSI_14'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "ema_12 = data['Close'].ewm(span=12, adjust=False).mean()\n",
    "ema_26 = data['Close'].ewm(span=26, adjust=False).mean()\n",
    "data['MACD'] = ema_12 - ema_26\n",
    "\n",
    "data['Return'] = data['Close'].pct_change()\n",
    "data['Volatility'] = data['Return'].rolling(window=10).std()\n",
    "\n",
    "bb_indicator = BollingerBands(close=data['Close'], window=20, window_dev=2)\n",
    "data['BB_bandWidth'] = bb_indicator.bollinger_hband() - bb_indicator.bollinger_lband()\n",
    "\n",
    "data['SMA_17'] = data['Close'].rolling(window=17).mean()\n",
    "data['SMA_5'] = data['Close'].rolling(window=5).mean()\n",
    "data['SMA_9'] = data['Close'].rolling(window=9).mean()\n",
    "\n",
    "data['MACD_Signal'] = data['MACD'].ewm(span=9, adjust=False).mean()\n",
    "data['MACD_Histogram'] = data['MACD'] - data['MACD_Signal']\n",
    "\n",
    "# Wavelet features (5 features)\n",
    "def compute_wavelet_features(prices, wavelet='db4', levels=3):\n",
    "    coeffs = pywt.wavedec(prices, wavelet, level=levels, mode='smooth')\n",
    "    # Extract cA3, cD3, cD2, cD1, cA2\n",
    "    cA3, cD3, cD2, cD1 = coeffs[0], coeffs[1], coeffs[2], coeffs[3]\n",
    "    cA2 = pywt.waverec([coeffs[0], coeffs[1], None, None], wavelet)[:len(prices)]\n",
    "    # Truncate/pad to match length\n",
    "    min_len = min(len(cA3), len(cD3), len(cD2), len(cD1), len(cA2), len(prices))\n",
    "    return [np.pad(c[:min_len], (0, len(prices) - min_len), mode='constant') for c in [cA3, cD3, cD2, cD1, cA2]]\n",
    "\n",
    "wavelet_features = compute_wavelet_features(data['Target'].values)\n",
    "for i, name in enumerate(['Wavelet_cA3', 'Wavelet_cD3', 'Wavelet_cD2', 'Wavelet_cD1', 'Wavelet_cA2']):\n",
    "    data[name] = wavelet_features[i]\n",
    "\n",
    "# Normalize features and target\n",
    "features = ['SMA_crossover', 'SMA_17', 'SMA_9', 'SMA_5', 'ATR_14', 'OBV',\n",
    "            'VWAP', 'Volume', 'BB_bandWidth', 'MACD', 'RSI_14', 'MACD_Histogram', 'Volatility',\n",
    "            'Wavelet_cA3', 'Wavelet_cD3', 'Wavelet_cD2', 'Wavelet_cD1', 'Wavelet_cA2']\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "data[features] = scaler_X.fit_transform(data[features])\n",
    "data['Target'] = scaler_y.fit_transform(data['Target'].values.reshape(-1, 1))\n",
    "\n",
    "# Create labels\n",
    "def create_labels(y):\n",
    "    labels = np.zeros((len(y), 3))\n",
    "    for i in range(len(y) - 1):\n",
    "        diff = y[i + 1] - y[i]\n",
    "        if diff > 0.003:\n",
    "            labels[i, 0] = 1\n",
    "        elif diff < -0.003:\n",
    "            labels[i, 1] = 1\n",
    "        else:\n",
    "            labels[i, 2] = 1\n",
    "    labels[-1, 2] = 1\n",
    "    return labels\n",
    "\n",
    "# Clean data\n",
    "data.dropna(inplace=True)\n",
    "print(f\"Data shape after dropna: {data.shape}\")\n",
    "\n",
    "# Upsample to 6915 rows (placeholder)\n",
    "if len(data) < 6915:\n",
    "    data = data.reindex(pd.date_range(start=data.index[0], end=data.index[-1], freq='D'))\n",
    "    data = data.interpolate(method='linear')\n",
    "    data = data.dropna()\n",
    "    print(f\"Data shape after upsampling: {data.shape}\")\n",
    "\n",
    "X = data[features].values\n",
    "y = data['Target'].values\n",
    "y_class = create_labels(y)\n",
    "\n",
    "# Debug: Verify shapes and class distribution\n",
    "print(\"Feature data shape:\", X.shape)\n",
    "print(\"Prices shape:\", y.shape)\n",
    "print(\"Trend labels shape:\", y_class.shape)\n",
    "print(\"Class distribution (UP, DOWN, NEUTRAL):\", np.bincount(np.argmax(y_class, axis=1)))\n",
    "\n",
    "# Train-test split\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "y_train_class, y_test_class = y_class[:split], y_class[split:]\n",
    "\n",
    "# Apply SMOTE to training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_flat = X_train\n",
    "y_train_class_flat = np.argmax(y_train_class, axis=1)\n",
    "X_train_resampled, y_train_class_resampled = smote.fit_resample(X_train_flat, y_train_class_flat)\n",
    "y_train_class_resampled = np.eye(3)[y_train_class_resampled]\n",
    "\n",
    "# Align y_train_resampled with X_train_resampled\n",
    "n_samples = len(X_train_resampled)\n",
    "y_train_resampled = np.zeros(n_samples)\n",
    "indices = np.arange(len(y_train))\n",
    "sampled_indices = np.random.choice(indices, size=n_samples, replace=True)\n",
    "y_train_resampled = y_train[sampled_indices]\n",
    "\n",
    "# Debug: Verify resampled shapes\n",
    "print(\"X_train_resampled shape:\", X_train_resampled.shape)\n",
    "print(\"y_train_class_resampled shape:\", y_train_class_resampled.shape)\n",
    "print(\"y_train_resampled shape:\", y_train_resampled.shape)\n",
    "\n",
    "# Prepare sequences for LSTM\n",
    "lookback = 10\n",
    "def create_sequences(data, labels, prices, lookback):\n",
    "    if not (len(data) == len(labels) == len(prices)):\n",
    "        raise ValueError(f\"Length mismatch: data={len(data)}, labels={len(labels)}, prices={len(prices)}\")\n",
    "    n_samples = min(len(data), len(labels), len(prices))\n",
    "    sequences = []\n",
    "    seq_labels = []\n",
    "    seq_prices = []\n",
    "    for i in range(lookback, n_samples):\n",
    "        sequences.append(data[i-lookback:i])\n",
    "        seq_labels.append(labels[i])\n",
    "        seq_prices.append(prices[i])\n",
    "    return np.array(sequences), np.array(seq_labels), np.array(seq_prices)\n",
    "\n",
    "# Create sequences\n",
    "X_train_seq, y_train_seq, y_train_prices = create_sequences(X_train_resampled, y_train_class_resampled, y_train_resampled, lookback)\n",
    "X_test_seq, y_test_seq, y_test_prices = create_sequences(X_test, y_test_class, y_test, lookback)\n",
    "\n",
    "# Debug: Verify sequence shapes\n",
    "print(\"Train sequence shape:\", X_train_seq.shape)\n",
    "print(\"Train labels shape:\", y_train_seq.shape)\n",
    "print(\"Train prices shape:\", y_train_prices.shape)\n",
    "print(\"Test sequence shape:\", X_test_seq.shape)\n",
    "print(\"Test labels shape:\", y_test_seq.shape)\n",
    "print(\"Test prices shape:\", y_test_prices.shape)\n",
    "print(\"Resampled class distribution:\", np.bincount(np.argmax(y_train_seq, axis=1)))\n",
    "\n",
    "# Initialize LSTM agent\n",
    "agent = LSTMAgent(sequence_length=lookback, num_features=len(features), num_wavelet_features=5)\n",
    "\n",
    "# Train LSTM model\n",
    "agent.model.fit(\n",
    "    X_train_seq, {'price': y_train_prices, 'class': y_train_seq},\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Initialize test environment\n",
    "test_env = TradingEnvironment(X_test, features, y_test, y_test_class, max_steps=len(y_test))\n",
    "\n",
    "# Testing phase\n",
    "print(\"\\nTesting...\")\n",
    "state = test_env.reset()\n",
    "total_reward = 0\n",
    "start_time = time.time()\n",
    "for step in range(lookback, len(X_test)):\n",
    "    sequence = test_env.normalized_data[step-lookback:step]\n",
    "    if len(sequence) < lookback:\n",
    "        sequence = np.pad(sequence, ((lookback-len(sequence), 0), (0, 0)), mode='constant')\n",
    "    action = agent.act(sequence)\n",
    "    next_state, reward, done, info = test_env.step(action)\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "    if done:\n",
    "        break\n",
    "metrics = test_env.get_metrics()\n",
    "print(f\"Test Results: Total Reward: {total_reward:.2f}, \"\n",
    "      f\"Portfolio Value: {metrics['final_portfolio_value']:.2f}, \"\n",
    "      f\"Profit/Loss: {metrics['profit_loss']:.2f}, \"\n",
    "      f\"Sharpe: {metrics['sharpe_ratio']:.2f}, \"\n",
    "      f\"Accuracy: {metrics['action_accuracy']:.2f}, \"\n",
    "      f\"Buy Count: {metrics['buy_count']}, \"\n",
    "      f\"Sell Count: {metrics['sell_count']}, \"\n",
    "      f\"Hold Count: {metrics['hold_count']}, \"\n",
    "      f\"Time: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(metrics['true_labels'], metrics['predicted_labels'])\n",
    "\n",
    "# Plot prices with buy/sell signals\n",
    "test_indices = np.arange(len(metrics['predicted_labels']))\n",
    "plot_prices(y_test[lookback:lookback+len(metrics['predicted_labels'])], metrics['predicted_labels'], test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59ae31cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data date range: 2010-01-04 00:00:00 to 2025-04-11 00:00:00\n",
      "Raw data shape: (3843, 5)\n",
      "MultiIndex detected, flattening columns...\n",
      "Initial columns: ['Close', 'High', 'Low', 'Open', 'Volume']\n",
      "Data shape after dropna: (3644, 25)\n",
      "Data shape after upsampling: (5290, 25)\n",
      "Feature data shape: (5290, 18)\n",
      "Prices shape: (5290,)\n",
      "Trend labels shape: (5290, 3)\n",
      "Class distribution (UP, DOWN, NEUTRAL): [1102  732 3456]\n",
      "X_train_resampled shape: (15000, 18)\n",
      "y_train_class_resampled shape: (15000, 3)\n",
      "y_train_resampled shape: (15000,)\n",
      "Train sequence shape: (14980, 20, 18)\n",
      "Train labels shape: (14980, 3)\n",
      "Train prices shape: (14980,)\n",
      "Test sequence shape: (1038, 20, 18)\n",
      "Test labels shape: (1038, 3)\n",
      "Test prices shape: (1038,)\n",
      "Resampled class distribution: [4998 5000 4982]\n",
      "Creating LSTM model with units: 128 and dropout rate: 0.4 for sequence length: 20 and num features: 18\n",
      "Epoch 1/50\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 283ms/step - class_accuracy: 0.4866 - class_loss: 0.0961 - loss: 0.1855 - price_loss: 0.2978 - price_mae: 0.1674 - val_class_accuracy: 0.5123 - val_class_loss: 0.0513 - val_loss: 0.1220 - val_price_loss: 0.2355 - val_price_mae: 0.1512 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 217ms/step - class_accuracy: 0.7954 - class_loss: 0.0693 - loss: 0.1475 - price_loss: 0.2605 - price_mae: 0.1566 - val_class_accuracy: 0.6195 - val_class_loss: 0.0360 - val_loss: 0.1058 - val_price_loss: 0.2330 - val_price_mae: 0.1453 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 200ms/step - class_accuracy: 0.8262 - class_loss: 0.0590 - loss: 0.1348 - price_loss: 0.2526 - price_mae: 0.1535 - val_class_accuracy: 0.9403 - val_class_loss: 0.0330 - val_loss: 0.1024 - val_price_loss: 0.2316 - val_price_mae: 0.1449 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 198ms/step - class_accuracy: 0.8399 - class_loss: 0.0553 - loss: 0.1307 - price_loss: 0.2512 - price_mae: 0.1532 - val_class_accuracy: 0.9579 - val_class_loss: 0.0251 - val_loss: 0.0948 - val_price_loss: 0.2325 - val_price_mae: 0.1458 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 199ms/step - class_accuracy: 0.8500 - class_loss: 0.0533 - loss: 0.1300 - price_loss: 0.2559 - price_mae: 0.1556 - val_class_accuracy: 0.9262 - val_class_loss: 0.0247 - val_loss: 0.0955 - val_price_loss: 0.2361 - val_price_mae: 0.1497 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 198ms/step - class_accuracy: 0.8524 - class_loss: 0.0512 - loss: 0.1243 - price_loss: 0.2437 - price_mae: 0.1514 - val_class_accuracy: 0.7847 - val_class_loss: 0.0270 - val_loss: 0.0978 - val_price_loss: 0.2360 - val_price_mae: 0.1494 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 195ms/step - class_accuracy: 0.8575 - class_loss: 0.0473 - loss: 0.1236 - price_loss: 0.2544 - price_mae: 0.1556 - val_class_accuracy: 0.6559 - val_class_loss: 0.0320 - val_loss: 0.1026 - val_price_loss: 0.2355 - val_price_mae: 0.1497 - learning_rate: 1.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 256ms/step - class_accuracy: 0.8603 - class_loss: 0.0460 - loss: 0.1200 - price_loss: 0.2466 - price_mae: 0.1526 - val_class_accuracy: 0.8495 - val_class_loss: 0.0231 - val_loss: 0.0927 - val_price_loss: 0.2321 - val_price_mae: 0.1453 - learning_rate: 1.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 204ms/step - class_accuracy: 0.8653 - class_loss: 0.0428 - loss: 0.1163 - price_loss: 0.2448 - price_mae: 0.1504 - val_class_accuracy: 0.7236 - val_class_loss: 0.0279 - val_loss: 0.0979 - val_price_loss: 0.2337 - val_price_mae: 0.1451 - learning_rate: 1.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 231ms/step - class_accuracy: 0.8617 - class_loss: 0.0415 - loss: 0.1144 - price_loss: 0.2427 - price_mae: 0.1513 - val_class_accuracy: 0.7824 - val_class_loss: 0.0250 - val_loss: 0.0954 - val_price_loss: 0.2348 - val_price_mae: 0.1450 - learning_rate: 1.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 201ms/step - class_accuracy: 0.8674 - class_loss: 0.0393 - loss: 0.1135 - price_loss: 0.2476 - price_mae: 0.1526 - val_class_accuracy: 0.7881 - val_class_loss: 0.0265 - val_loss: 0.0961 - val_price_loss: 0.2319 - val_price_mae: 0.1449 - learning_rate: 1.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 291ms/step - class_accuracy: 0.8752 - class_loss: 0.0369 - loss: 0.1095 - price_loss: 0.2421 - price_mae: 0.1504 - val_class_accuracy: 0.7326 - val_class_loss: 0.0277 - val_loss: 0.0975 - val_price_loss: 0.2328 - val_price_mae: 0.1459 - learning_rate: 1.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 354ms/step - class_accuracy: 0.8807 - class_loss: 0.0361 - loss: 0.1076 - price_loss: 0.2384 - price_mae: 0.1488 - val_class_accuracy: 0.8471 - val_class_loss: 0.0216 - val_loss: 0.0915 - val_price_loss: 0.2330 - val_price_mae: 0.1461 - learning_rate: 1.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 469ms/step - class_accuracy: 0.8764 - class_loss: 0.0358 - loss: 0.1079 - price_loss: 0.2403 - price_mae: 0.1502 - val_class_accuracy: 0.8198 - val_class_loss: 0.0208 - val_loss: 0.0906 - val_price_loss: 0.2329 - val_price_mae: 0.1453 - learning_rate: 1.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 214ms/step - class_accuracy: 0.8791 - class_loss: 0.0359 - loss: 0.1077 - price_loss: 0.2393 - price_mae: 0.1494 - val_class_accuracy: 0.7246 - val_class_loss: 0.0281 - val_loss: 0.0977 - val_price_loss: 0.2322 - val_price_mae: 0.1458 - learning_rate: 1.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 210ms/step - class_accuracy: 0.8804 - class_loss: 0.0351 - loss: 0.1078 - price_loss: 0.2424 - price_mae: 0.1511 - val_class_accuracy: 0.6252 - val_class_loss: 0.0305 - val_loss: 0.0996 - val_price_loss: 0.2306 - val_price_mae: 0.1454 - learning_rate: 1.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 214ms/step - class_accuracy: 0.8845 - class_loss: 0.0339 - loss: 0.1055 - price_loss: 0.2390 - price_mae: 0.1499 - val_class_accuracy: 0.6666 - val_class_loss: 0.0292 - val_loss: 0.0989 - val_price_loss: 0.2327 - val_price_mae: 0.1453 - learning_rate: 1.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 196ms/step - class_accuracy: 0.8833 - class_loss: 0.0332 - loss: 0.1045 - price_loss: 0.2377 - price_mae: 0.1491 - val_class_accuracy: 0.6679 - val_class_loss: 0.0299 - val_loss: 0.0998 - val_price_loss: 0.2330 - val_price_mae: 0.1450 - learning_rate: 1.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 218ms/step - class_accuracy: 0.8900 - class_loss: 0.0316 - loss: 0.1029 - price_loss: 0.2377 - price_mae: 0.1504 - val_class_accuracy: 0.4706 - val_class_loss: 0.0427 - val_loss: 0.1112 - val_price_loss: 0.2289 - val_price_mae: 0.1449 - learning_rate: 1.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 199ms/step - class_accuracy: 0.8943 - class_loss: 0.0312 - loss: 0.1029 - price_loss: 0.2389 - price_mae: 0.1498 - val_class_accuracy: 0.5083 - val_class_loss: 0.0394 - val_loss: 0.1092 - val_price_loss: 0.2330 - val_price_mae: 0.1456 - learning_rate: 5.0000e-05\n",
      "Epoch 21/50\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 209ms/step - class_accuracy: 0.8834 - class_loss: 0.0316 - loss: 0.1008 - price_loss: 0.2308 - price_mae: 0.1471 - val_class_accuracy: 0.6265 - val_class_loss: 0.0328 - val_loss: 0.1025 - val_price_loss: 0.2324 - val_price_mae: 0.1466 - learning_rate: 5.0000e-05\n",
      "Epoch 22/50\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 206ms/step - class_accuracy: 0.8906 - class_loss: 0.0305 - loss: 0.1001 - price_loss: 0.2319 - price_mae: 0.1474 - val_class_accuracy: 0.7223 - val_class_loss: 0.0283 - val_loss: 0.0971 - val_price_loss: 0.2297 - val_price_mae: 0.1449 - learning_rate: 5.0000e-05\n",
      "Epoch 23/50\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 199ms/step - class_accuracy: 0.8850 - class_loss: 0.0304 - loss: 0.1011 - price_loss: 0.2357 - price_mae: 0.1476 - val_class_accuracy: 0.6011 - val_class_loss: 0.0345 - val_loss: 0.1037 - val_price_loss: 0.2312 - val_price_mae: 0.1448 - learning_rate: 5.0000e-05\n",
      "Epoch 24/50\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 200ms/step - class_accuracy: 0.8908 - class_loss: 0.0292 - loss: 0.1000 - price_loss: 0.2363 - price_mae: 0.1482 - val_class_accuracy: 0.6011 - val_class_loss: 0.0356 - val_loss: 0.1049 - val_price_loss: 0.2314 - val_price_mae: 0.1450 - learning_rate: 5.0000e-05\n",
      "Data shape: (1058, 18), Prices shape: (1058,), Labels shape: (1058, 3)\n",
      "Observation space shape: (18,)\n",
      "\n",
      "Testing...\n",
      "Test Results: Total Reward: -90.91, Portfolio Value: 9999.29, Profit/Loss: -0.71, Sharpe: -0.43, Accuracy: 0.41, Buy Count: 55, Sell Count: 24, Hold Count: 959, Time: 56.66s\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, GRU, Dense, Dropout, Bidirectional, Concatenate\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import yfinance as yf\n",
    "from ta.volatility import BollingerBands\n",
    "from pykalman import KalmanFilter\n",
    "import pywt\n",
    "import time\n",
    "\n",
    "# Custom Loss for Regression\n",
    "def custom_loss(y_true, y_pred):\n",
    "    error = tf.abs(y_true - y_pred)\n",
    "    weight = tf.where(error > 0.05, 5.0, 1.0)\n",
    "    weighted_mse = tf.reduce_mean(weight * tf.square(y_true - y_pred))\n",
    "    direction = tf.reduce_mean(tf.sign(y_true[1:] - y_true[:-1]) * tf.sign(y_pred[1:] - y_pred[:-1]))\n",
    "    drop_penalty = tf.reduce_mean(tf.where(y_true < y_pred, 15.0 * tf.square(y_true - y_pred), 0.0))\n",
    "    actual_vol = tf.math.reduce_variance(y_true)\n",
    "    pred_vol = tf.math.reduce_variance(y_pred)\n",
    "    vol_penalty = tf.square(actual_vol - pred_vol)\n",
    "    return weighted_mse - 0.1 * direction + 0.5 * drop_penalty + 0.2 * vol_penalty\n",
    "\n",
    "# Focal Loss for Classification with Class Weights\n",
    "def focal_loss(gamma=3.0, alpha=0.5):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0 - 1e-7)\n",
    "        pt = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        alpha_t = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "        focal_loss = -tf.reduce_mean(alpha_t * tf.pow(1.0 - pt, gamma) * tf.math.log(pt))\n",
    "        weights = tf.reduce_sum(y_true * tf.constant([2.0, 2.0, 1.0]), axis=-1)\n",
    "        return focal_loss * weights\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# Improved Attention Layer\n",
    "class ImprovedAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads=4):\n",
    "        super(ImprovedAttentionLayer, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        assert self.d_model % self.num_heads == 0\n",
    "        self.depth = self.d_model // self.num_heads\n",
    "        self.Wq = self.add_weight(name='Wq', shape=(self.d_model, self.d_model), initializer='glorot_uniform')\n",
    "        self.Wk = self.add_weight(name='Wk', shape=(self.d_model, self.d_model), initializer='glorot_uniform')\n",
    "        self.Wv = self.add_weight(name='Wv', shape=(self.d_model, self.d_model), initializer='glorot_uniform')\n",
    "        self.Wo = self.add_weight(name='Wo', shape=(self.d_model, self.d_model), initializer='glorot_uniform')\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        Q = tf.matmul(inputs, self.Wq)\n",
    "        K = tf.matmul(inputs, self.Wk)\n",
    "        V = tf.matmul(inputs, self.Wv)\n",
    "        Q = self.split_heads(Q, batch_size)\n",
    "        K = self.split_heads(K, batch_size)\n",
    "        V = self.split_heads(V, batch_size)\n",
    "        dk = tf.cast(self.depth, tf.float32)\n",
    "        score = tf.matmul(Q, K, transpose_b=True) / tf.math.sqrt(dk)\n",
    "        attention_weights = tf.nn.softmax(score, axis=-1)\n",
    "        context = tf.matmul(attention_weights, V)\n",
    "        context = tf.transpose(context, perm=[0, 2, 1, 3])\n",
    "        context = tf.reshape(context, (batch_size, -1, self.d_model))\n",
    "        output = tf.matmul(context, self.Wo)\n",
    "        return output\n",
    "\n",
    "# Updated LSTM Model with GRU and Wavelet Branch\n",
    "def create_lstm_model(units, dropout_rate, sequence_length, num_features, num_wavelet_features=5):\n",
    "    print(\"Creating LSTM model with units:\", units, \"and dropout rate:\", dropout_rate,\n",
    "          \"for sequence length:\", sequence_length, \"and num features:\", num_features)\n",
    "    inputs = Input(shape=(sequence_length, num_features))\n",
    "    main_features = inputs[:, :, :-num_wavelet_features]\n",
    "    wavelet_features = inputs[:, :, -num_wavelet_features:]\n",
    "\n",
    "    # Main branch\n",
    "    x_main = Bidirectional(LSTM(units, return_sequences=True))(main_features)\n",
    "    x_main = ImprovedAttentionLayer()(x_main)\n",
    "    x_main = Dropout(dropout_rate)(x_main)\n",
    "    x_main = GRU(64, return_sequences=False)(x_main)\n",
    "    x_main = Dropout(dropout_rate)(x_main)\n",
    "\n",
    "    # Wavelet branch\n",
    "    x_wavelet = Bidirectional(LSTM(units, return_sequences=True))(wavelet_features)\n",
    "    x_wavelet = ImprovedAttentionLayer()(x_wavelet)\n",
    "    x_wavelet = Dropout(dropout_rate)(x_wavelet)\n",
    "    x_wavelet = GRU(64, return_sequences=False)(x_wavelet)\n",
    "    x_wavelet = Dropout(dropout_rate)(x_wavelet)\n",
    "\n",
    "    # Combine branches\n",
    "    x = Concatenate()([x_main, x_wavelet])\n",
    "    price_output = Dense(1, activation='linear', name='price')(x)\n",
    "    class_output = Dense(3, activation='softmax', name='class')(x)\n",
    "    model = Model(inputs, outputs=[price_output, class_output])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001, clipvalue=0.5)\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss={'price': custom_loss, 'class': focal_loss(gamma=3.0, alpha=0.5)},\n",
    "                  loss_weights={'price': 0.3, 'class': 1.0},\n",
    "                  metrics={'price': 'mae', 'class': 'accuracy'})\n",
    "    return model\n",
    "\n",
    "# TradingEnvironment\n",
    "class TradingEnvironment(gym.Env):\n",
    "    def __init__(self, data, features, prices, trend_labels, initial_cash=10000, max_steps=1000):\n",
    "        super(TradingEnvironment, self).__init__()\n",
    "        self.data = data\n",
    "        self.features = features\n",
    "        self.prices = prices\n",
    "        self.trend_labels = np.array(trend_labels)\n",
    "        self.initial_cash = initial_cash\n",
    "        self.max_steps = max_steps\n",
    "        self.current_step = 0\n",
    "        self.cash = initial_cash\n",
    "        self.holdings = 0\n",
    "        self.portfolio_values = []\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf,\n",
    "            shape=(len(features),),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        self.feature_means = np.mean(data, axis=0)\n",
    "        self.feature_stds = np.std(data, axis=0) + 1e-6\n",
    "        self.normalized_data = (data - self.feature_means) / self.feature_stds\n",
    "        self.actions = []\n",
    "        self.buy_count = 0\n",
    "        self.sell_count = 0\n",
    "        self.hold_count = 0\n",
    "        self.prev_action = 2\n",
    "        print(f\"Data shape: {self.data.shape}, Prices shape: {self.prices.shape}, Labels shape: {self.trend_labels.shape}\")\n",
    "        print(f\"Observation space shape: {self.observation_space.shape}\")\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.cash = self.initial_cash\n",
    "        self.holdings = 0\n",
    "        self.portfolio_values = []\n",
    "        self.actions = []\n",
    "        self.buy_count = 0\n",
    "        self.sell_count = 0\n",
    "        self.hold_count = 0\n",
    "        self.prev_action = 2\n",
    "        return self._get_observation()\n",
    "\n",
    "    def _get_observation(self):\n",
    "        if self.current_step >= len(self.data):\n",
    "            return np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        state = self.normalized_data[self.current_step]\n",
    "        return state.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        current_price = self.prices[self.current_step]\n",
    "        reward = 0\n",
    "        if action == 0 and self.cash >= current_price:\n",
    "            self.holdings += 1\n",
    "            self.cash -= current_price\n",
    "            self.buy_count += 1\n",
    "            reward += 0.05  # Encourage BUY\n",
    "        elif action == 1 and self.holdings > 0:\n",
    "            self.holdings -= 1\n",
    "            self.cash += current_price\n",
    "            self.sell_count += 1\n",
    "            reward += 0.05  # Encourage SELL\n",
    "        else:\n",
    "            self.hold_count += 1\n",
    "            if self.prev_action == 2:  # Consecutive HOLD\n",
    "                reward -= 0.1\n",
    "            else:\n",
    "                reward -= 0.05\n",
    "        self.actions.append(action)\n",
    "        self.prev_action = action\n",
    "        portfolio_value = self.cash + self.holdings * current_price\n",
    "        self.portfolio_values.append(portfolio_value)\n",
    "        if len(self.portfolio_values) > 1:\n",
    "            portfolio_change = (portfolio_value - self.portfolio_values[-2]) / self.initial_cash\n",
    "            reward += portfolio_change * 100\n",
    "            if len(self.portfolio_values) >= 11:\n",
    "                returns = np.diff(self.portfolio_values[-10:]) / self.portfolio_values[-11:-2]\n",
    "                volatility = np.std(returns)\n",
    "                reward -= volatility * 1\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= min(len(self.data), self.max_steps)\n",
    "        next_state = self._get_observation() if not done else np.zeros_like(self._get_observation())\n",
    "        info = {'portfolio_value': portfolio_value}\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def get_metrics(self):\n",
    "        total_steps = len(self.actions)\n",
    "        predicted_labels = np.array(self.actions)\n",
    "        true_labels = np.argmax(self.trend_labels[:total_steps], axis=1)\n",
    "        accuracy = np.mean(predicted_labels == true_labels) if total_steps > 0 else 0\n",
    "        final_value = self.portfolio_values[-1] if self.portfolio_values else self.initial_cash\n",
    "        sharpe = 0\n",
    "        if len(self.portfolio_values) > 1:\n",
    "            returns = np.diff(self.portfolio_values) / np.array(self.portfolio_values[:-1])\n",
    "            if len(returns) > 0 and np.std(returns) > 0:\n",
    "                sharpe = np.mean(returns) / np.std(returns) * np.sqrt(252)\n",
    "        return {\n",
    "            'final_portfolio_value': final_value,\n",
    "            'profit_loss': final_value - self.initial_cash,\n",
    "            'sharpe_ratio': sharpe,\n",
    "            'buy_count': self.buy_count,\n",
    "            'sell_count': self.sell_count,\n",
    "            'hold_count': self.hold_count,\n",
    "            'action_accuracy': accuracy,\n",
    "            'predicted_labels': predicted_labels,\n",
    "            'true_labels': true_labels\n",
    "        }\n",
    "\n",
    "# LSTM Agent\n",
    "class LSTMAgent:\n",
    "    def __init__(self, sequence_length, num_features, num_wavelet_features=5):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.num_features = num_features\n",
    "        self.num_wavelet_features = num_wavelet_features\n",
    "        self.model = create_lstm_model(\n",
    "            units=128,\n",
    "            dropout_rate=0.4,\n",
    "            sequence_length=sequence_length,\n",
    "            num_features=num_features,\n",
    "            num_wavelet_features=num_wavelet_features\n",
    "        )\n",
    "\n",
    "    def prepare_sequence(self, data, current_step):\n",
    "        if current_step < self.sequence_length:\n",
    "            return np.zeros((self.sequence_length, self.num_features), dtype=np.float32)\n",
    "        sequence = data[max(0, current_step - self.sequence_length):current_step]\n",
    "        if len(sequence) < self.sequence_length:\n",
    "            padding = np.zeros((self.sequence_length - len(sequence), self.num_features), dtype=np.float32)\n",
    "            sequence = np.vstack([padding, sequence])\n",
    "        return sequence\n",
    "\n",
    "    def act(self, sequence):\n",
    "        sequence = np.expand_dims(sequence, axis=0)\n",
    "        _, class_probs = self.model.predict(sequence, verbose=0)\n",
    "        return np.argmax(class_probs[0])\n",
    "\n",
    "# Plotting functions\n",
    "def plot_confusion_matrix(true_labels, predicted_labels, classes=['UP', 'DOWN', 'NEUTRAL']):\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.title('Confusion Matrix (LSTM)')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.savefig('confusion_matrix_lstm.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_prices(prices, actions, indices):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(indices, prices, label='Normalized Price (Target)', color='blue')\n",
    "    buy_signals = indices[actions == 0]\n",
    "    sell_signals = indices[actions == 1]\n",
    "    buy_prices = prices[actions == 0]\n",
    "    sell_prices = prices[actions == 1]\n",
    "    plt.scatter(buy_signals, buy_prices, color='green', marker='^', label='Buy', alpha=0.6)\n",
    "    plt.scatter(sell_signals, sell_prices, color='red', marker='v', label='Sell', alpha=0.6)\n",
    "    plt.title('Normalized Price with Buy/Sell Signals (LSTM)')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Normalized Price')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('price_plot_lstm.png')\n",
    "    plt.close()\n",
    "\n",
    "# Data preparation\n",
    "data = yf.download('^GSPC', start='2010-01-01', end='2025-04-12')\n",
    "\n",
    "# Validate date range\n",
    "print(f\"Data date range: {data.index[0]} to {data.index[-1]}\")\n",
    "print(f\"Raw data shape: {data.shape}\")\n",
    "\n",
    "# Flatten MultiIndex columns\n",
    "if isinstance(data.columns, pd.MultiIndex):\n",
    "    print(\"MultiIndex detected, flattening columns...\")\n",
    "    data.columns = [col[0] for col in data.columns]\n",
    "print(\"Initial columns:\", data.columns.tolist())\n",
    "\n",
    "# Apply Kalman Filter to smooth Close prices\n",
    "kf = KalmanFilter(\n",
    "    transition_matrices=[1],\n",
    "    observation_matrices=[1],\n",
    "    initial_state_mean=data['Close'].iloc[0],\n",
    "    initial_state_covariance=1.0,\n",
    "    observation_covariance=1.0,\n",
    "    transition_covariance=0.1\n",
    ")\n",
    "smoothed_prices, _ = kf.filter(data['Close'].values)\n",
    "data['Target'] = smoothed_prices\n",
    "\n",
    "# Compute indicators\n",
    "data['SMA_crossover'] = data['Close'].rolling(window=200).mean() - data['Close'].rolling(window=50).mean()\n",
    "\n",
    "def average_true_range(df, window=14):\n",
    "    high_low = df['High'] - df['Low']\n",
    "    high_close = abs(df['High'] - df['Close'].shift(1))\n",
    "    low_close = abs(df['Low'] - df['Close'].shift(1))\n",
    "    true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    df['ATR_14'] = true_range.rolling(window).mean()\n",
    "    return df\n",
    "\n",
    "data = average_true_range(data)\n",
    "\n",
    "def on_balance_volume(df):\n",
    "    df['OBV'] = (np.sign(df['Close'].diff()) * df['Volume']).cumsum()\n",
    "    return df\n",
    "\n",
    "data = on_balance_volume(data)\n",
    "\n",
    "def vwap(df):\n",
    "    df['VWAP'] = (df['Close'] * df['Volume']).cumsum() / df['Volume'].cumsum()\n",
    "    return df\n",
    "\n",
    "data = vwap(data)\n",
    "\n",
    "delta = data['Close'].diff()\n",
    "gain = delta.where(delta > 0, 0)\n",
    "loss = -delta.where(delta < 0, 0)\n",
    "avg_gain = gain.rolling(window=14).mean()\n",
    "avg_loss = loss.rolling(window=14).mean()\n",
    "rs = avg_gain / avg_loss\n",
    "data['RSI_14'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "ema_12 = data['Close'].ewm(span=12, adjust=False).mean()\n",
    "ema_26 = data['Close'].ewm(span=26, adjust=False).mean()\n",
    "data['MACD'] = ema_12 - ema_26\n",
    "\n",
    "data['Return'] = data['Close'].pct_change()\n",
    "data['Volatility'] = data['Return'].rolling(window=10).std()\n",
    "\n",
    "bb_indicator = BollingerBands(close=data['Close'], window=20, window_dev=2)\n",
    "data['BB_bandWidth'] = bb_indicator.bollinger_hband() - bb_indicator.bollinger_lband()\n",
    "\n",
    "data['SMA_17'] = data['Close'].rolling(window=17).mean()\n",
    "data['SMA_5'] = data['Close'].rolling(window=5).mean()\n",
    "data['SMA_9'] = data['Close'].rolling(window=9).mean()\n",
    "\n",
    "data['MACD_Signal'] = data['MACD'].ewm(span=9, adjust=False).mean()\n",
    "data['MACD_Histogram'] = data['MACD'] - data['MACD_Signal']\n",
    "\n",
    "# Wavelet features (5 features)\n",
    "def compute_wavelet_features(prices, wavelet='db4', levels=3):\n",
    "    coeffs = pywt.wavedec(prices, wavelet, level=levels, mode='smooth')\n",
    "    cA3, cD3, cD2, cD1 = coeffs[0], coeffs[1], coeffs[2], coeffs[3]\n",
    "    cA2 = pywt.waverec([coeffs[0], coeffs[1], None, None], wavelet)[:len(prices)]\n",
    "    min_len = min(len(cA3), len(cD3), len(cD2), len(cD1), len(cA2), len(prices))\n",
    "    return [np.pad(c[:min_len], (0, len(prices) - min_len), mode='constant') for c in [cA3, cD3, cD2, cD1, cA2]]\n",
    "\n",
    "wavelet_features = compute_wavelet_features(data['Target'].values)\n",
    "for i, name in enumerate(['Wavelet_cA3', 'Wavelet_cD3', 'Wavelet_cD2', 'Wavelet_cD1', 'Wavelet_cA2']):\n",
    "    data[name] = wavelet_features[i]\n",
    "\n",
    "# Normalize features and target\n",
    "features = ['SMA_crossover', 'SMA_17', 'SMA_9', 'SMA_5', 'ATR_14', 'OBV',\n",
    "            'VWAP', 'Volume', 'BB_bandWidth', 'MACD', 'RSI_14', 'MACD_Histogram', 'Volatility',\n",
    "            'Wavelet_cA3', 'Wavelet_cD3', 'Wavelet_cD2', 'Wavelet_cD1', 'Wavelet_cA2']\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "data[features] = scaler_X.fit_transform(data[features])\n",
    "data['Target'] = scaler_y.fit_transform(data['Target'].values.reshape(-1, 1))\n",
    "\n",
    "# Create labels with adjusted threshold\n",
    "def create_labels(y):\n",
    "    labels = np.zeros((len(y), 3))\n",
    "    for i in range(len(y) - 1):\n",
    "        diff = y[i + 1] - y[i]\n",
    "        if diff > 0.001:\n",
    "            labels[i, 0] = 1\n",
    "        elif diff < -0.001:\n",
    "            labels[i, 1] = 1\n",
    "        else:\n",
    "            labels[i, 2] = 1\n",
    "    labels[-1, 2] = 1\n",
    "    return labels\n",
    "\n",
    "# Clean data\n",
    "data.dropna(inplace=True)\n",
    "print(f\"Data shape after dropna: {data.shape}\")\n",
    "\n",
    "# Upsample to 6915 rows (placeholder)\n",
    "if len(data) < 6915:\n",
    "    data = data.reindex(pd.date_range(start=data.index[0], end=data.index[-1], freq='D'))\n",
    "    data = data.interpolate(method='linear')\n",
    "    data = data.dropna()\n",
    "    print(f\"Data shape after upsampling: {data.shape}\")\n",
    "\n",
    "X = data[features].values\n",
    "y = data['Target'].values\n",
    "y_class = create_labels(y)\n",
    "\n",
    "# Debug: Verify shapes and class distribution\n",
    "print(\"Feature data shape:\", X.shape)\n",
    "print(\"Prices shape:\", y.shape)\n",
    "print(\"Trend labels shape:\", y_class.shape)\n",
    "print(\"Class distribution (UP, DOWN, NEUTRAL):\", np.bincount(np.argmax(y_class, axis=1)))\n",
    "\n",
    "# Train-test split\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "y_train_class, y_test_class = y_class[:split], y_class[split:]\n",
    "\n",
    "# Apply SMOTE to training data\n",
    "smote = SMOTE(sampling_strategy={0: 5000, 1: 5000, 2: 5000}, random_state=42)\n",
    "X_train_flat = X_train\n",
    "y_train_class_flat = np.argmax(y_train_class, axis=1)\n",
    "X_train_resampled, y_train_class_resampled = smote.fit_resample(X_train_flat, y_train_class_flat)\n",
    "y_train_class_resampled = np.eye(3)[y_train_class_resampled]\n",
    "\n",
    "# Align y_train_resampled with X_train_resampled\n",
    "n_samples = len(X_train_resampled)\n",
    "y_train_resampled = np.zeros(n_samples)\n",
    "indices = np.arange(len(y_train))\n",
    "sampled_indices = np.random.choice(indices, size=n_samples, replace=True)\n",
    "y_train_resampled = y_train[sampled_indices]\n",
    "\n",
    "# Debug: Verify resampled shapes\n",
    "print(\"X_train_resampled shape:\", X_train_resampled.shape)\n",
    "print(\"y_train_class_resampled shape:\", y_train_class_resampled.shape)\n",
    "print(\"y_train_resampled shape:\", y_train_resampled.shape)\n",
    "\n",
    "# Prepare sequences for LSTM\n",
    "lookback = 20\n",
    "def create_sequences(data, labels, prices, lookback):\n",
    "    if not (len(data) == len(labels) == len(prices)):\n",
    "        raise ValueError(f\"Length mismatch: data={len(data)}, labels={len(labels)}, prices={len(prices)}\")\n",
    "    n_samples = min(len(data), len(labels), len(prices))\n",
    "    sequences = []\n",
    "    seq_labels = []\n",
    "    seq_prices = []\n",
    "    for i in range(lookback, n_samples):\n",
    "        sequences.append(data[i-lookback:i])\n",
    "        seq_labels.append(labels[i])\n",
    "        seq_prices.append(prices[i])\n",
    "    return np.array(sequences), np.array(seq_labels), np.array(seq_prices)\n",
    "\n",
    "# Create sequences\n",
    "X_train_seq, y_train_seq, y_train_prices = create_sequences(X_train_resampled, y_train_class_resampled, y_train_resampled, lookback)\n",
    "X_test_seq, y_test_seq, y_test_prices = create_sequences(X_test, y_test_class, y_test, lookback)\n",
    "\n",
    "# Debug: Verify sequence shapes\n",
    "print(\"Train sequence shape:\", X_train_seq.shape)\n",
    "print(\"Train labels shape:\", y_train_seq.shape)\n",
    "print(\"Train prices shape:\", y_train_prices.shape)\n",
    "print(\"Test sequence shape:\", X_test_seq.shape)\n",
    "print(\"Test labels shape:\", y_test_seq.shape)\n",
    "print(\"Test prices shape:\", y_test_prices.shape)\n",
    "print(\"Resampled class distribution:\", np.bincount(np.argmax(y_train_seq, axis=1)))\n",
    "\n",
    "# Initialize LSTM agent\n",
    "agent = LSTMAgent(sequence_length=lookback, num_features=len(features), num_wavelet_features=5)\n",
    "\n",
    "# Train LSTM model\n",
    "agent.model.fit(\n",
    "    X_train_seq, {'price': y_train_prices, 'class': y_train_seq},\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Initialize test environment\n",
    "test_env = TradingEnvironment(X_test, features, y_test, y_test_class, max_steps=len(y_test))\n",
    "\n",
    "# Testing phase\n",
    "print(\"\\nTesting...\")\n",
    "state = test_env.reset()\n",
    "total_reward = 0\n",
    "start_time = time.time()\n",
    "for step in range(lookback, len(X_test)):\n",
    "    sequence = test_env.normalized_data[step-lookback:step]\n",
    "    if len(sequence) < lookback:\n",
    "        sequence = np.pad(sequence, ((lookback-len(sequence), 0), (0, 0)), mode='constant')\n",
    "    action = agent.act(sequence)\n",
    "    next_state, reward, done, info = test_env.step(action)\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "    if done:\n",
    "        break\n",
    "metrics = test_env.get_metrics()\n",
    "print(f\"Test Results: Total Reward: {total_reward:.2f}, \"\n",
    "      f\"Portfolio Value: {metrics['final_portfolio_value']:.2f}, \"\n",
    "      f\"Profit/Loss: {metrics['profit_loss']:.2f}, \"\n",
    "      f\"Sharpe: {metrics['sharpe_ratio']:.2f}, \"\n",
    "      f\"Accuracy: {metrics['action_accuracy']:.2f}, \"\n",
    "      f\"Buy Count: {metrics['buy_count']}, \"\n",
    "      f\"Sell Count: {metrics['sell_count']}, \"\n",
    "      f\"Hold Count: {metrics['hold_count']}, \"\n",
    "      f\"Time: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(metrics['true_labels'], metrics['predicted_labels'])\n",
    "\n",
    "# Plot prices with buy/sell signals\n",
    "test_indices = np.arange(len(metrics['predicted_labels']))\n",
    "plot_prices(y_test[lookback:lookback+len(metrics['predicted_labels'])], metrics['predicted_labels'], test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e8db5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data date range: 2010-01-04 00:00:00 to 2025-04-11 00:00:00\n",
      "Raw data shape: (3843, 5)\n",
      "MultiIndex detected, flattening columns...\n",
      "Initial columns: ['Close', 'High', 'Low', 'Open', 'Volume']\n",
      "Data shape after dropna: (3644, 25)\n",
      "Data shape after upsampling: (5290, 25)\n",
      "Feature data shape: (5290, 18)\n",
      "Prices shape: (5290,)\n",
      "Trend labels shape: (5290, 3)\n",
      "Class distribution (UP, DOWN, NEUTRAL): [1878 1087 2325]\n",
      "X_train_resampled shape: (16000, 18)\n",
      "y_train_class_resampled shape: (16000, 3)\n",
      "y_train_resampled shape: (16000,)\n",
      "Train sequence shape: (15985, 15, 18)\n",
      "Train labels shape: (15985, 3)\n",
      "Train prices shape: (15985,)\n",
      "Test sequence shape: (1043, 15, 18)\n",
      "Test labels shape: (1043, 3)\n",
      "Test prices shape: (1043,)\n",
      "Resampled class distribution: [3999 4000 7986]\n",
      "Creating LSTM model with units: 128 and dropout rate: 0.5 for sequence length: 15 and num features: 18\n",
      "Epoch 1/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 270ms/step - accuracy: 0.5418 - loss: 0.1682 - val_accuracy: 0.9740 - val_loss: 0.0729 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 210ms/step - accuracy: 0.7689 - loss: 0.1290 - val_accuracy: 0.9753 - val_loss: 0.0599 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 211ms/step - accuracy: 0.7849 - loss: 0.1155 - val_accuracy: 0.9778 - val_loss: 0.0520 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 205ms/step - accuracy: 0.7931 - loss: 0.1024 - val_accuracy: 0.9837 - val_loss: 0.0409 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 233ms/step - accuracy: 0.7923 - loss: 0.0946 - val_accuracy: 0.9162 - val_loss: 0.0375 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 242ms/step - accuracy: 0.7996 - loss: 0.0871 - val_accuracy: 0.9725 - val_loss: 0.0309 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 204ms/step - accuracy: 0.7831 - loss: 0.0837 - val_accuracy: 0.9465 - val_loss: 0.0311 - learning_rate: 1.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 216ms/step - accuracy: 0.8078 - loss: 0.0770 - val_accuracy: 0.9647 - val_loss: 0.0235 - learning_rate: 1.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 292ms/step - accuracy: 0.8027 - loss: 0.0715 - val_accuracy: 0.9609 - val_loss: 0.0210 - learning_rate: 1.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 239ms/step - accuracy: 0.7967 - loss: 0.0683 - val_accuracy: 0.9662 - val_loss: 0.0173 - learning_rate: 1.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8067 - loss: 0.0618 - val_accuracy: 0.9775 - val_loss: 0.0152 - learning_rate: 1.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 209ms/step - accuracy: 0.8034 - loss: 0.0618 - val_accuracy: 0.9140 - val_loss: 0.0179 - learning_rate: 1.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 213ms/step - accuracy: 0.8110 - loss: 0.0586 - val_accuracy: 0.9643 - val_loss: 0.0117 - learning_rate: 1.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 223ms/step - accuracy: 0.8138 - loss: 0.0573 - val_accuracy: 0.9687 - val_loss: 0.0113 - learning_rate: 1.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 213ms/step - accuracy: 0.8105 - loss: 0.0552 - val_accuracy: 0.9775 - val_loss: 0.0106 - learning_rate: 1.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 219ms/step - accuracy: 0.8128 - loss: 0.0534 - val_accuracy: 0.9659 - val_loss: 0.0114 - learning_rate: 1.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 298ms/step - accuracy: 0.8146 - loss: 0.0522 - val_accuracy: 0.9252 - val_loss: 0.0103 - learning_rate: 1.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 407ms/step - accuracy: 0.8197 - loss: 0.0501 - val_accuracy: 0.9737 - val_loss: 0.0074 - learning_rate: 1.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 225ms/step - accuracy: 0.8210 - loss: 0.0497 - val_accuracy: 0.9105 - val_loss: 0.0099 - learning_rate: 1.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 260ms/step - accuracy: 0.8184 - loss: 0.0491 - val_accuracy: 0.9600 - val_loss: 0.0063 - learning_rate: 1.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 270ms/step - accuracy: 0.8268 - loss: 0.0470 - val_accuracy: 0.9390 - val_loss: 0.0082 - learning_rate: 1.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 253ms/step - accuracy: 0.8313 - loss: 0.0459 - val_accuracy: 0.9615 - val_loss: 0.0057 - learning_rate: 1.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 230ms/step - accuracy: 0.8243 - loss: 0.0464 - val_accuracy: 0.9418 - val_loss: 0.0070 - learning_rate: 1.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 244ms/step - accuracy: 0.8353 - loss: 0.0433 - val_accuracy: 0.9647 - val_loss: 0.0053 - learning_rate: 1.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 216ms/step - accuracy: 0.8364 - loss: 0.0426 - val_accuracy: 0.9640 - val_loss: 0.0053 - learning_rate: 1.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 220ms/step - accuracy: 0.8396 - loss: 0.0420 - val_accuracy: 0.9665 - val_loss: 0.0048 - learning_rate: 1.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 210ms/step - accuracy: 0.8359 - loss: 0.0411 - val_accuracy: 0.9762 - val_loss: 0.0044 - learning_rate: 1.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 215ms/step - accuracy: 0.8372 - loss: 0.0413 - val_accuracy: 0.9534 - val_loss: 0.0060 - learning_rate: 1.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 208ms/step - accuracy: 0.8383 - loss: 0.0407 - val_accuracy: 0.9559 - val_loss: 0.0055 - learning_rate: 1.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 209ms/step - accuracy: 0.8444 - loss: 0.0406 - val_accuracy: 0.9656 - val_loss: 0.0052 - learning_rate: 1.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 194ms/step - accuracy: 0.8433 - loss: 0.0411 - val_accuracy: 0.9600 - val_loss: 0.0049 - learning_rate: 1.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 246ms/step - accuracy: 0.8434 - loss: 0.0405 - val_accuracy: 0.9243 - val_loss: 0.0075 - learning_rate: 1.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 296ms/step - accuracy: 0.8472 - loss: 0.0391 - val_accuracy: 0.9409 - val_loss: 0.0063 - learning_rate: 5.0000e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 205ms/step - accuracy: 0.8480 - loss: 0.0397 - val_accuracy: 0.9578 - val_loss: 0.0049 - learning_rate: 5.0000e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 215ms/step - accuracy: 0.8477 - loss: 0.0384 - val_accuracy: 0.9765 - val_loss: 0.0037 - learning_rate: 5.0000e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8428 - loss: 0.0399 - val_accuracy: 0.9612 - val_loss: 0.0052 - learning_rate: 5.0000e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 206ms/step - accuracy: 0.8494 - loss: 0.0380 - val_accuracy: 0.9584 - val_loss: 0.0050 - learning_rate: 5.0000e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 210ms/step - accuracy: 0.8486 - loss: 0.0379 - val_accuracy: 0.9359 - val_loss: 0.0069 - learning_rate: 5.0000e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 198ms/step - accuracy: 0.8439 - loss: 0.0389 - val_accuracy: 0.9712 - val_loss: 0.0038 - learning_rate: 5.0000e-05\n",
      "Epoch 40/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 205ms/step - accuracy: 0.8419 - loss: 0.0390 - val_accuracy: 0.9603 - val_loss: 0.0046 - learning_rate: 5.0000e-05\n",
      "Epoch 41/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 204ms/step - accuracy: 0.8496 - loss: 0.0378 - val_accuracy: 0.9647 - val_loss: 0.0045 - learning_rate: 2.5000e-05\n",
      "Epoch 42/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 210ms/step - accuracy: 0.8501 - loss: 0.0381 - val_accuracy: 0.9634 - val_loss: 0.0043 - learning_rate: 2.5000e-05\n",
      "Epoch 43/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 205ms/step - accuracy: 0.8414 - loss: 0.0385 - val_accuracy: 0.9706 - val_loss: 0.0041 - learning_rate: 2.5000e-05\n",
      "Epoch 44/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 210ms/step - accuracy: 0.8443 - loss: 0.0385 - val_accuracy: 0.9650 - val_loss: 0.0046 - learning_rate: 2.5000e-05\n",
      "Epoch 45/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 206ms/step - accuracy: 0.8459 - loss: 0.0385 - val_accuracy: 0.9559 - val_loss: 0.0049 - learning_rate: 2.5000e-05\n",
      "Data shape: (1058, 18), Prices shape: (1058,), Labels shape: (1058, 3)\n",
      "Observation space shape: (18,)\n",
      "\n",
      "Testing...\n",
      "Test Results: Total Reward: -103.22, Portfolio Value: 10029.43, Profit/Loss: 29.43, Sharpe: 2.94, Accuracy: 0.37, Buy Count: 297, Sell Count: 212, Hold Count: 534, Time: 59.53s\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, GRU, Dense, Dropout, Bidirectional, Concatenate\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import yfinance as yf\n",
    "from ta.volatility import BollingerBands\n",
    "from pykalman import KalmanFilter\n",
    "import pywt\n",
    "import time\n",
    "\n",
    "# Focal Loss for Classification with Class Weights\n",
    "def focal_loss(gamma=4.0, alpha=0.75):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0 - 1e-7)\n",
    "        pt = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        alpha_t = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "        focal_loss = -tf.reduce_mean(alpha_t * tf.pow(1.0 - pt, gamma) * tf.math.log(pt))\n",
    "        weights = tf.reduce_sum(y_true * tf.constant([3.0, 3.0, 1.0]), axis=-1)\n",
    "        return focal_loss * weights\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# Improved Attention Layer\n",
    "class ImprovedAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads=4):\n",
    "        super(ImprovedAttentionLayer, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        assert self.d_model % self.num_heads == 0\n",
    "        self.depth = self.d_model // self.num_heads\n",
    "        self.Wq = self.add_weight(name='Wq', shape=(self.d_model, self.d_model), initializer='glorot_uniform')\n",
    "        self.Wk = self.add_weight(name='Wk', shape=(self.d_model, self.d_model), initializer='glorot_uniform')\n",
    "        self.Wv = self.add_weight(name='Wv', shape=(self.d_model, self.d_model), initializer='glorot_uniform')\n",
    "        self.Wo = self.add_weight(name='Wo', shape=(self.d_model, self.d_model), initializer='glorot_uniform')\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        Q = tf.matmul(inputs, self.Wq)\n",
    "        K = tf.matmul(inputs, self.Wk)\n",
    "        V = tf.matmul(inputs, self.Wv)\n",
    "        Q = self.split_heads(Q, batch_size)\n",
    "        K = self.split_heads(K, batch_size)\n",
    "        V = self.split_heads(V, batch_size)\n",
    "        dk = tf.cast(self.depth, tf.float32)\n",
    "        score = tf.matmul(Q, K, transpose_b=True) / tf.math.sqrt(dk)\n",
    "        attention_weights = tf.nn.softmax(score, axis=-1)\n",
    "        context = tf.matmul(attention_weights, V)\n",
    "        context = tf.transpose(context, perm=[0, 2, 1, 3])\n",
    "        context = tf.reshape(context, (batch_size, -1, self.d_model))\n",
    "        output = tf.matmul(context, self.Wo)\n",
    "        return output\n",
    "\n",
    "# Updated LSTM Model (Classification Only)\n",
    "def create_lstm_model(units, dropout_rate, sequence_length, num_features, num_wavelet_features=3):\n",
    "    print(\"Creating LSTM model with units:\", units, \"and dropout rate:\", dropout_rate,\n",
    "          \"for sequence length:\", sequence_length, \"and num features:\", num_features)\n",
    "    inputs = Input(shape=(sequence_length, num_features))\n",
    "    main_features = inputs[:, :, :-num_wavelet_features]\n",
    "    wavelet_features = inputs[:, :, -num_wavelet_features:]\n",
    "\n",
    "    # Main branch\n",
    "    x_main = Bidirectional(LSTM(units, return_sequences=True))(main_features)\n",
    "    x_main = ImprovedAttentionLayer()(x_main)\n",
    "    x_main = Dropout(dropout_rate)(x_main)\n",
    "    x_main = GRU(64, return_sequences=False)(x_main)\n",
    "    x_main = Dropout(dropout_rate)(x_main)\n",
    "\n",
    "    # Wavelet branch\n",
    "    x_wavelet = Bidirectional(LSTM(units, return_sequences=True))(wavelet_features)\n",
    "    x_wavelet = ImprovedAttentionLayer()(x_wavelet)\n",
    "    x_wavelet = Dropout(dropout_rate)(x_wavelet)\n",
    "    x_wavelet = GRU(64, return_sequences=False)(x_wavelet)\n",
    "    x_wavelet = Dropout(dropout_rate)(x_wavelet)\n",
    "\n",
    "    # Combine branches\n",
    "    x = Concatenate()([x_main, x_wavelet])\n",
    "    class_output = Dense(3, activation='softmax', name='class', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "    model = Model(inputs, outputs=class_output)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001, clipvalue=0.5)\n",
    "    model.compile(optimizer=optimizer, loss=focal_loss(gamma=4.0, alpha=0.75), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# TradingEnvironment\n",
    "class TradingEnvironment(gym.Env):\n",
    "    def __init__(self, data, features, prices, trend_labels, initial_cash=10000, max_steps=1000):\n",
    "        super(TradingEnvironment, self).__init__()\n",
    "        self.data = data\n",
    "        self.features = features\n",
    "        self.prices = prices\n",
    "        self.trend_labels = np.array(trend_labels)\n",
    "        self.initial_cash = initial_cash\n",
    "        self.max_steps = max_steps\n",
    "        self.current_step = 0\n",
    "        self.cash = initial_cash\n",
    "        self.holdings = 0\n",
    "        self.portfolio_values = []\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf,\n",
    "            shape=(len(features),),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        self.feature_means = np.mean(data, axis=0)\n",
    "        self.feature_stds = np.std(data, axis=0) + 1e-6\n",
    "        self.normalized_data = (data - self.feature_means) / self.feature_stds\n",
    "        self.actions = []\n",
    "        self.buy_count = 0\n",
    "        self.sell_count = 0\n",
    "        self.hold_count = 0\n",
    "        self.prev_action = 2\n",
    "        print(f\"Data shape: {self.data.shape}, Prices shape: {self.prices.shape}, Labels shape: {self.trend_labels.shape}\")\n",
    "        print(f\"Observation space shape: {self.observation_space.shape}\")\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.cash = self.initial_cash\n",
    "        self.holdings = 0\n",
    "        self.portfolio_values = []\n",
    "        self.actions = []\n",
    "        self.buy_count = 0\n",
    "        self.sell_count = 0\n",
    "        self.hold_count = 0\n",
    "        self.prev_action = 2\n",
    "        return self._get_observation()\n",
    "\n",
    "    def _get_observation(self):\n",
    "        if self.current_step >= len(self.data):\n",
    "            return np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        state = self.normalized_data[self.current_step]\n",
    "        return state.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        current_price = self.prices[self.current_step]\n",
    "        reward = 0\n",
    "        if action == 0 and self.cash >= current_price:\n",
    "            self.holdings += 1\n",
    "            self.cash -= current_price\n",
    "            self.buy_count += 1\n",
    "            reward += 0.1\n",
    "        elif action == 1 and self.holdings > 0:\n",
    "            self.holdings -= 1\n",
    "            self.cash += current_price\n",
    "            self.sell_count += 1\n",
    "            reward += 0.1\n",
    "        else:\n",
    "            self.hold_count += 1\n",
    "            if self.prev_action == 2:\n",
    "                reward -= 0.3\n",
    "            else:\n",
    "                reward -= 0.1\n",
    "        self.actions.append(action)\n",
    "        self.prev_action = action\n",
    "        portfolio_value = self.cash + self.holdings * current_price\n",
    "        self.portfolio_values.append(portfolio_value)\n",
    "        if len(self.portfolio_values) > 1:\n",
    "            portfolio_change = (portfolio_value - self.portfolio_values[-2]) / self.initial_cash\n",
    "            reward += portfolio_change * 100\n",
    "            if len(self.portfolio_values) >= 11:\n",
    "                returns = np.diff(self.portfolio_values[-10:]) / self.portfolio_values[-11:-2]\n",
    "                volatility = np.std(returns)\n",
    "                reward -= volatility * 1\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= min(len(self.data), self.max_steps)\n",
    "        next_state = self._get_observation() if not done else np.zeros_like(self._get_observation())\n",
    "        info = {'portfolio_value': portfolio_value}\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def get_metrics(self):\n",
    "        total_steps = len(self.actions)\n",
    "        predicted_labels = np.array(self.actions)\n",
    "        true_labels = np.argmax(self.trend_labels[:total_steps], axis=1)\n",
    "        accuracy = np.mean(predicted_labels == true_labels) if total_steps > 0 else 0\n",
    "        final_value = self.portfolio_values[-1] if self.portfolio_values else self.initial_cash\n",
    "        sharpe = 0\n",
    "        if len(self.portfolio_values) > 1:\n",
    "            returns = np.diff(self.portfolio_values) / np.array(self.portfolio_values[:-1])\n",
    "            if len(returns) > 0 and np.std(returns) > 0:\n",
    "                sharpe = np.mean(returns) / np.std(returns) * np.sqrt(252)\n",
    "        return {\n",
    "            'final_portfolio_value': final_value,\n",
    "            'profit_loss': final_value - self.initial_cash,\n",
    "            'sharpe_ratio': sharpe,\n",
    "            'buy_count': self.buy_count,\n",
    "            'sell_count': self.sell_count,\n",
    "            'hold_count': self.hold_count,\n",
    "            'action_accuracy': accuracy,\n",
    "            'predicted_labels': predicted_labels,\n",
    "            'true_labels': true_labels\n",
    "        }\n",
    "\n",
    "# LSTM Agent\n",
    "class LSTMAgent:\n",
    "    def __init__(self, sequence_length, num_features, num_wavelet_features=3):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.num_features = num_features\n",
    "        self.num_wavelet_features = num_wavelet_features\n",
    "        self.model = create_lstm_model(\n",
    "            units=128,\n",
    "            dropout_rate=0.5,\n",
    "            sequence_length=sequence_length,\n",
    "            num_features=num_features,\n",
    "            num_wavelet_features=num_wavelet_features\n",
    "        )\n",
    "\n",
    "    def prepare_sequence(self, data, current_step):\n",
    "        if current_step < self.sequence_length:\n",
    "            return np.zeros((self.sequence_length, self.num_features), dtype=np.float32)\n",
    "        sequence = data[max(0, current_step - self.sequence_length):current_step]\n",
    "        if len(sequence) < self.sequence_length:\n",
    "            padding = np.zeros((self.sequence_length - len(sequence), self.num_features), dtype=np.float32)\n",
    "            sequence = np.vstack([padding, sequence])\n",
    "        return sequence\n",
    "\n",
    "    def act(self, sequence):\n",
    "        sequence = np.expand_dims(sequence, axis=0)\n",
    "        class_probs = self.model.predict(sequence, verbose=0)\n",
    "        return np.argmax(class_probs[0])\n",
    "\n",
    "# Plotting functions\n",
    "def plot_confusion_matrix(true_labels, predicted_labels, classes=['UP', 'DOWN', 'NEUTRAL']):\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.title('Confusion Matrix (LSTM)')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.savefig('confusion_matrix_lstm.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_prices(prices, actions, indices):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(indices, prices, label='Normalized Price (Target)', color='blue')\n",
    "    buy_signals = indices[actions == 0]\n",
    "    sell_signals = indices[actions == 1]\n",
    "    buy_prices = prices[actions == 0]\n",
    "    sell_prices = prices[actions == 1]\n",
    "    plt.scatter(buy_signals, buy_prices, color='green', marker='^', label='Buy', alpha=0.6)\n",
    "    plt.scatter(sell_signals, sell_prices, color='red', marker='v', label='Sell', alpha=0.6)\n",
    "    plt.title('Normalized Price with Buy/Sell Signals (LSTM)')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Normalized Price')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('price_plot_lstm.png')\n",
    "    plt.close()\n",
    "\n",
    "# Data preparation\n",
    "data = yf.download('^GSPC', start='2010-01-01', end='2025-04-12')\n",
    "\n",
    "# Validate date range\n",
    "print(f\"Data date range: {data.index[0]} to {data.index[-1]}\")\n",
    "print(f\"Raw data shape: {data.shape}\")\n",
    "\n",
    "# Flatten MultiIndex columns\n",
    "if isinstance(data.columns, pd.MultiIndex):\n",
    "    print(\"MultiIndex detected, flattening columns...\")\n",
    "    data.columns = [col[0] for col in data.columns]\n",
    "print(\"Initial columns:\", data.columns.tolist())\n",
    "\n",
    "# Apply Kalman Filter to smooth Close prices\n",
    "kf = KalmanFilter(\n",
    "    transition_matrices=[1],\n",
    "    observation_matrices=[1],\n",
    "    initial_state_mean=data['Close'].iloc[0],\n",
    "    initial_state_covariance=1.0,\n",
    "    observation_covariance=1.0,\n",
    "    transition_covariance=0.1\n",
    ")\n",
    "smoothed_prices, _ = kf.filter(data['Close'].values)\n",
    "data['Target'] = smoothed_prices\n",
    "\n",
    "# Compute indicators\n",
    "data['SMA_crossover'] = data['Close'].rolling(window=200).mean() - data['Close'].rolling(window=50).mean()\n",
    "\n",
    "def average_true_range(df, window=14):\n",
    "    high_low = df['High'] - df['Low']\n",
    "    high_close = abs(df['High'] - df['Close'].shift(1))\n",
    "    low_close = abs(df['Low'] - df['Close'].shift(1))\n",
    "    true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    df['ATR_14'] = true_range.rolling(window).mean()\n",
    "    return df\n",
    "\n",
    "data = average_true_range(data)\n",
    "\n",
    "def on_balance_volume(df):\n",
    "    df['OBV'] = (np.sign(df['Close'].diff()) * df['Volume']).cumsum()\n",
    "    return df\n",
    "\n",
    "data = on_balance_volume(data)\n",
    "\n",
    "def vwap(df):\n",
    "    df['VWAP'] = (df['Close'] * df['Volume']).cumsum() / df['Volume'].cumsum()\n",
    "    return df\n",
    "\n",
    "data = vwap(data)\n",
    "\n",
    "delta = data['Close'].diff()\n",
    "gain = delta.where(delta > 0, 0)\n",
    "loss = -delta.where(delta < 0, 0)\n",
    "avg_gain = gain.rolling(window=14).mean()\n",
    "avg_loss = loss.rolling(window=14).mean()\n",
    "rs = avg_gain / avg_loss\n",
    "data['RSI_14'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "ema_12 = data['Close'].ewm(span=12, adjust=False).mean()\n",
    "ema_26 = data['Close'].ewm(span=26, adjust=False).mean()\n",
    "data['MACD'] = ema_12 - ema_26\n",
    "\n",
    "data['Return'] = data['Close'].pct_change()\n",
    "data['Volatility'] = data['Return'].rolling(window=10).std()\n",
    "\n",
    "bb_indicator = BollingerBands(close=data['Close'], window=20, window_dev=2)\n",
    "data['BB_bandWidth'] = bb_indicator.bollinger_hband() - bb_indicator.bollinger_lband()\n",
    "\n",
    "data['SMA_17'] = data['Close'].rolling(window=17).mean()\n",
    "data['SMA_5'] = data['Close'].rolling(window=5).mean()\n",
    "data['SMA_9'] = data['Close'].rolling(window=9).mean()\n",
    "\n",
    "data['MACD_Signal'] = data['MACD'].ewm(span=9, adjust=False).mean()\n",
    "data['MACD_Histogram'] = data['MACD'] - data['MACD_Signal']\n",
    "\n",
    "# Add lagged features\n",
    "data['Return_Lag1'] = data['Return'].shift(1)\n",
    "data['Volatility_Lag1'] = data['Volatility'].shift(1)\n",
    "\n",
    "# Wavelet features (3 features, reduced levels)\n",
    "def compute_wavelet_features(prices, wavelet='db4', levels=2):\n",
    "    coeffs = pywt.wavedec(prices, wavelet, level=levels, mode='smooth')\n",
    "    cA2, cD2, cD1 = coeffs[0], coeffs[1], coeffs[2]\n",
    "    min_len = min(len(cA2), len(cD2), len(cD1), len(prices))\n",
    "    return [np.pad(c[:min_len], (0, len(prices) - min_len), mode='constant') for c in [cA2, cD2, cD1]]\n",
    "\n",
    "wavelet_features = compute_wavelet_features(data['Target'].values)\n",
    "for i, name in enumerate(['Wavelet_cA2', 'Wavelet_cD2', 'Wavelet_cD1']):\n",
    "    data[name] = wavelet_features[i]\n",
    "\n",
    "# Normalize features and target\n",
    "features = ['SMA_crossover', 'SMA_17', 'SMA_9', 'SMA_5', 'ATR_14', 'OBV',\n",
    "            'VWAP', 'Volume', 'BB_bandWidth', 'MACD', 'RSI_14', 'MACD_Histogram', 'Volatility',\n",
    "            'Wavelet_cA2', 'Wavelet_cD2', 'Wavelet_cD1', 'Return_Lag1', 'Volatility_Lag1']\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "data[features] = scaler_X.fit_transform(data[features])\n",
    "data['Target'] = scaler_y.fit_transform(data['Target'].values.reshape(-1, 1))\n",
    "\n",
    "# Create labels with adjusted threshold\n",
    "def create_labels(y):\n",
    "    labels = np.zeros((len(y), 3))\n",
    "    for i in range(len(y) - 1):\n",
    "        diff = y[i + 1] - y[i]\n",
    "        if diff > 0.0005:\n",
    "            labels[i, 0] = 1\n",
    "        elif diff < -0.0005:\n",
    "            labels[i, 1] = 1\n",
    "        else:\n",
    "            labels[i, 2] = 1\n",
    "    labels[-1, 2] = 1\n",
    "    return labels\n",
    "\n",
    "# Clean data\n",
    "data.dropna(inplace=True)\n",
    "print(f\"Data shape after dropna: {data.shape}\")\n",
    "\n",
    "# Upsample to 6915 rows (placeholder)\n",
    "if len(data) < 6915:\n",
    "    data = data.reindex(pd.date_range(start=data.index[0], end=data.index[-1], freq='D'))\n",
    "    data = data.interpolate(method='linear')\n",
    "    data = data.dropna()\n",
    "    print(f\"Data shape after upsampling: {data.shape}\")\n",
    "\n",
    "X = data[features].values\n",
    "y = data['Target'].values\n",
    "y_class = create_labels(y)\n",
    "\n",
    "# Debug: Verify shapes and class distribution\n",
    "print(\"Feature data shape:\", X.shape)\n",
    "print(\"Prices shape:\", y.shape)\n",
    "print(\"Trend labels shape:\", y_class.shape)\n",
    "print(\"Class distribution (UP, DOWN, NEUTRAL):\", np.bincount(np.argmax(y_class, axis=1)))\n",
    "\n",
    "# Train-test split\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "y_train_class, y_test_class = y_class[:split], y_class[split:]\n",
    "\n",
    "# Apply SMOTE to training data\n",
    "smote = SMOTE(sampling_strategy={0: 4000, 1: 4000, 2: 8000}, random_state=42)\n",
    "X_train_flat = X_train\n",
    "y_train_class_flat = np.argmax(y_train_class, axis=1)\n",
    "X_train_resampled, y_train_class_resampled = smote.fit_resample(X_train_flat, y_train_class_flat)\n",
    "y_train_class_resampled = np.eye(3)[y_train_class_resampled]\n",
    "\n",
    "# Align y_train_resampled with X_train_resampled\n",
    "n_samples = len(X_train_resampled)\n",
    "y_train_resampled = np.zeros(n_samples)\n",
    "indices = np.arange(len(y_train))\n",
    "sampled_indices = np.random.choice(indices, size=n_samples, replace=True)\n",
    "y_train_resampled = y_train[sampled_indices]\n",
    "\n",
    "# Debug: Verify resampled shapes\n",
    "print(\"X_train_resampled shape:\", X_train_resampled.shape)\n",
    "print(\"y_train_class_resampled shape:\", y_train_class_resampled.shape)\n",
    "print(\"y_train_resampled shape:\", y_train_resampled.shape)\n",
    "\n",
    "# Prepare sequences for LSTM\n",
    "lookback = 15\n",
    "def create_sequences(data, labels, prices, lookback):\n",
    "    if not (len(data) == len(labels) == len(prices)):\n",
    "        raise ValueError(f\"Length mismatch: data={len(data)}, labels={len(labels)}, prices={len(prices)}\")\n",
    "    n_samples = min(len(data), len(labels), len(prices))\n",
    "    sequences = []\n",
    "    seq_labels = []\n",
    "    seq_prices = []\n",
    "    for i in range(lookback, n_samples):\n",
    "        sequences.append(data[i-lookback:i])\n",
    "        seq_labels.append(labels[i])\n",
    "        seq_prices.append(prices[i])\n",
    "    return np.array(sequences), np.array(seq_labels), np.array(seq_prices)\n",
    "\n",
    "# Create sequences\n",
    "X_train_seq, y_train_seq, y_train_prices = create_sequences(X_train_resampled, y_train_class_resampled, y_train_resampled, lookback)\n",
    "X_test_seq, y_test_seq, y_test_prices = create_sequences(X_test, y_test_class, y_test, lookback)\n",
    "\n",
    "# Debug: Verify sequence shapes\n",
    "print(\"Train sequence shape:\", X_train_seq.shape)\n",
    "print(\"Train labels shape:\", y_train_seq.shape)\n",
    "print(\"Train prices shape:\", y_train_prices.shape)\n",
    "print(\"Test sequence shape:\", X_test_seq.shape)\n",
    "print(\"Test labels shape:\", y_test_seq.shape)\n",
    "print(\"Test prices shape:\", y_test_prices.shape)\n",
    "print(\"Resampled class distribution:\", np.bincount(np.argmax(y_train_seq, axis=1)))\n",
    "\n",
    "# Initialize LSTM agent\n",
    "agent = LSTMAgent(sequence_length=lookback, num_features=len(features), num_wavelet_features=3)\n",
    "\n",
    "# Train LSTM model\n",
    "agent.model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Initialize test environment\n",
    "test_env = TradingEnvironment(X_test, features, y_test, y_test_class, max_steps=len(y_test))\n",
    "\n",
    "# Testing phase\n",
    "print(\"\\nTesting...\")\n",
    "state = test_env.reset()\n",
    "total_reward = 0\n",
    "start_time = time.time()\n",
    "for step in range(lookback, len(X_test)):\n",
    "    sequence = test_env.normalized_data[step-lookback:step]\n",
    "    if len(sequence) < lookback:\n",
    "        sequence = np.pad(sequence, ((lookback-len(sequence), 0), (0, 0)), mode='constant')\n",
    "    action = agent.act(sequence)\n",
    "    next_state, reward, done, info = test_env.step(action)\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "    if done:\n",
    "        break\n",
    "metrics = test_env.get_metrics()\n",
    "print(f\"Test Results: Total Reward: {total_reward:.2f}, \"\n",
    "      f\"Portfolio Value: {metrics['final_portfolio_value']:.2f}, \"\n",
    "      f\"Profit/Loss: {metrics['profit_loss']:.2f}, \"\n",
    "      f\"Sharpe: {metrics['sharpe_ratio']:.2f}, \"\n",
    "      f\"Accuracy: {metrics['action_accuracy']:.2f}, \"\n",
    "      f\"Buy Count: {metrics['buy_count']}, \"\n",
    "      f\"Sell Count: {metrics['sell_count']}, \"\n",
    "      f\"Hold Count: {metrics['hold_count']}, \"\n",
    "      f\"Time: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(metrics['true_labels'], metrics['predicted_labels'])\n",
    "\n",
    "# Plot prices with buy/sell signals\n",
    "test_indices = np.arange(len(metrics['predicted_labels']))\n",
    "plot_prices(y_test[lookback:lookback+len(metrics['predicted_labels'])], metrics['predicted_labels'], test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb3cdfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data date range: 2010-01-04 00:00:00 to 2025-04-11 00:00:00\n",
      "Raw data shape: (3843, 5)\n",
      "MultiIndex detected, flattening columns...\n",
      "Initial columns: ['Close', 'High', 'Low', 'Open', 'Volume']\n",
      "Data shape after dropna: (3644, 28)\n",
      "Data shape after upsampling: (5290, 28)\n",
      "Feature data shape: (5290, 21)\n",
      "Prices shape: (5290,)\n",
      "Trend labels shape: (5290, 3)\n",
      "Class distribution (UP, DOWN, NEUTRAL): [  34   72 5184]\n",
      "Test set class distribution (UP, DOWN, NEUTRAL): [  22   36 1000]\n",
      "X_train_resampled shape: (12000, 21)\n",
      "y_train_class_resampled shape: (12000, 3)\n",
      "y_train_resampled shape: (12000,)\n",
      "Train sequence shape: (11980, 20, 21)\n",
      "Train labels shape: (11980, 3)\n",
      "Train prices shape: (11980,)\n",
      "Test sequence shape: (1038, 20, 21)\n",
      "Test labels shape: (1038, 3)\n",
      "Test prices shape: (1038,)\n",
      "Resampled class distribution: [2000 2000 7980]\n",
      "Creating LSTM model with units: 128 and dropout rate: 0.5 for sequence length: 20 and num features: 21\n",
      "Epoch 1/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 124ms/step - accuracy: 0.7318 - loss: 0.1702 - val_accuracy: 1.0000 - val_loss: 0.1074 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 108ms/step - accuracy: 0.9737 - loss: 0.1151 - val_accuracy: 1.0000 - val_loss: 0.0922 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 109ms/step - accuracy: 0.9772 - loss: 0.0970 - val_accuracy: 1.0000 - val_loss: 0.0791 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 115ms/step - accuracy: 0.9786 - loss: 0.0831 - val_accuracy: 1.0000 - val_loss: 0.0677 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 122ms/step - accuracy: 0.9840 - loss: 0.0700 - val_accuracy: 1.0000 - val_loss: 0.0577 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 112ms/step - accuracy: 0.9863 - loss: 0.0597 - val_accuracy: 1.0000 - val_loss: 0.0490 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 114ms/step - accuracy: 0.9890 - loss: 0.0510 - val_accuracy: 1.0000 - val_loss: 0.0414 - learning_rate: 1.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 135ms/step - accuracy: 0.9886 - loss: 0.0434 - val_accuracy: 1.0000 - val_loss: 0.0349 - learning_rate: 1.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 113ms/step - accuracy: 0.9881 - loss: 0.0368 - val_accuracy: 1.0000 - val_loss: 0.0293 - learning_rate: 1.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 102ms/step - accuracy: 0.9897 - loss: 0.0317 - val_accuracy: 1.0000 - val_loss: 0.0244 - learning_rate: 1.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 104ms/step - accuracy: 0.9925 - loss: 0.0260 - val_accuracy: 1.0000 - val_loss: 0.0203 - learning_rate: 1.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 113ms/step - accuracy: 0.9908 - loss: 0.0224 - val_accuracy: 1.0000 - val_loss: 0.0168 - learning_rate: 1.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 110ms/step - accuracy: 0.9912 - loss: 0.0189 - val_accuracy: 1.0000 - val_loss: 0.0139 - learning_rate: 1.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 109ms/step - accuracy: 0.9927 - loss: 0.0162 - val_accuracy: 1.0000 - val_loss: 0.0115 - learning_rate: 1.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 109ms/step - accuracy: 0.9918 - loss: 0.0140 - val_accuracy: 1.0000 - val_loss: 0.0095 - learning_rate: 1.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 106ms/step - accuracy: 0.9937 - loss: 0.0117 - val_accuracy: 1.0000 - val_loss: 0.0078 - learning_rate: 1.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 106ms/step - accuracy: 0.9938 - loss: 0.0105 - val_accuracy: 1.0000 - val_loss: 0.0065 - learning_rate: 1.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 105ms/step - accuracy: 0.9936 - loss: 0.0089 - val_accuracy: 1.0000 - val_loss: 0.0055 - learning_rate: 1.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 110ms/step - accuracy: 0.9952 - loss: 0.0076 - val_accuracy: 1.0000 - val_loss: 0.0046 - learning_rate: 1.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 194ms/step - accuracy: 0.9926 - loss: 0.0072 - val_accuracy: 1.0000 - val_loss: 0.0040 - learning_rate: 1.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 139ms/step - accuracy: 0.9941 - loss: 0.0066 - val_accuracy: 1.0000 - val_loss: 0.0036 - learning_rate: 1.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 308ms/step - accuracy: 0.9930 - loss: 0.0064 - val_accuracy: 1.0000 - val_loss: 0.0032 - learning_rate: 1.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 331ms/step - accuracy: 0.9927 - loss: 0.0061 - val_accuracy: 1.0000 - val_loss: 0.0030 - learning_rate: 1.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 188ms/step - accuracy: 0.9919 - loss: 0.0061 - val_accuracy: 1.0000 - val_loss: 0.0027 - learning_rate: 1.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 151ms/step - accuracy: 0.9930 - loss: 0.0058 - val_accuracy: 1.0000 - val_loss: 0.0026 - learning_rate: 1.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 131ms/step - accuracy: 0.9953 - loss: 0.0049 - val_accuracy: 1.0000 - val_loss: 0.0025 - learning_rate: 1.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 197ms/step - accuracy: 0.9950 - loss: 0.0047 - val_accuracy: 1.0000 - val_loss: 0.0024 - learning_rate: 1.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 128ms/step - accuracy: 0.9919 - loss: 0.0053 - val_accuracy: 1.0000 - val_loss: 0.0023 - learning_rate: 1.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 106ms/step - accuracy: 0.9939 - loss: 0.0049 - val_accuracy: 1.0000 - val_loss: 0.0023 - learning_rate: 1.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 103ms/step - accuracy: 0.9868 - loss: 0.0064 - val_accuracy: 1.0000 - val_loss: 0.0022 - learning_rate: 1.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 105ms/step - accuracy: 0.9944 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 0.0022 - learning_rate: 1.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 119ms/step - accuracy: 0.9948 - loss: 0.0047 - val_accuracy: 1.0000 - val_loss: 0.0022 - learning_rate: 1.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 121ms/step - accuracy: 0.9942 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 0.0022 - learning_rate: 1.0000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 106ms/step - accuracy: 0.9935 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 0.0022 - learning_rate: 1.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 105ms/step - accuracy: 0.9943 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 0.0021 - learning_rate: 5.0000e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 105ms/step - accuracy: 0.9939 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 0.0022 - learning_rate: 5.0000e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 108ms/step - accuracy: 0.9939 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 0.0021 - learning_rate: 5.0000e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 135ms/step - accuracy: 0.9959 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 0.0021 - learning_rate: 5.0000e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 134ms/step - accuracy: 0.9940 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 0.0021 - learning_rate: 5.0000e-05\n",
      "Epoch 40/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 127ms/step - accuracy: 0.9949 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 0.0021 - learning_rate: 5.0000e-05\n",
      "Epoch 41/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 131ms/step - accuracy: 0.9945 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 0.0021 - learning_rate: 2.5000e-05\n",
      "Epoch 42/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 151ms/step - accuracy: 0.9948 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 0.0021 - learning_rate: 2.5000e-05\n",
      "Epoch 43/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 127ms/step - accuracy: 0.9950 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 0.0021 - learning_rate: 2.5000e-05\n",
      "Epoch 44/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 125ms/step - accuracy: 0.9928 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 0.0021 - learning_rate: 2.5000e-05\n",
      "Epoch 45/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 140ms/step - accuracy: 0.9940 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 0.0021 - learning_rate: 2.5000e-05\n",
      "Epoch 46/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 131ms/step - accuracy: 0.9936 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 0.0021 - learning_rate: 1.2500e-05\n",
      "Epoch 47/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 141ms/step - accuracy: 0.9942 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 0.0021 - learning_rate: 1.2500e-05\n",
      "Epoch 48/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 169ms/step - accuracy: 0.9949 - loss: 0.0043 - val_accuracy: 1.0000 - val_loss: 0.0021 - learning_rate: 1.2500e-05\n",
      "Epoch 49/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 130ms/step - accuracy: 0.9942 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 0.0021 - learning_rate: 1.2500e-05\n",
      "Epoch 50/50\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 129ms/step - accuracy: 0.9955 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 0.0021 - learning_rate: 1.2500e-05\n",
      "Data shape: (1058, 21), Prices shape: (1058,), Labels shape: (1058, 3)\n",
      "Observation space shape: (21,)\n",
      "\n",
      "Testing...\n",
      "Test Results: Total Reward: -78.67, Portfolio Value: 10006.54, Profit/Loss: 6.54, Sharpe: 3.82, Accuracy: 0.75, Buy Count: 65, Sell Count: 65, Hold Count: 908, Time: 50.27s\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, GRU, Dense, Dropout, Bidirectional\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import yfinance as yf\n",
    "from ta.volatility import BollingerBands\n",
    "from pykalman import KalmanFilter\n",
    "import pywt\n",
    "import time\n",
    "\n",
    "# Focal Loss for Classification with Class Weights\n",
    "def focal_loss(gamma=3.0, alpha=0.5):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0 - 1e-7)\n",
    "        pt = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        alpha_t = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "        focal_loss = -tf.reduce_mean(alpha_t * tf.pow(1.0 - pt, gamma) * tf.math.log(pt))\n",
    "        weights = tf.reduce_sum(y_true * tf.constant([2.0, 2.0, 1.0]), axis=-1)\n",
    "        return focal_loss * weights\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# Improved Attention Layer\n",
    "class ImprovedAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads=4):\n",
    "        super(ImprovedAttentionLayer, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        assert self.d_model % self.num_heads == 0\n",
    "        self.depth = self.d_model // self.num_heads\n",
    "        self.Wq = self.add_weight(name='Wq', shape=(self.d_model, self.d_model), initializer='glorot_uniform')\n",
    "        self.Wk = self.add_weight(name='Wk', shape=(self.d_model, self.d_model), initializer='glorot_uniform')\n",
    "        self.Wv = self.add_weight(name='Wv', shape=(self.d_model, self.d_model), initializer='glorot_uniform')\n",
    "        self.Wo = self.add_weight(name='Wo', shape=(self.d_model, self.d_model), initializer='glorot_uniform')\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        Q = tf.matmul(inputs, self.Wq)\n",
    "        K = tf.matmul(inputs, self.Wk)\n",
    "        V = tf.matmul(inputs, self.Wv)\n",
    "        Q = self.split_heads(Q, batch_size)\n",
    "        K = self.split_heads(K, batch_size)\n",
    "        V = self.split_heads(V, batch_size)\n",
    "        dk = tf.cast(self.depth, tf.float32)\n",
    "        score = tf.matmul(Q, K, transpose_b=True) / tf.math.sqrt(dk)\n",
    "        attention_weights = tf.nn.softmax(score, axis=-1)\n",
    "        context = tf.matmul(attention_weights, V)\n",
    "        context = tf.transpose(context, perm=[0, 2, 1, 3])\n",
    "        context = tf.reshape(context, (batch_size, -1, self.d_model))\n",
    "        output = tf.matmul(context, self.Wo)\n",
    "        return output\n",
    "\n",
    "# Simplified LSTM Model (Single Branch)\n",
    "def create_lstm_model(units, dropout_rate, sequence_length, num_features):\n",
    "    print(\"Creating LSTM model with units:\", units, \"and dropout rate:\", dropout_rate,\n",
    "          \"for sequence length:\", sequence_length, \"and num features:\", num_features)\n",
    "    inputs = Input(shape=(sequence_length, num_features))\n",
    "    x = Bidirectional(LSTM(units, return_sequences=True))(inputs)\n",
    "    x = ImprovedAttentionLayer()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = GRU(64, return_sequences=False)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    class_output = Dense(3, activation='softmax', name='class', kernel_regularizer=tf.keras.regularizers.l2(0.02))(x)\n",
    "    model = Model(inputs, outputs=class_output)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001, clipvalue=0.5)\n",
    "    model.compile(optimizer=optimizer, loss=focal_loss(gamma=3.0, alpha=0.5), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# TradingEnvironment\n",
    "class TradingEnvironment(gym.Env):\n",
    "    def __init__(self, data, features, prices, trend_labels, initial_cash=10000, max_steps=1000):\n",
    "        super(TradingEnvironment, self).__init__()\n",
    "        self.data = data\n",
    "        self.features = features\n",
    "        self.prices = prices\n",
    "        self.trend_labels = np.array(trend_labels)\n",
    "        self.initial_cash = initial_cash\n",
    "        self.max_steps = max_steps\n",
    "        self.current_step = 0\n",
    "        self.cash = initial_cash\n",
    "        self.holdings = 0\n",
    "        self.portfolio_values = []\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf,\n",
    "            shape=(len(features),),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        self.feature_means = np.mean(data, axis=0)\n",
    "        self.feature_stds = np.std(data, axis=0) + 1e-6\n",
    "        self.normalized_data = (data - self.feature_means) / self.feature_stds\n",
    "        self.actions = []\n",
    "        self.buy_count = 0\n",
    "        self.sell_count = 0\n",
    "        self.hold_count = 0\n",
    "        self.prev_action = 2\n",
    "        print(f\"Data shape: {self.data.shape}, Prices shape: {self.prices.shape}, Labels shape: {self.trend_labels.shape}\")\n",
    "        print(f\"Observation space shape: {self.observation_space.shape}\")\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.cash = self.initial_cash\n",
    "        self.holdings = 0\n",
    "        self.portfolio_values = []\n",
    "        self.actions = []\n",
    "        self.buy_count = 0\n",
    "        self.sell_count = 0\n",
    "        self.hold_count = 0\n",
    "        self.prev_action = 2\n",
    "        return self._get_observation()\n",
    "\n",
    "    def _get_observation(self):\n",
    "        if self.current_step >= len(self.data):\n",
    "            return np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        state = self.normalized_data[self.current_step]\n",
    "        return state.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        current_price = self.prices[self.current_step]\n",
    "        reward = 0\n",
    "        if action == 0 and self.cash >= current_price:\n",
    "            self.holdings += 1\n",
    "            self.cash -= current_price\n",
    "            self.buy_count += 1\n",
    "            reward += 0.05\n",
    "        elif action == 1 and self.holdings > 0:\n",
    "            self.holdings -= 1\n",
    "            self.cash += current_price\n",
    "            self.sell_count += 1\n",
    "            reward += 0.05\n",
    "        else:\n",
    "            self.hold_count += 1\n",
    "            if self.prev_action == 2:\n",
    "                reward -= 0.1\n",
    "            else:\n",
    "                reward -= 0.05\n",
    "        self.actions.append(action)\n",
    "        self.prev_action = action\n",
    "        portfolio_value = self.cash + self.holdings * current_price\n",
    "        self.portfolio_values.append(portfolio_value)\n",
    "        if len(self.portfolio_values) > 1:\n",
    "            portfolio_change = (portfolio_value - self.portfolio_values[-2]) / self.initial_cash\n",
    "            reward += portfolio_change * 200\n",
    "            if len(self.portfolio_values) >= 11:\n",
    "                returns = np.diff(self.portfolio_values[-10:]) / self.portfolio_values[-11:-2]\n",
    "                volatility = np.std(returns)\n",
    "                reward -= volatility * 0.5\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= min(len(self.data), self.max_steps)\n",
    "        next_state = self._get_observation() if not done else np.zeros_like(self._get_observation())\n",
    "        info = {'portfolio_value': portfolio_value}\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def get_metrics(self):\n",
    "        total_steps = len(self.actions)\n",
    "        predicted_labels = np.array(self.actions)\n",
    "        true_labels = np.argmax(self.trend_labels[:total_steps], axis=1)\n",
    "        accuracy = np.mean(predicted_labels == true_labels) if total_steps > 0 else 0\n",
    "        final_value = self.portfolio_values[-1] if self.portfolio_values else self.initial_cash\n",
    "        sharpe = 0\n",
    "        if len(self.portfolio_values) > 1:\n",
    "            returns = np.diff(self.portfolio_values) / np.array(self.portfolio_values[:-1])\n",
    "            if len(returns) > 0 and np.std(returns) > 0:\n",
    "                sharpe = np.mean(returns) / np.std(returns) * np.sqrt(252)\n",
    "        return {\n",
    "            'final_portfolio_value': final_value,\n",
    "            'profit_loss': final_value - self.initial_cash,\n",
    "            'sharpe_ratio': sharpe,\n",
    "            'buy_count': self.buy_count,\n",
    "            'sell_count': self.sell_count,\n",
    "            'hold_count': self.hold_count,\n",
    "            'action_accuracy': accuracy,\n",
    "            'predicted_labels': predicted_labels,\n",
    "            'true_labels': true_labels\n",
    "        }\n",
    "\n",
    "# LSTM Agent\n",
    "class LSTMAgent:\n",
    "    def __init__(self, sequence_length, num_features):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.num_features = num_features\n",
    "        self.model = create_lstm_model(\n",
    "            units=128,\n",
    "            dropout_rate=0.5,\n",
    "            sequence_length=sequence_length,\n",
    "            num_features=num_features\n",
    "        )\n",
    "\n",
    "    def prepare_sequence(self, data, current_step):\n",
    "        if current_step < self.sequence_length:\n",
    "            return np.zeros((self.sequence_length, self.num_features), dtype=np.float32)\n",
    "        sequence = data[max(0, current_step - self.sequence_length):current_step]\n",
    "        if len(sequence) < self.sequence_length:\n",
    "            padding = np.zeros((self.sequence_length - len(sequence), self.num_features), dtype=np.float32)\n",
    "            sequence = np.vstack([padding, sequence])\n",
    "        return sequence\n",
    "\n",
    "    def act(self, sequence):\n",
    "        sequence = np.expand_dims(sequence, axis=0)\n",
    "        class_probs = self.model.predict(sequence, verbose=0)\n",
    "        return np.argmax(class_probs[0])\n",
    "\n",
    "# Plotting functions\n",
    "def plot_confusion_matrix(true_labels, predicted_labels, classes=['UP', 'DOWN', 'NEUTRAL']):\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.title('Confusion Matrix (LSTM)')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.savefig('confusion_matrix_lstm.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_prices(prices, actions, indices):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(indices, prices, label='Normalized Price (Target)', color='blue')\n",
    "    buy_signals = indices[actions == 0]\n",
    "    sell_signals = indices[actions == 1]\n",
    "    buy_prices = prices[actions == 0]\n",
    "    sell_prices = prices[actions == 1]\n",
    "    plt.scatter(buy_signals, buy_prices, color='green', marker='^', label='Buy', alpha=0.6)\n",
    "    plt.scatter(sell_signals, sell_prices, color='red', marker='v', label='Sell', alpha=0.6)\n",
    "    plt.title('Normalized Price with Buy/Sell Signals (LSTM)')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Normalized Price')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('price_plot_lstm.png')\n",
    "    plt.close()\n",
    "\n",
    "# Data preparation\n",
    "data = yf.download('^GSPC', start='2010-01-01', end='2025-04-12')\n",
    "\n",
    "# Validate date range\n",
    "print(f\"Data date range: {data.index[0]} to {data.index[-1]}\")\n",
    "print(f\"Raw data shape: {data.shape}\")\n",
    "\n",
    "# Flatten MultiIndex columns\n",
    "if isinstance(data.columns, pd.MultiIndex):\n",
    "    print(\"MultiIndex detected, flattening columns...\")\n",
    "    data.columns = [col[0] for col in data.columns]\n",
    "print(\"Initial columns:\", data.columns.tolist())\n",
    "\n",
    "# Apply Kalman Filter to smooth Close prices\n",
    "kf = KalmanFilter(\n",
    "    transition_matrices=[1],\n",
    "    observation_matrices=[1],\n",
    "    initial_state_mean=data['Close'].iloc[0],\n",
    "    initial_state_covariance=1.0,\n",
    "    observation_covariance=1.0,\n",
    "    transition_covariance=0.1\n",
    ")\n",
    "smoothed_prices, _ = kf.filter(data['Close'].values)\n",
    "data['Target'] = smoothed_prices\n",
    "\n",
    "# Compute indicators\n",
    "data['SMA_crossover'] = data['Close'].rolling(window=200).mean() - data['Close'].rolling(window=50).mean()\n",
    "\n",
    "def average_true_range(df, window=14):\n",
    "    high_low = df['High'] - df['Low']\n",
    "    high_close = abs(df['High'] - df['Close'].shift(1))\n",
    "    low_close = abs(df['Low'] - df['Close'].shift(1))\n",
    "    true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    df['ATR_14'] = true_range.rolling(window).mean()\n",
    "    return df\n",
    "\n",
    "data = average_true_range(data)\n",
    "\n",
    "def on_balance_volume(df):\n",
    "    df['OBV'] = (np.sign(df['Close'].diff()) * df['Volume']).cumsum()\n",
    "    return df\n",
    "\n",
    "data = on_balance_volume(data)\n",
    "\n",
    "def vwap(df):\n",
    "    df['VWAP'] = (df['Close'] * df['Volume']).cumsum() / df['Volume'].cumsum()\n",
    "    return df\n",
    "\n",
    "data = vwap(data)\n",
    "\n",
    "delta = data['Close'].diff()\n",
    "gain = delta.where(delta > 0, 0)\n",
    "loss = -delta.where(delta < 0, 0)\n",
    "avg_gain = gain.rolling(window=14).mean()\n",
    "avg_loss = loss.rolling(window=14).mean()\n",
    "rs = avg_gain / avg_loss\n",
    "data['RSI_14'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "ema_12 = data['Close'].ewm(span=12, adjust=False).mean()\n",
    "ema_26 = data['Close'].ewm(span=26, adjust=False).mean()\n",
    "data['MACD'] = ema_12 - ema_26\n",
    "\n",
    "data['Return'] = data['Close'].pct_change()\n",
    "data['Volatility'] = data['Return'].rolling(window=10).std()\n",
    "\n",
    "bb_indicator = BollingerBands(close=data['Close'], window=20, window_dev=2)\n",
    "data['BB_bandWidth'] = bb_indicator.bollinger_hband() - bb_indicator.bollinger_lband()\n",
    "\n",
    "data['SMA_17'] = data['Close'].rolling(window=17).mean()\n",
    "data['SMA_5'] = data['Close'].rolling(window=5).mean()\n",
    "data['SMA_9'] = data['Close'].rolling(window=9).mean()\n",
    "\n",
    "data['MACD_Signal'] = data['MACD'].ewm(span=9, adjust=False).mean()\n",
    "data['MACD_Histogram'] = data['MACD'] - data['MACD_Signal']\n",
    "\n",
    "# Add lagged features\n",
    "data['Return_Lag1'] = data['Return'].shift(1)\n",
    "data['Volatility_Lag1'] = data['Volatility'].shift(1)\n",
    "data['Return_Lag2'] = data['Return'].shift(2)\n",
    "data['Volatility_Lag2'] = data['Volatility'].shift(2)\n",
    "data['VIX'] = data['Volatility'].rolling(window=20).mean()  # Placeholder for VIX\n",
    "\n",
    "# Wavelet features (3 features)\n",
    "def compute_wavelet_features(prices, wavelet='db4', levels=2):\n",
    "    coeffs = pywt.wavedec(prices, wavelet, level=levels, mode='smooth')\n",
    "    cA2, cD2, cD1 = coeffs[0], coeffs[1], coeffs[2]\n",
    "    min_len = min(len(cA2), len(cD2), len(cD1), len(prices))\n",
    "    return [np.pad(c[:min_len], (0, len(prices) - min_len), mode='constant') for c in [cA2, cD2, cD1]]\n",
    "\n",
    "wavelet_features = compute_wavelet_features(data['Target'].values)\n",
    "for i, name in enumerate(['Wavelet_cA2', 'Wavelet_cD2', 'Wavelet_cD1']):\n",
    "    data[name] = wavelet_features[i]\n",
    "\n",
    "# Normalize features and target\n",
    "features = ['SMA_crossover', 'SMA_17', 'SMA_9', 'SMA_5', 'ATR_14', 'OBV',\n",
    "            'VWAP', 'Volume', 'BB_bandWidth', 'MACD', 'RSI_14', 'MACD_Histogram', 'Volatility',\n",
    "            'Wavelet_cA2', 'Wavelet_cD2', 'Wavelet_cD1', 'Return_Lag1', 'Volatility_Lag1',\n",
    "            'Return_Lag2', 'Volatility_Lag2', 'VIX']\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "data[features] = scaler_X.fit_transform(data[features])\n",
    "data['Target'] = scaler_y.fit_transform(data['Target'].values.reshape(-1, 1))\n",
    "\n",
    "# Create labels with adjusted threshold\n",
    "def create_labels(y):\n",
    "    labels = np.zeros((len(y), 3))\n",
    "    for i in range(len(y) - 1):\n",
    "        diff = y[i + 1] - y[i]\n",
    "        if diff > 0.006:\n",
    "            labels[i, 0] = 1\n",
    "        elif diff < -0.006:\n",
    "            labels[i, 1] = 1\n",
    "        else:\n",
    "            labels[i, 2] = 1\n",
    "    labels[-1, 2] = 1\n",
    "    return labels\n",
    "\n",
    "# Clean data\n",
    "data.dropna(inplace=True)\n",
    "print(f\"Data shape after dropna: {data.shape}\")\n",
    "\n",
    "# Upsample to 6915 rows (placeholder)\n",
    "if len(data) < 6915:\n",
    "    data = data.reindex(pd.date_range(start=data.index[0], end=data.index[-1], freq='D'))\n",
    "    data = data.interpolate(method='linear')\n",
    "    data = data.dropna()\n",
    "    print(f\"Data shape after upsampling: {data.shape}\")\n",
    "\n",
    "X = data[features].values\n",
    "y = data['Target'].values\n",
    "y_class = create_labels(y)\n",
    "\n",
    "# Debug: Verify shapes and class distribution\n",
    "print(\"Feature data shape:\", X.shape)\n",
    "print(\"Prices shape:\", y.shape)\n",
    "print(\"Trend labels shape:\", y_class.shape)\n",
    "print(\"Class distribution (UP, DOWN, NEUTRAL):\", np.bincount(np.argmax(y_class, axis=1)))\n",
    "\n",
    "# Train-test split\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "y_train_class, y_test_class = y_class[:split], y_class[split:]\n",
    "\n",
    "# Debug: Test set class distribution\n",
    "print(\"Test set class distribution (UP, DOWN, NEUTRAL):\", np.bincount(np.argmax(y_test_class, axis=1)))\n",
    "\n",
    "# Apply SMOTE to training data\n",
    "smote = SMOTE(sampling_strategy={0: 2000, 1: 2000, 2: 8000}, random_state=42)\n",
    "X_train_flat = X_train\n",
    "y_train_class_flat = np.argmax(y_train_class, axis=1)\n",
    "X_train_resampled, y_train_class_resampled = smote.fit_resample(X_train_flat, y_train_class_flat)\n",
    "y_train_class_resampled = np.eye(3)[y_train_class_resampled]\n",
    "\n",
    "# Align y_train_resampled with X_train_resampled\n",
    "n_samples = len(X_train_resampled)\n",
    "y_train_resampled = np.zeros(n_samples)\n",
    "indices = np.arange(len(y_train))\n",
    "sampled_indices = np.random.choice(indices, size=n_samples, replace=True)\n",
    "y_train_resampled = y_train[sampled_indices]\n",
    "\n",
    "# Debug: Verify resampled shapes\n",
    "print(\"X_train_resampled shape:\", X_train_resampled.shape)\n",
    "print(\"y_train_class_resampled shape:\", y_train_class_resampled.shape)\n",
    "print(\"y_train_resampled shape:\", y_train_resampled.shape)\n",
    "\n",
    "# Prepare sequences for LSTM\n",
    "lookback = 20\n",
    "def create_sequences(data, labels, prices, lookback):\n",
    "    if not (len(data) == len(labels) == len(prices)):\n",
    "        raise ValueError(f\"Length mismatch: data={len(data)}, labels={len(labels)}, prices={len(prices)}\")\n",
    "    n_samples = min(len(data), len(labels), len(prices))\n",
    "    sequences = []\n",
    "    seq_labels = []\n",
    "    seq_prices = []\n",
    "    for i in range(lookback, n_samples):\n",
    "        sequences.append(data[i-lookback:i])\n",
    "        seq_labels.append(labels[i])\n",
    "        seq_prices.append(prices[i])\n",
    "    return np.array(sequences), np.array(seq_labels), np.array(seq_prices)\n",
    "\n",
    "# Create sequences\n",
    "X_train_seq, y_train_seq, y_train_prices = create_sequences(X_train_resampled, y_train_class_resampled, y_train_resampled, lookback)\n",
    "X_test_seq, y_test_seq, y_test_prices = create_sequences(X_test, y_test_class, y_test, lookback)\n",
    "\n",
    "# Debug: Verify sequence shapes\n",
    "print(\"Train sequence shape:\", X_train_seq.shape)\n",
    "print(\"Train labels shape:\", y_train_seq.shape)\n",
    "print(\"Train prices shape:\", y_train_prices.shape)\n",
    "print(\"Test sequence shape:\", X_test_seq.shape)\n",
    "print(\"Test labels shape:\", y_test_seq.shape)\n",
    "print(\"Test prices shape:\", y_test_prices.shape)\n",
    "print(\"Resampled class distribution:\", np.bincount(np.argmax(y_train_seq, axis=1)))\n",
    "\n",
    "# Initialize LSTM agent\n",
    "agent = LSTMAgent(sequence_length=lookback, num_features=len(features))\n",
    "\n",
    "# Train LSTM model\n",
    "agent.model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Initialize test environment\n",
    "test_env = TradingEnvironment(X_test, features, y_test, y_test_class, max_steps=len(y_test))\n",
    "\n",
    "# Testing phase\n",
    "print(\"\\nTesting...\")\n",
    "state = test_env.reset()\n",
    "total_reward = 0\n",
    "start_time = time.time()\n",
    "for step in range(lookback, len(X_test)):\n",
    "    sequence = test_env.normalized_data[step-lookback:step]\n",
    "    if len(sequence) < lookback:\n",
    "        sequence = np.pad(sequence, ((lookback-len(sequence), 0), (0, 0)), mode='constant')\n",
    "    action = agent.act(sequence)\n",
    "    next_state, reward, done, info = test_env.step(action)\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "    if done:\n",
    "        break\n",
    "metrics = test_env.get_metrics()\n",
    "print(f\"Test Results: Total Reward: {total_reward:.2f}, \"\n",
    "      f\"Portfolio Value: {metrics['final_portfolio_value']:.2f}, \"\n",
    "      f\"Profit/Loss: {metrics['profit_loss']:.2f}, \"\n",
    "      f\"Sharpe: {metrics['sharpe_ratio']:.2f}, \"\n",
    "      f\"Accuracy: {metrics['action_accuracy']:.2f}, \"\n",
    "      f\"Buy Count: {metrics['buy_count']}, \"\n",
    "      f\"Sell Count: {metrics['sell_count']}, \"\n",
    "      f\"Hold Count: {metrics['hold_count']}, \"\n",
    "      f\"Time: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(metrics['true_labels'], metrics['predicted_labels'])\n",
    "\n",
    "# Plot prices with buy/sell signals\n",
    "test_indices = np.arange(len(metrics['predicted_labels']))\n",
    "plot_prices(y_test[lookback:lookback+len(metrics['predicted_labels'])], metrics['predicted_labels'], test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1886698d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data date range: 2010-01-04 00:00:00 to 2025-04-11 00:00:00\n",
      "Raw data shape: (3843, 5)\n",
      "MultiIndex detected, flattening columns...\n",
      "Initial columns: ['Close', 'High', 'Low', 'Open', 'Volume']\n",
      "Target min (original): 1040.0108244311277, Target max (original): 6111.065995579977\n",
      "Data shape after dropna: (3644, 28)\n",
      "Data shape after upsampling: (5290, 28)\n",
      "Feature data shape: (5290, 21)\n",
      "Prices shape: (5290,)\n",
      "Trend labels shape: (5290, 3)\n",
      "Class distribution (UP, DOWN, NEUTRAL): [  60  106 5124]\n",
      "Test set class distribution (UP, DOWN, NEUTRAL): [ 39  53 966]\n",
      "X_train_resampled shape: (12000, 21)\n",
      "y_train_class_resampled shape: (12000, 3)\n",
      "y_train_resampled shape: (12000,)\n",
      "Train sequence shape: (11980, 20, 21)\n",
      "Train labels shape: (11980, 3)\n",
      "Train prices shape: (11980,)\n",
      "Test sequence shape: (1038, 20, 21)\n",
      "Test labels shape: (1038, 3)\n",
      "Test prices shape: (1038,)\n",
      "Resampled class distribution: [2000 2000 7980]\n",
      "Creating LSTM model with units: 128 and dropout rate: 0.5 for sequence length: 20 and num features: 21\n",
      "Epoch 1/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 73ms/step - accuracy: 0.8588 - loss: 0.1290 - val_accuracy: 1.0000 - val_loss: 0.0628 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 93ms/step - accuracy: 0.9721 - loss: 0.0649 - val_accuracy: 1.0000 - val_loss: 0.0347 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 96ms/step - accuracy: 0.9766 - loss: 0.0385 - val_accuracy: 1.0000 - val_loss: 0.0188 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 101ms/step - accuracy: 0.9814 - loss: 0.0222 - val_accuracy: 1.0000 - val_loss: 0.0099 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 89ms/step - accuracy: 0.9868 - loss: 0.0134 - val_accuracy: 1.0000 - val_loss: 0.0056 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 87ms/step - accuracy: 0.9858 - loss: 0.0095 - val_accuracy: 1.0000 - val_loss: 0.0036 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 86ms/step - accuracy: 0.9877 - loss: 0.0077 - val_accuracy: 1.0000 - val_loss: 0.0029 - learning_rate: 1.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 105ms/step - accuracy: 0.9864 - loss: 0.0075 - val_accuracy: 1.0000 - val_loss: 0.0025 - learning_rate: 1.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 112ms/step - accuracy: 0.9879 - loss: 0.0068 - val_accuracy: 1.0000 - val_loss: 0.0023 - learning_rate: 1.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 105ms/step - accuracy: 0.9880 - loss: 0.0066 - val_accuracy: 1.0000 - val_loss: 0.0023 - learning_rate: 1.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 82ms/step - accuracy: 0.9872 - loss: 0.0066 - val_accuracy: 1.0000 - val_loss: 0.0022 - learning_rate: 1.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 154ms/step - accuracy: 0.9892 - loss: 0.0062 - val_accuracy: 1.0000 - val_loss: 0.0023 - learning_rate: 1.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 88ms/step - accuracy: 0.9880 - loss: 0.0061 - val_accuracy: 1.0000 - val_loss: 0.0023 - learning_rate: 1.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 98ms/step - accuracy: 0.9883 - loss: 0.0058 - val_accuracy: 1.0000 - val_loss: 0.0022 - learning_rate: 1.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 155ms/step - accuracy: 0.9881 - loss: 0.0059 - val_accuracy: 1.0000 - val_loss: 0.0022 - learning_rate: 1.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 120ms/step - accuracy: 0.9892 - loss: 0.0055 - val_accuracy: 1.0000 - val_loss: 0.0022 - learning_rate: 1.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 137ms/step - accuracy: 0.9896 - loss: 0.0057 - val_accuracy: 1.0000 - val_loss: 0.0022 - learning_rate: 5.0000e-05\n",
      "Epoch 18/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 129ms/step - accuracy: 0.9914 - loss: 0.0052 - val_accuracy: 1.0000 - val_loss: 0.0022 - learning_rate: 5.0000e-05\n",
      "Epoch 19/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 131ms/step - accuracy: 0.9896 - loss: 0.0060 - val_accuracy: 1.0000 - val_loss: 0.0022 - learning_rate: 5.0000e-05\n",
      "Epoch 20/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 77ms/step - accuracy: 0.9925 - loss: 0.0050 - val_accuracy: 1.0000 - val_loss: 0.0022 - learning_rate: 5.0000e-05\n",
      "Epoch 21/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 110ms/step - accuracy: 0.9902 - loss: 0.0058 - val_accuracy: 1.0000 - val_loss: 0.0022 - learning_rate: 5.0000e-05\n",
      "Epoch 22/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 120ms/step - accuracy: 0.9919 - loss: 0.0055 - val_accuracy: 1.0000 - val_loss: 0.0022 - learning_rate: 2.5000e-05\n",
      "Epoch 23/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 132ms/step - accuracy: 0.9906 - loss: 0.0051 - val_accuracy: 1.0000 - val_loss: 0.0021 - learning_rate: 2.5000e-05\n",
      "Epoch 24/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 96ms/step - accuracy: 0.9921 - loss: 0.0052 - val_accuracy: 1.0000 - val_loss: 0.0021 - learning_rate: 2.5000e-05\n",
      "Epoch 25/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 93ms/step - accuracy: 0.9918 - loss: 0.0050 - val_accuracy: 1.0000 - val_loss: 0.0022 - learning_rate: 2.5000e-05\n",
      "Epoch 26/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 78ms/step - accuracy: 0.9928 - loss: 0.0051 - val_accuracy: 1.0000 - val_loss: 0.0021 - learning_rate: 2.5000e-05\n",
      "Epoch 27/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 140ms/step - accuracy: 0.9911 - loss: 0.0054 - val_accuracy: 1.0000 - val_loss: 0.0021 - learning_rate: 1.2500e-05\n",
      "Epoch 28/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 168ms/step - accuracy: 0.9914 - loss: 0.0049 - val_accuracy: 1.0000 - val_loss: 0.0021 - learning_rate: 1.2500e-05\n",
      "Epoch 29/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 139ms/step - accuracy: 0.9926 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 0.0021 - learning_rate: 1.2500e-05\n",
      "Epoch 30/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 199ms/step - accuracy: 0.9926 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 0.0021 - learning_rate: 1.2500e-05\n",
      "Epoch 31/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 133ms/step - accuracy: 0.9907 - loss: 0.0053 - val_accuracy: 1.0000 - val_loss: 0.0021 - learning_rate: 1.2500e-05\n",
      "Epoch 32/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 178ms/step - accuracy: 0.9914 - loss: 0.0052 - val_accuracy: 1.0000 - val_loss: 0.0021 - learning_rate: 6.2500e-06\n",
      "Epoch 33/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 153ms/step - accuracy: 0.9922 - loss: 0.0049 - val_accuracy: 1.0000 - val_loss: 0.0021 - learning_rate: 6.2500e-06\n",
      "Epoch 34/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 144ms/step - accuracy: 0.9920 - loss: 0.0049 - val_accuracy: 1.0000 - val_loss: 0.0021 - learning_rate: 6.2500e-06\n",
      "Epoch 35/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 116ms/step - accuracy: 0.9915 - loss: 0.0050 - val_accuracy: 1.0000 - val_loss: 0.0021 - learning_rate: 6.2500e-06\n",
      "Epoch 36/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 75ms/step - accuracy: 0.9919 - loss: 0.0052 - val_accuracy: 1.0000 - val_loss: 0.0021 - learning_rate: 6.2500e-06\n",
      "Epoch 37/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 71ms/step - accuracy: 0.9921 - loss: 0.0050 - val_accuracy: 1.0000 - val_loss: 0.0021 - learning_rate: 3.1250e-06\n",
      "Epoch 38/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 77ms/step - accuracy: 0.9922 - loss: 0.0050 - val_accuracy: 1.0000 - val_loss: 0.0021 - learning_rate: 3.1250e-06\n",
      "Data shape: (1058, 21), Prices shape: (1058,), Labels shape: (1058, 3)\n",
      "Observation space shape: (21,)\n",
      "\n",
      "Testing...\n",
      "Test Results: Total Reward: -56.53, Portfolio Value: 11897.54, Profit/Loss: 1897.54, Sharpe: 2.10, Accuracy: 0.80, Buy Count: 8, Sell Count: 8, Hold Count: 1022, Time: 43.41s\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, GRU, Dense, Dropout, Bidirectional\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import yfinance as yf\n",
    "from ta.volatility import BollingerBands\n",
    "from pykalman import KalmanFilter\n",
    "import pywt\n",
    "import time\n",
    "\n",
    "# Focal Loss for Classification with Class Weights\n",
    "def focal_loss(gamma=3.0, alpha=0.5):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0 - 1e-7)\n",
    "        pt = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        alpha_t = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "        focal_loss = -tf.reduce_mean(alpha_t * tf.pow(1.0 - pt, gamma) * tf.math.log(pt))\n",
    "        weights = tf.reduce_sum(y_true * tf.constant([2.0, 2.0, 1.0]), axis=-1)\n",
    "        return focal_loss * weights\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# Improved Attention Layer\n",
    "class ImprovedAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads=4):\n",
    "        super(ImprovedAttentionLayer, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        assert self.d_model % self.num_heads == 0\n",
    "        self.depth = self.d_model // self.num_heads\n",
    "        self.Wq = self.add_weight(name='Wq', shape=(self.d_model, self.d_model), initializer='glorot_uniform')\n",
    "        self.Wk = self.add_weight(name='Wk', shape=(self.d_model, self.d_model), initializer='glorot_uniform')\n",
    "        self.Wv = self.add_weight(name='Wv', shape=(self.d_model, self.d_model), initializer='glorot_uniform')\n",
    "        self.Wo = self.add_weight(name='Wo', shape=(self.d_model, self.d_model), initializer='glorot_uniform')\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        Q = tf.matmul(inputs, self.Wq)\n",
    "        K = tf.matmul(inputs, self.Wk)\n",
    "        V = tf.matmul(inputs, self.Wv)\n",
    "        Q = self.split_heads(Q, batch_size)\n",
    "        K = self.split_heads(K, batch_size)\n",
    "        V = self.split_heads(V, batch_size)\n",
    "        dk = tf.cast(self.depth, tf.float32)\n",
    "        score = tf.matmul(Q, K, transpose_b=True) / tf.math.sqrt(dk)\n",
    "        attention_weights = tf.nn.softmax(score, axis=-1)\n",
    "        context = tf.matmul(attention_weights, V)\n",
    "        context = tf.transpose(context, perm=[0, 2, 1, 3])\n",
    "        context = tf.reshape(context, (batch_size, -1, self.d_model))\n",
    "        output = tf.matmul(context, self.Wo)\n",
    "        return output\n",
    "\n",
    "# Simplified LSTM Model (Single Branch)\n",
    "def create_lstm_model(units, dropout_rate, sequence_length, num_features):\n",
    "    print(\"Creating LSTM model with units:\", units, \"and dropout rate:\", dropout_rate,\n",
    "          \"for sequence length:\", sequence_length, \"and num features:\", num_features)\n",
    "    inputs = Input(shape=(sequence_length, num_features))\n",
    "    x = Bidirectional(LSTM(units, return_sequences=True))(inputs)\n",
    "    x = ImprovedAttentionLayer()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = GRU(64, return_sequences=False)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    class_output = Dense(3, activation='softmax', name='class', kernel_regularizer=tf.keras.regularizers.l2(0.02))(x)\n",
    "    model = Model(inputs, outputs=class_output)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001, clipvalue=0.5)\n",
    "    model.compile(optimizer=optimizer, loss=focal_loss(gamma=3.0, alpha=0.5), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# TradingEnvironment (Updated to Denormalize Portfolio Value)\n",
    "class TradingEnvironment(gym.Env):\n",
    "    def __init__(self, data, features, prices, trend_labels, scaler_y, initial_cash=10000, max_steps=1000):\n",
    "        super(TradingEnvironment, self).__init__()\n",
    "        self.data = data\n",
    "        self.features = features\n",
    "        self.prices = prices  # Normalized prices\n",
    "        self.trend_labels = np.array(trend_labels)\n",
    "        self.scaler_y = scaler_y  # Scaler to denormalize prices\n",
    "        self.initial_cash = initial_cash\n",
    "        self.max_steps = max_steps\n",
    "        self.current_step = 0\n",
    "        # Denormalize initial_cash based on the price scale\n",
    "        self.cash = initial_cash\n",
    "        self.holdings = 0\n",
    "        self.portfolio_values = []\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf,\n",
    "            shape=(len(features),),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        self.feature_means = np.mean(data, axis=0)\n",
    "        self.feature_stds = np.std(data, axis=0) + 1e-6\n",
    "        self.normalized_data = (data - self.feature_means) / self.feature_stds\n",
    "        self.actions = []\n",
    "        self.buy_count = 0\n",
    "        self.sell_count = 0\n",
    "        self.hold_count = 0\n",
    "        self.prev_action = 2\n",
    "        print(f\"Data shape: {self.data.shape}, Prices shape: {self.prices.shape}, Labels shape: {self.trend_labels.shape}\")\n",
    "        print(f\"Observation space shape: {self.observation_space.shape}\")\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.cash = self.initial_cash\n",
    "        self.holdings = 0\n",
    "        self.portfolio_values = []\n",
    "        self.actions = []\n",
    "        self.buy_count = 0\n",
    "        self.sell_count = 0\n",
    "        self.hold_count = 0\n",
    "        self.prev_action = 2\n",
    "        return self._get_observation()\n",
    "\n",
    "    def _get_observation(self):\n",
    "        if self.current_step >= len(self.data):\n",
    "            return np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        state = self.normalized_data[self.current_step]\n",
    "        return state.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        # Denormalize the current price\n",
    "        current_price_normalized = self.prices[self.current_step]\n",
    "        current_price = self.scaler_y.inverse_transform([[current_price_normalized]])[0][0]\n",
    "\n",
    "        reward = 0\n",
    "        if action == 0 and self.cash >= current_price:\n",
    "            self.holdings += 1\n",
    "            self.cash -= current_price\n",
    "            self.buy_count += 1\n",
    "            reward += 0.05\n",
    "        elif action == 1 and self.holdings > 0:\n",
    "            self.holdings -= 1\n",
    "            self.cash += current_price\n",
    "            self.sell_count += 1\n",
    "            reward += 0.05\n",
    "        else:\n",
    "            self.hold_count += 1\n",
    "            if self.prev_action == 2:\n",
    "                reward -= 0.1\n",
    "            else:\n",
    "                reward -= 0.05\n",
    "        self.actions.append(action)\n",
    "        self.prev_action = action\n",
    "\n",
    "        # Compute portfolio value in the original scale\n",
    "        portfolio_value = self.cash + self.holdings * current_price\n",
    "        self.portfolio_values.append(portfolio_value)\n",
    "\n",
    "        if len(self.portfolio_values) > 1:\n",
    "            portfolio_change = (portfolio_value - self.portfolio_values[-2]) / self.initial_cash\n",
    "            reward += portfolio_change * 200\n",
    "            if len(self.portfolio_values) >= 11:\n",
    "                returns = np.diff(self.portfolio_values[-10:]) / self.portfolio_values[-11:-2]\n",
    "                volatility = np.std(returns)\n",
    "                reward -= volatility * 0.5\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= min(len(self.data), self.max_steps)\n",
    "        next_state = self._get_observation() if not done else np.zeros_like(self._get_observation())\n",
    "        info = {'portfolio_value': portfolio_value}\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def get_metrics(self):\n",
    "        total_steps = len(self.actions)\n",
    "        predicted_labels = np.array(self.actions)\n",
    "        true_labels = np.argmax(self.trend_labels[:total_steps], axis=1)\n",
    "        accuracy = np.mean(predicted_labels == true_labels) if total_steps > 0 else 0\n",
    "        final_value = self.portfolio_values[-1] if self.portfolio_values else self.initial_cash\n",
    "        sharpe = 0\n",
    "        if len(self.portfolio_values) > 1:\n",
    "            returns = np.diff(self.portfolio_values) / np.array(self.portfolio_values[:-1])\n",
    "            if len(returns) > 0 and np.std(returns) > 0:\n",
    "                sharpe = np.mean(returns) / np.std(returns) * np.sqrt(252)\n",
    "        return {\n",
    "            'final_portfolio_value': final_value,  # Now in original scale\n",
    "            'profit_loss': final_value - self.initial_cash,  # Now in original scale\n",
    "            'sharpe_ratio': sharpe,\n",
    "            'buy_count': self.buy_count,\n",
    "            'sell_count': self.sell_count,\n",
    "            'hold_count': self.hold_count,\n",
    "            'action_accuracy': accuracy,\n",
    "            'predicted_labels': predicted_labels,\n",
    "            'true_labels': true_labels\n",
    "        }\n",
    "\n",
    "# LSTM Agent\n",
    "class LSTMAgent:\n",
    "    def __init__(self, sequence_length, num_features):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.num_features = num_features\n",
    "        self.model = create_lstm_model(\n",
    "            units=128,\n",
    "            dropout_rate=0.5,\n",
    "            sequence_length=sequence_length,\n",
    "            num_features=num_features\n",
    "        )\n",
    "\n",
    "    def prepare_sequence(self, data, current_step):\n",
    "        if current_step < self.sequence_length:\n",
    "            return np.zeros((self.sequence_length, self.num_features), dtype=np.float32)\n",
    "        sequence = data[max(0, current_step - self.sequence_length):current_step]\n",
    "        if len(sequence) < self.sequence_length:\n",
    "            padding = np.zeros((self.sequence_length - len(sequence), self.num_features), dtype=np.float32)\n",
    "            sequence = np.vstack([padding, sequence])\n",
    "        return sequence\n",
    "\n",
    "    def act(self, sequence):\n",
    "        sequence = np.expand_dims(sequence, axis=0)\n",
    "        class_probs = self.model.predict(sequence, verbose=0)\n",
    "        return np.argmax(class_probs[0])\n",
    "\n",
    "# Plotting functions\n",
    "def plot_confusion_matrix(true_labels, predicted_labels, classes=['UP', 'DOWN', 'NEUTRAL']):\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.title('Confusion Matrix (LSTM)')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.savefig('confusion_matrix_lstm.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_prices(prices, actions, indices, scaler_y):\n",
    "    # Denormalize prices for plotting\n",
    "    prices_denorm = scaler_y.inverse_transform(prices.reshape(-1, 1)).flatten()\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(indices, prices_denorm, label='Price (Target)', color='blue')\n",
    "    buy_signals = indices[actions == 0]\n",
    "    sell_signals = indices[actions == 1]\n",
    "    buy_prices = prices_denorm[actions == 0]\n",
    "    sell_prices = prices_denorm[actions == 1]\n",
    "    plt.scatter(buy_signals, buy_prices, color='green', marker='^', label='Buy', alpha=0.6)\n",
    "    plt.scatter(sell_signals, sell_prices, color='red', marker='v', label='Sell', alpha=0.6)\n",
    "    plt.title('Price with Buy/Sell Signals (LSTM)')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Price (Original Scale)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('price_plot_lstm.png')\n",
    "    plt.close()\n",
    "\n",
    "# Data preparation\n",
    "data = yf.download('^GSPC', start='2010-01-01', end='2025-04-12')\n",
    "\n",
    "# Validate date range\n",
    "print(f\"Data date range: {data.index[0]} to {data.index[-1]}\")\n",
    "print(f\"Raw data shape: {data.shape}\")\n",
    "\n",
    "# Flatten MultiIndex columns\n",
    "if isinstance(data.columns, pd.MultiIndex):\n",
    "    print(\"MultiIndex detected, flattening columns...\")\n",
    "    data.columns = [col[0] for col in data.columns]\n",
    "print(\"Initial columns:\", data.columns.tolist())\n",
    "\n",
    "# Apply Kalman Filter to smooth Close prices\n",
    "kf = KalmanFilter(\n",
    "    transition_matrices=[1],\n",
    "    observation_matrices=[1],\n",
    "    initial_state_mean=data['Close'].iloc[0],\n",
    "    initial_state_covariance=1.0,\n",
    "    observation_covariance=1.0,\n",
    "    transition_covariance=0.1\n",
    ")\n",
    "smoothed_prices, _ = kf.filter(data['Close'].values)\n",
    "data['Target'] = smoothed_prices\n",
    "\n",
    "# Compute indicators\n",
    "data['SMA_crossover'] = data['Close'].rolling(window=200).mean() - data['Close'].rolling(window=50).mean()\n",
    "\n",
    "def average_true_range(df, window=14):\n",
    "    high_low = df['High'] - df['Low']\n",
    "    high_close = abs(df['High'] - df['Close'].shift(1))\n",
    "    low_close = abs(df['Low'] - df['Close'].shift(1))\n",
    "    true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    df['ATR_14'] = true_range.rolling(window).mean()\n",
    "    return df\n",
    "\n",
    "data = average_true_range(data)\n",
    "\n",
    "def on_balance_volume(df):\n",
    "    df['OBV'] = (np.sign(df['Close'].diff()) * df['Volume']).cumsum()\n",
    "    return df\n",
    "\n",
    "data = on_balance_volume(data)\n",
    "\n",
    "def vwap(df):\n",
    "    df['VWAP'] = (df['Close'] * df['Volume']).cumsum() / df['Volume'].cumsum()\n",
    "    return df\n",
    "\n",
    "data = vwap(data)\n",
    "\n",
    "delta = data['Close'].diff()\n",
    "gain = delta.where(delta > 0, 0)\n",
    "loss = -delta.where(delta < 0, 0)\n",
    "avg_gain = gain.rolling(window=14).mean()\n",
    "avg_loss = loss.rolling(window=14).mean()\n",
    "rs = avg_gain / avg_loss\n",
    "data['RSI_14'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "ema_12 = data['Close'].ewm(span=12, adjust=False).mean()\n",
    "ema_26 = data['Close'].ewm(span=26, adjust=False).mean()\n",
    "data['MACD'] = ema_12 - ema_26\n",
    "\n",
    "data['Return'] = data['Close'].pct_change()\n",
    "data['Volatility'] = data['Return'].rolling(window=10).std()\n",
    "\n",
    "bb_indicator = BollingerBands(close=data['Close'], window=20, window_dev=2)\n",
    "data['BB_bandWidth'] = bb_indicator.bollinger_hband() - bb_indicator.bollinger_lband()\n",
    "\n",
    "data['SMA_17'] = data['Close'].rolling(window=17).mean()\n",
    "data['SMA_5'] = data['Close'].rolling(window=5).mean()\n",
    "data['SMA_9'] = data['Close'].rolling(window=9).mean()\n",
    "\n",
    "data['MACD_Signal'] = data['MACD'].ewm(span=9, adjust=False).mean()\n",
    "data['MACD_Histogram'] = data['MACD'] - data['MACD_Signal']\n",
    "\n",
    "# Add lagged features\n",
    "data['Return_Lag1'] = data['Return'].shift(1)\n",
    "data['Volatility_Lag1'] = data['Volatility'].shift(1)\n",
    "data['Return_Lag2'] = data['Return'].shift(2)\n",
    "data['Volatility_Lag2'] = data['Volatility'].shift(2)\n",
    "data['VIX'] = data['Volatility'].rolling(window=20).mean()  # Placeholder for VIX\n",
    "\n",
    "# Wavelet features (3 features)\n",
    "def compute_wavelet_features(prices, wavelet='db4', levels=2):\n",
    "    coeffs = pywt.wavedec(prices, wavelet, level=levels, mode='smooth')\n",
    "    cA2, cD2, cD1 = coeffs[0], coeffs[1], coeffs[2]\n",
    "    min_len = min(len(cA2), len(cD2), len(cD1), len(prices))\n",
    "    return [np.pad(c[:min_len], (0, len(prices) - min_len), mode='constant') for c in [cA2, cD2, cD1]]\n",
    "\n",
    "wavelet_features = compute_wavelet_features(data['Target'].values)\n",
    "for i, name in enumerate(['Wavelet_cA2', 'Wavelet_cD2', 'Wavelet_cD1']):\n",
    "    data[name] = wavelet_features[i]\n",
    "\n",
    "# Normalize features and target\n",
    "features = ['SMA_crossover', 'SMA_17', 'SMA_9', 'SMA_5', 'ATR_14', 'OBV',\n",
    "            'VWAP', 'Volume', 'BB_bandWidth', 'MACD', 'RSI_14', 'MACD_Histogram', 'Volatility',\n",
    "            'Wavelet_cA2', 'Wavelet_cD2', 'Wavelet_cD1', 'Return_Lag1', 'Volatility_Lag1',\n",
    "            'Return_Lag2', 'Volatility_Lag2', 'VIX']\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "data[features] = scaler_X.fit_transform(data[features])\n",
    "data['Target'] = scaler_y.fit_transform(data['Target'].values.reshape(-1, 1))\n",
    "\n",
    "# Debug: Print min and max of Target for denormalization reference\n",
    "print(f\"Target min (original): {scaler_y.data_min_[0]}, Target max (original): {scaler_y.data_max_[0]}\")\n",
    "\n",
    "# Create labels with adjusted threshold\n",
    "def create_labels(y):\n",
    "    labels = np.zeros((len(y), 3))\n",
    "    for i in range(len(y) - 1):\n",
    "        diff = y[i + 1] - y[i]\n",
    "        if diff > 0.005:\n",
    "            labels[i, 0] = 1\n",
    "        elif diff < -0.005:\n",
    "            labels[i, 1] = 1\n",
    "        else:\n",
    "            labels[i, 2] = 1\n",
    "    labels[-1, 2] = 1\n",
    "    return labels\n",
    "\n",
    "# Clean data\n",
    "data.dropna(inplace=True)\n",
    "print(f\"Data shape after dropna: {data.shape}\")\n",
    "\n",
    "# Upsample to 6915 rows (placeholder)\n",
    "if len(data) < 6915:\n",
    "    data = data.reindex(pd.date_range(start=data.index[0], end=data.index[-1], freq='D'))\n",
    "    data = data.interpolate(method='linear')\n",
    "    data = data.dropna()\n",
    "    print(f\"Data shape after upsampling: {data.shape}\")\n",
    "\n",
    "X = data[features].values\n",
    "y = data['Target'].values\n",
    "y_class = create_labels(y)\n",
    "\n",
    "# Debug: Verify shapes and class distribution\n",
    "print(\"Feature data shape:\", X.shape)\n",
    "print(\"Prices shape:\", y.shape)\n",
    "print(\"Trend labels shape:\", y_class.shape)\n",
    "print(\"Class distribution (UP, DOWN, NEUTRAL):\", np.bincount(np.argmax(y_class, axis=1)))\n",
    "\n",
    "# Train-test split\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "y_train_class, y_test_class = y_class[:split], y_class[split:]\n",
    "\n",
    "# Debug: Test set class distribution\n",
    "print(\"Test set class distribution (UP, DOWN, NEUTRAL):\", np.bincount(np.argmax(y_test_class, axis=1)))\n",
    "\n",
    "# Apply SMOTE to training data\n",
    "smote = SMOTE(sampling_strategy={0: 2000, 1: 2000, 2: 8000}, random_state=42)\n",
    "X_train_flat = X_train\n",
    "y_train_class_flat = np.argmax(y_train_class, axis=1)\n",
    "X_train_resampled, y_train_class_resampled = smote.fit_resample(X_train_flat, y_train_class_flat)\n",
    "y_train_class_resampled = np.eye(3)[y_train_class_resampled]\n",
    "\n",
    "# Align y_train_resampled with X_train_resampled\n",
    "n_samples = len(X_train_resampled)\n",
    "y_train_resampled = np.zeros(n_samples)\n",
    "indices = np.arange(len(y_train))\n",
    "sampled_indices = np.random.choice(indices, size=n_samples, replace=True)\n",
    "y_train_resampled = y_train[sampled_indices]\n",
    "\n",
    "# Debug: Verify resampled shapes\n",
    "print(\"X_train_resampled shape:\", X_train_resampled.shape)\n",
    "print(\"y_train_class_resampled shape:\", y_train_class_resampled.shape)\n",
    "print(\"y_train_resampled shape:\", y_train_resampled.shape)\n",
    "\n",
    "# Prepare sequences for LSTM\n",
    "lookback = 20\n",
    "def create_sequences(data, labels, prices, lookback):\n",
    "    if not (len(data) == len(labels) == len(prices)):\n",
    "        raise ValueError(f\"Length mismatch: data={len(data)}, labels={len(labels)}, prices={len(prices)}\")\n",
    "    n_samples = min(len(data), len(labels), len(prices))\n",
    "    sequences = []\n",
    "    seq_labels = []\n",
    "    seq_prices = []\n",
    "    for i in range(lookback, n_samples):\n",
    "        sequences.append(data[i-lookback:i])\n",
    "        seq_labels.append(labels[i])\n",
    "        seq_prices.append(prices[i])\n",
    "    return np.array(sequences), np.array(seq_labels), np.array(seq_prices)\n",
    "\n",
    "# Create sequences\n",
    "X_train_seq, y_train_seq, y_train_prices = create_sequences(X_train_resampled, y_train_class_resampled, y_train_resampled, lookback)\n",
    "X_test_seq, y_test_seq, y_test_prices = create_sequences(X_test, y_test_class, y_test, lookback)\n",
    "\n",
    "# Debug: Verify sequence shapes\n",
    "print(\"Train sequence shape:\", X_train_seq.shape)\n",
    "print(\"Train labels shape:\", y_train_seq.shape)\n",
    "print(\"Train prices shape:\", y_train_prices.shape)\n",
    "print(\"Test sequence shape:\", X_test_seq.shape)\n",
    "print(\"Test labels shape:\", y_test_seq.shape)\n",
    "print(\"Test prices shape:\", y_test_prices.shape)\n",
    "print(\"Resampled class distribution:\", np.bincount(np.argmax(y_train_seq, axis=1)))\n",
    "\n",
    "# Initialize LSTM agent\n",
    "agent = LSTMAgent(sequence_length=lookback, num_features=len(features))\n",
    "\n",
    "# Train LSTM model\n",
    "agent.model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Initialize test environment with scaler_y\n",
    "test_env = TradingEnvironment(X_test, features, y_test, y_test_class, scaler_y=scaler_y, max_steps=len(y_test))\n",
    "\n",
    "# Testing phase\n",
    "print(\"\\nTesting...\")\n",
    "state = test_env.reset()\n",
    "total_reward = 0\n",
    "start_time = time.time()\n",
    "for step in range(lookback, len(X_test)):\n",
    "    sequence = test_env.normalized_data[step-lookback:step]\n",
    "    if len(sequence) < lookback:\n",
    "        sequence = np.pad(sequence, ((lookback-len(sequence), 0), (0, 0)), mode='constant')\n",
    "    action = agent.act(sequence)\n",
    "    next_state, reward, done, info = test_env.step(action)\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "    if done:\n",
    "        break\n",
    "metrics = test_env.get_metrics()\n",
    "print(f\"Test Results: Total Reward: {total_reward:.2f}, \"\n",
    "      f\"Portfolio Value: {metrics['final_portfolio_value']:.2f}, \"\n",
    "      f\"Profit/Loss: {metrics['profit_loss']:.2f}, \"\n",
    "      f\"Sharpe: {metrics['sharpe_ratio']:.2f}, \"\n",
    "      f\"Accuracy: {metrics['action_accuracy']:.2f}, \"\n",
    "      f\"Buy Count: {metrics['buy_count']}, \"\n",
    "      f\"Sell Count: {metrics['sell_count']}, \"\n",
    "      f\"Hold Count: {metrics['hold_count']}, \"\n",
    "      f\"Time: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(metrics['true_labels'], metrics['predicted_labels'])\n",
    "\n",
    "# Plot prices with buy/sell signals (denormalized)\n",
    "test_indices = np.arange(len(metrics['predicted_labels']))\n",
    "plot_prices(y_test[lookback:lookback+len(metrics['predicted_labels'])], metrics['predicted_labels'], test_indices, scaler_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f479e096",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data date range: 2010-01-04 00:00:00 to 2025-04-11 00:00:00\n",
      "Raw data shape: (3843, 5)\n",
      "MultiIndex detected, flattening columns...\n",
      "Initial columns: ['Close', 'High', 'Low', 'Open', 'Volume']\n",
      "Target min (original): 1040.0108244311277, Target max (original): 6111.065995579977\n",
      "Data shape after dropna: (3635, 28)\n",
      "Data shape after upsampling: (5287, 28)\n",
      "Feature data shape: (5287, 21)\n",
      "Prices shape: (5287,)\n",
      "Trend labels shape: (5287, 3)\n",
      "Class distribution (UP, DOWN, NEUTRAL): [  65  101 5121]\n",
      "Test set class distribution (UP, DOWN, NEUTRAL): [ 38  45 975]\n",
      "X_train_resampled shape: (12000, 21)\n",
      "y_train_class_resampled shape: (12000, 3)\n",
      "y_train_resampled shape: (12000,)\n",
      "Train sequence shape: (11980, 20, 21)\n",
      "Train labels shape: (11980, 3)\n",
      "Train prices shape: (11980,)\n",
      "Test sequence shape: (1038, 20, 21)\n",
      "Test labels shape: (1038, 3)\n",
      "Test prices shape: (1038,)\n",
      "Resampled class distribution: [2000 2000 7980]\n",
      "Creating LSTM model with units: 128 and dropout rate: 0.5 for sequence length: 20 and num features: 21\n",
      "Epoch 1/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 65ms/step - accuracy: 0.8360 - loss: 0.1335 - val_accuracy: 1.0000 - val_loss: 0.0646 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 61ms/step - accuracy: 0.9456 - loss: 0.0727 - val_accuracy: 1.0000 - val_loss: 0.0370 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 65ms/step - accuracy: 0.9536 - loss: 0.0453 - val_accuracy: 1.0000 - val_loss: 0.0209 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 65ms/step - accuracy: 0.9705 - loss: 0.0266 - val_accuracy: 1.0000 - val_loss: 0.0116 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 65ms/step - accuracy: 0.9726 - loss: 0.0180 - val_accuracy: 1.0000 - val_loss: 0.0068 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 62ms/step - accuracy: 0.9778 - loss: 0.0128 - val_accuracy: 0.9983 - val_loss: 0.0048 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 65ms/step - accuracy: 0.9814 - loss: 0.0104 - val_accuracy: 0.9992 - val_loss: 0.0035 - learning_rate: 1.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 67ms/step - accuracy: 0.9819 - loss: 0.0089 - val_accuracy: 0.9992 - val_loss: 0.0030 - learning_rate: 1.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 68ms/step - accuracy: 0.9841 - loss: 0.0082 - val_accuracy: 0.9954 - val_loss: 0.0034 - learning_rate: 1.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 62ms/step - accuracy: 0.9820 - loss: 0.0081 - val_accuracy: 0.9992 - val_loss: 0.0025 - learning_rate: 1.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 67ms/step - accuracy: 0.9835 - loss: 0.0074 - val_accuracy: 0.9983 - val_loss: 0.0026 - learning_rate: 1.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 68ms/step - accuracy: 0.9848 - loss: 0.0074 - val_accuracy: 0.9996 - val_loss: 0.0024 - learning_rate: 1.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 68ms/step - accuracy: 0.9863 - loss: 0.0071 - val_accuracy: 0.9962 - val_loss: 0.0031 - learning_rate: 1.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 63ms/step - accuracy: 0.9832 - loss: 0.0075 - val_accuracy: 0.9983 - val_loss: 0.0026 - learning_rate: 1.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 64ms/step - accuracy: 0.9834 - loss: 0.0076 - val_accuracy: 0.9983 - val_loss: 0.0026 - learning_rate: 1.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 69ms/step - accuracy: 0.9861 - loss: 0.0068 - val_accuracy: 0.9996 - val_loss: 0.0022 - learning_rate: 1.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 79ms/step - accuracy: 0.9867 - loss: 0.0069 - val_accuracy: 0.9992 - val_loss: 0.0023 - learning_rate: 1.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 65ms/step - accuracy: 0.9878 - loss: 0.0061 - val_accuracy: 0.9979 - val_loss: 0.0028 - learning_rate: 1.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 65ms/step - accuracy: 0.9825 - loss: 0.0069 - val_accuracy: 0.9983 - val_loss: 0.0026 - learning_rate: 1.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 78ms/step - accuracy: 0.9893 - loss: 0.0063 - val_accuracy: 0.9983 - val_loss: 0.0027 - learning_rate: 1.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 64ms/step - accuracy: 0.9867 - loss: 0.0063 - val_accuracy: 0.9979 - val_loss: 0.0029 - learning_rate: 1.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 62ms/step - accuracy: 0.9858 - loss: 0.0063 - val_accuracy: 0.9987 - val_loss: 0.0026 - learning_rate: 5.0000e-05\n",
      "Epoch 23/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 61ms/step - accuracy: 0.9906 - loss: 0.0057 - val_accuracy: 0.9983 - val_loss: 0.0027 - learning_rate: 5.0000e-05\n",
      "Epoch 24/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 62ms/step - accuracy: 0.9864 - loss: 0.0064 - val_accuracy: 0.9987 - val_loss: 0.0025 - learning_rate: 5.0000e-05\n",
      "Epoch 25/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 64ms/step - accuracy: 0.9891 - loss: 0.0059 - val_accuracy: 0.9987 - val_loss: 0.0026 - learning_rate: 5.0000e-05\n",
      "Epoch 26/50\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 59ms/step - accuracy: 0.9881 - loss: 0.0061 - val_accuracy: 0.9983 - val_loss: 0.0026 - learning_rate: 5.0000e-05\n",
      "Data shape: (1058, 21), Prices shape: (1058,), Labels shape: (1058, 3)\n",
      "Observation space shape: (21,)\n",
      "\n",
      "Testing...\n",
      "Test Results: Total Reward: -84.48, Portfolio Value: 10548.62, Profit/Loss: 548.62, Sharpe: 1.04, Accuracy: 0.80, Buy Count: 2, Sell Count: 1, Hold Count: 1035, Time: 41.94s\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, GRU, Dense, Dropout, Bidirectional\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import yfinance as yf\n",
    "from ta.volatility import BollingerBands\n",
    "from pykalman import KalmanFilter\n",
    "import pywt\n",
    "import time\n",
    "\n",
    "# Focal Loss for Classification with Class Weights\n",
    "def focal_loss(gamma=3.0, alpha=0.5):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0 - 1e-7)\n",
    "        pt = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        alpha_t = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "        focal_loss = -tf.reduce_mean(alpha_t * tf.pow(1.0 - pt, gamma) * tf.math.log(pt))\n",
    "        weights = tf.reduce_sum(y_true * tf.constant([2.0, 2.0, 1.0]), axis=-1)\n",
    "        return focal_loss * weights\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# Improved Attention Layer\n",
    "class ImprovedAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads=4):\n",
    "        super(ImprovedAttentionLayer, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        assert self.d_model % self.num_heads == 0\n",
    "        self.depth = self.d_model // self.num_heads\n",
    "        self.Wq = self.add_weight(name='Wq', shape=(self.d_model, self.d_model), initializer='glorot_uniform')\n",
    "        self.Wk = self.add_weight(name='Wk', shape=(self.d_model, self.d_model), initializer='glorot_uniform')\n",
    "        self.Wv = self.add_weight(name='Wv', shape=(self.d_model, self.d_model), initializer='glorot_uniform')\n",
    "        self.Wo = self.add_weight(name='Wo', shape=(self.d_model, self.d_model), initializer='glorot_uniform')\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        Q = tf.matmul(inputs, self.Wq)\n",
    "        K = tf.matmul(inputs, self.Wk)\n",
    "        V = tf.matmul(inputs, self.Wv)\n",
    "        Q = self.split_heads(Q, batch_size)\n",
    "        K = self.split_heads(K, batch_size)\n",
    "        V = self.split_heads(V, batch_size)\n",
    "        dk = tf.cast(self.depth, tf.float32)\n",
    "        score = tf.matmul(Q, K, transpose_b=True) / tf.math.sqrt(dk)\n",
    "        attention_weights = tf.nn.softmax(score, axis=-1)\n",
    "        context = tf.matmul(attention_weights, V)\n",
    "        context = tf.transpose(context, perm=[0, 2, 1, 3])\n",
    "        context = tf.reshape(context, (batch_size, -1, self.d_model))\n",
    "        output = tf.matmul(context, self.Wo)\n",
    "        return output\n",
    "\n",
    "# Simplified LSTM Model (Single Branch)\n",
    "def create_lstm_model(units, dropout_rate, sequence_length, num_features):\n",
    "    print(\"Creating LSTM model with units:\", units, \"and dropout rate:\", dropout_rate,\n",
    "          \"for sequence length:\", sequence_length, \"and num features:\", num_features)\n",
    "    inputs = Input(shape=(sequence_length, num_features))\n",
    "    x = Bidirectional(LSTM(units, return_sequences=True))(inputs)\n",
    "    x = ImprovedAttentionLayer()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = GRU(64, return_sequences=False)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    class_output = Dense(3, activation='softmax', name='class', kernel_regularizer=tf.keras.regularizers.l2(0.02))(x)\n",
    "    model = Model(inputs, outputs=class_output)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001, clipvalue=0.5)\n",
    "    model.compile(optimizer=optimizer, loss=focal_loss(gamma=3.0, alpha=0.5), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# TradingEnvironment (Updated to Denormalize Portfolio Value)\n",
    "class TradingEnvironment(gym.Env):\n",
    "    def __init__(self, data, features, prices, trend_labels, scaler_y, initial_cash=10000, max_steps=1000):\n",
    "        super(TradingEnvironment, self).__init__()\n",
    "        self.data = data\n",
    "        self.features = features\n",
    "        self.prices = prices  # Normalized prices\n",
    "        self.trend_labels = np.array(trend_labels)\n",
    "        self.scaler_y = scaler_y  # Scaler to denormalize prices\n",
    "        self.initial_cash = initial_cash\n",
    "        self.max_steps = max_steps\n",
    "        self.current_step = 0\n",
    "        # Denormalize initial_cash based on the price scale\n",
    "        self.cash = initial_cash\n",
    "        self.holdings = 0\n",
    "        self.portfolio_values = []\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf,\n",
    "            shape=(len(features),),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        self.feature_means = np.mean(data, axis=0)\n",
    "        self.feature_stds = np.std(data, axis=0) + 1e-6\n",
    "        self.normalized_data = (data - self.feature_means) / self.feature_stds\n",
    "        self.actions = []\n",
    "        self.buy_count = 0\n",
    "        self.sell_count = 0\n",
    "        self.hold_count = 0\n",
    "        self.prev_action = 2\n",
    "        print(f\"Data shape: {self.data.shape}, Prices shape: {self.prices.shape}, Labels shape: {self.trend_labels.shape}\")\n",
    "        print(f\"Observation space shape: {self.observation_space.shape}\")\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.cash = self.initial_cash\n",
    "        self.holdings = 0\n",
    "        self.portfolio_values = []\n",
    "        self.actions = []\n",
    "        self.buy_count = 0\n",
    "        self.sell_count = 0\n",
    "        self.hold_count = 0\n",
    "        self.prev_action = 2\n",
    "        return self._get_observation()\n",
    "\n",
    "    def _get_observation(self):\n",
    "        if self.current_step >= len(self.data):\n",
    "            return np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        state = self.normalized_data[self.current_step]\n",
    "        return state.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        # Denormalize the current price\n",
    "        current_price_normalized = self.prices[self.current_step]\n",
    "        current_price = self.scaler_y.inverse_transform([[current_price_normalized]])[0][0]\n",
    "\n",
    "        reward = 0\n",
    "        if action == 0 and self.cash >= current_price:\n",
    "            self.holdings += 1\n",
    "            self.cash -= current_price\n",
    "            self.buy_count += 1\n",
    "            reward += 0.05\n",
    "        elif action == 1 and self.holdings > 0:\n",
    "            self.holdings -= 1\n",
    "            self.cash += current_price\n",
    "            self.sell_count += 1\n",
    "            reward += 0.05\n",
    "        else:\n",
    "            self.hold_count += 1\n",
    "            if self.prev_action == 2:\n",
    "                reward -= 0.1\n",
    "            else:\n",
    "                reward -= 0.05\n",
    "        self.actions.append(action)\n",
    "        self.prev_action = action\n",
    "\n",
    "        # Compute portfolio value in the original scale\n",
    "        portfolio_value = self.cash + self.holdings * current_price\n",
    "        self.portfolio_values.append(portfolio_value)\n",
    "\n",
    "        if len(self.portfolio_values) > 1:\n",
    "            portfolio_change = (portfolio_value - self.portfolio_values[-2]) / self.initial_cash\n",
    "            reward += portfolio_change * 200\n",
    "            if len(self.portfolio_values) >= 11:\n",
    "                returns = np.diff(self.portfolio_values[-10:]) / self.portfolio_values[-11:-2]\n",
    "                volatility = np.std(returns)\n",
    "                reward -= volatility * 0.5\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= min(len(self.data), self.max_steps)\n",
    "        next_state = self._get_observation() if not done else np.zeros_like(self._get_observation())\n",
    "        info = {'portfolio_value': portfolio_value}\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def get_metrics(self):\n",
    "        total_steps = len(self.actions)\n",
    "        predicted_labels = np.array(self.actions)\n",
    "        true_labels = np.argmax(self.trend_labels[:total_steps], axis=1)\n",
    "        accuracy = np.mean(predicted_labels == true_labels) if total_steps > 0 else 0\n",
    "        final_value = self.portfolio_values[-1] if self.portfolio_values else self.initial_cash\n",
    "        sharpe = 0\n",
    "        if len(self.portfolio_values) > 1:\n",
    "            returns = np.diff(self.portfolio_values) / np.array(self.portfolio_values[:-1])\n",
    "            if len(returns) > 0 and np.std(returns) > 0:\n",
    "                sharpe = np.mean(returns) / np.std(returns) * np.sqrt(252)\n",
    "        return {\n",
    "            'final_portfolio_value': final_value,  # Now in original scale\n",
    "            'profit_loss': final_value - self.initial_cash,  # Now in original scale\n",
    "            'sharpe_ratio': sharpe,\n",
    "            'buy_count': self.buy_count,\n",
    "            'sell_count': self.sell_count,\n",
    "            'hold_count': self.hold_count,\n",
    "            'action_accuracy': accuracy,\n",
    "            'predicted_labels': predicted_labels,\n",
    "            'true_labels': true_labels\n",
    "        }\n",
    "\n",
    "# LSTM Agent\n",
    "class LSTMAgent:\n",
    "    def __init__(self, sequence_length, num_features):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.num_features = num_features\n",
    "        self.model = create_lstm_model(\n",
    "            units=128,\n",
    "            dropout_rate=0.5,\n",
    "            sequence_length=sequence_length,\n",
    "            num_features=num_features\n",
    "        )\n",
    "\n",
    "    def prepare_sequence(self, data, current_step):\n",
    "        if current_step < self.sequence_length:\n",
    "            return np.zeros((self.sequence_length, self.num_features), dtype=np.float32)\n",
    "        sequence = data[max(0, current_step - self.sequence_length):current_step]\n",
    "        if len(sequence) < self.sequence_length:\n",
    "            padding = np.zeros((self.sequence_length - len(sequence), self.num_features), dtype=np.float32)\n",
    "            sequence = np.vstack([padding, sequence])\n",
    "        return sequence\n",
    "\n",
    "    def act(self, sequence):\n",
    "        sequence = np.expand_dims(sequence, axis=0)\n",
    "        class_probs = self.model.predict(sequence, verbose=0)\n",
    "        return np.argmax(class_probs[0])\n",
    "\n",
    "# Plotting functions\n",
    "def plot_confusion_matrix(true_labels, predicted_labels, classes=['UP', 'DOWN', 'NEUTRAL']):\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.title('Confusion Matrix (LSTM)')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.savefig('confusion_matrix_lstm.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_prices(prices, actions, indices, scaler_y):\n",
    "    # Denormalize prices for plotting\n",
    "    prices_denorm = scaler_y.inverse_transform(prices.reshape(-1, 1)).flatten()\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(indices, prices_denorm, label='Price (Target)', color='blue')\n",
    "    buy_signals = indices[actions == 0]\n",
    "    sell_signals = indices[actions == 1]\n",
    "    buy_prices = prices_denorm[actions == 0]\n",
    "    sell_prices = prices_denorm[actions == 1]\n",
    "    plt.scatter(buy_signals, buy_prices, color='green', marker='^', label='Buy', alpha=0.6)\n",
    "    plt.scatter(sell_signals, sell_prices, color='red', marker='v', label='Sell', alpha=0.6)\n",
    "    plt.title('Price with Buy/Sell Signals (LSTM)')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Price (Original Scale)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('price_plot_lstm.png')\n",
    "    plt.close()\n",
    "\n",
    "# Data preparation\n",
    "data = yf.download('^GSPC', start='2010-01-01', end='2025-04-12')\n",
    "\n",
    "# Validate date range\n",
    "print(f\"Data date range: {data.index[0]} to {data.index[-1]}\")\n",
    "print(f\"Raw data shape: {data.shape}\")\n",
    "\n",
    "# Flatten MultiIndex columns\n",
    "if isinstance(data.columns, pd.MultiIndex):\n",
    "    print(\"MultiIndex detected, flattening columns...\")\n",
    "    data.columns = [col[0] for col in data.columns]\n",
    "print(\"Initial columns:\", data.columns.tolist())\n",
    "\n",
    "# Apply Kalman Filter to smooth Close prices\n",
    "kf = KalmanFilter(\n",
    "    transition_matrices=[1],\n",
    "    observation_matrices=[1],\n",
    "    initial_state_mean=data['Close'].iloc[0],\n",
    "    initial_state_covariance=1.0,\n",
    "    observation_covariance=1.0,\n",
    "    transition_covariance=0.1\n",
    ")\n",
    "smoothed_prices, _ = kf.filter(data['Close'].shift(-3).values)\n",
    "data['Target'] = smoothed_prices\n",
    "\n",
    "# Compute indicators\n",
    "data['SMA_crossover'] = data['Close'].rolling(window=200).mean() - data['Close'].rolling(window=50).mean()\n",
    "\n",
    "def average_true_range(df, window=14):\n",
    "    high_low = df['High'] - df['Low']\n",
    "    high_close = abs(df['High'] - df['Close'].shift(1))\n",
    "    low_close = abs(df['Low'] - df['Close'].shift(1))\n",
    "    true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    df['ATR_14'] = true_range.rolling(window).mean()\n",
    "    return df\n",
    "\n",
    "data = average_true_range(data)\n",
    "\n",
    "def on_balance_volume(df):\n",
    "    df['OBV'] = (np.sign(df['Close'].diff()) * df['Volume']).cumsum()\n",
    "    return df\n",
    "\n",
    "data = on_balance_volume(data)\n",
    "\n",
    "def vwap(df):\n",
    "    df['VWAP'] = (df['Close'] * df['Volume']).cumsum() / df['Volume'].cumsum()\n",
    "    return df\n",
    "\n",
    "data = vwap(data)\n",
    "\n",
    "delta = data['Close'].diff()\n",
    "gain = delta.where(delta > 0, 0)\n",
    "loss = -delta.where(delta < 0, 0)\n",
    "avg_gain = gain.rolling(window=14).mean()\n",
    "avg_loss = loss.rolling(window=14).mean()\n",
    "rs = avg_gain / avg_loss\n",
    "data['RSI_14'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "ema_12 = data['Close'].ewm(span=12, adjust=False).mean()\n",
    "ema_26 = data['Close'].ewm(span=26, adjust=False).mean()\n",
    "data['MACD'] = ema_12 - ema_26\n",
    "\n",
    "data['Return'] = data['Close'].pct_change()\n",
    "data['Volatility'] = data['Return'].rolling(window=10).std()\n",
    "\n",
    "bb_indicator = BollingerBands(close=data['Close'], window=20, window_dev=2)\n",
    "data['BB_bandWidth'] = bb_indicator.bollinger_hband() - bb_indicator.bollinger_lband()\n",
    "\n",
    "data['SMA_17'] = data['Close'].rolling(window=17).mean()\n",
    "data['SMA_5'] = data['Close'].rolling(window=5).mean()\n",
    "data['SMA_9'] = data['Close'].rolling(window=9).mean()\n",
    "\n",
    "data['MACD_Signal'] = data['MACD'].ewm(span=9, adjust=False).mean()\n",
    "data['MACD_Histogram'] = data['MACD'] - data['MACD_Signal']\n",
    "\n",
    "# Add lagged features\n",
    "data['Return_Lag1'] = data['Return'].shift(1)\n",
    "data['Volatility_Lag1'] = data['Volatility'].shift(1)\n",
    "data['Return_Lag2'] = data['Return'].shift(2)\n",
    "data['Volatility_Lag2'] = data['Volatility'].shift(2)\n",
    "data['VIX'] = data['Volatility'].rolling(window=20).mean()  # Placeholder for VIX\n",
    "\n",
    "# Wavelet features (3 features)\n",
    "def compute_wavelet_features(prices, wavelet='db4', levels=2):\n",
    "    coeffs = pywt.wavedec(prices, wavelet, level=levels, mode='smooth')\n",
    "    cA2, cD2, cD1 = coeffs[0], coeffs[1], coeffs[2]\n",
    "    min_len = min(len(cA2), len(cD2), len(cD1), len(prices))\n",
    "    return [np.pad(c[:min_len], (0, len(prices) - min_len), mode='constant') for c in [cA2, cD2, cD1]]\n",
    "\n",
    "wavelet_features = compute_wavelet_features(data['Target'].values)\n",
    "for i, name in enumerate(['Wavelet_cA2', 'Wavelet_cD2', 'Wavelet_cD1']):\n",
    "    data[name] = wavelet_features[i]\n",
    "\n",
    "# Normalize features and target\n",
    "features = ['SMA_crossover', 'SMA_17', 'SMA_9', 'SMA_5', 'ATR_14', 'OBV',\n",
    "            'VWAP', 'Volume', 'BB_bandWidth', 'MACD', 'RSI_14', 'MACD_Histogram', 'Volatility',\n",
    "            'Wavelet_cA2', 'Wavelet_cD2', 'Wavelet_cD1', 'Return_Lag1', 'Volatility_Lag1',\n",
    "            'Return_Lag2', 'Volatility_Lag2', 'VIX']\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "data[features] = scaler_X.fit_transform(data[features])\n",
    "data['Target'] = scaler_y.fit_transform(data['Target'].values.reshape(-1, 1))\n",
    "\n",
    "# Debug: Print min and max of Target for denormalization reference\n",
    "print(f\"Target min (original): {scaler_y.data_min_[0]}, Target max (original): {scaler_y.data_max_[0]}\")\n",
    "\n",
    "# Create labels with adjusted threshold\n",
    "def create_labels(y):\n",
    "    labels = np.zeros((len(y), 3))\n",
    "    for i in range(len(y) - 1):\n",
    "        diff = y[i + 1] - y[i]\n",
    "        if diff > 0.005:\n",
    "            labels[i, 0] = 1\n",
    "        elif diff < -0.005:\n",
    "            labels[i, 1] = 1\n",
    "        else:\n",
    "            labels[i, 2] = 1\n",
    "    labels[-1, 2] = 1\n",
    "    return labels\n",
    "\n",
    "# Clean data\n",
    "data.dropna(inplace=True)\n",
    "print(f\"Data shape after dropna: {data.shape}\")\n",
    "\n",
    "# Upsample to 6915 rows (placeholder)\n",
    "if len(data) < 6915:\n",
    "    data = data.reindex(pd.date_range(start=data.index[0], end=data.index[-1], freq='D'))\n",
    "    data = data.interpolate(method='linear')\n",
    "    data = data.dropna()\n",
    "    print(f\"Data shape after upsampling: {data.shape}\")\n",
    "\n",
    "X = data[features].values\n",
    "y = data['Target'].values\n",
    "y_class = create_labels(y)\n",
    "\n",
    "# Debug: Verify shapes and class distribution\n",
    "print(\"Feature data shape:\", X.shape)\n",
    "print(\"Prices shape:\", y.shape)\n",
    "print(\"Trend labels shape:\", y_class.shape)\n",
    "print(\"Class distribution (UP, DOWN, NEUTRAL):\", np.bincount(np.argmax(y_class, axis=1)))\n",
    "\n",
    "# Train-test split\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "y_train_class, y_test_class = y_class[:split], y_class[split:]\n",
    "\n",
    "# Debug: Test set class distribution\n",
    "print(\"Test set class distribution (UP, DOWN, NEUTRAL):\", np.bincount(np.argmax(y_test_class, axis=1)))\n",
    "\n",
    "# Apply SMOTE to training data\n",
    "smote = SMOTE(sampling_strategy={0: 2000, 1: 2000, 2: 8000}, random_state=42)\n",
    "X_train_flat = X_train\n",
    "y_train_class_flat = np.argmax(y_train_class, axis=1)\n",
    "X_train_resampled, y_train_class_resampled = smote.fit_resample(X_train_flat, y_train_class_flat)\n",
    "y_train_class_resampled = np.eye(3)[y_train_class_resampled]\n",
    "\n",
    "# Align y_train_resampled with X_train_resampled\n",
    "n_samples = len(X_train_resampled)\n",
    "y_train_resampled = np.zeros(n_samples)\n",
    "indices = np.arange(len(y_train))\n",
    "sampled_indices = np.random.choice(indices, size=n_samples, replace=True)\n",
    "y_train_resampled = y_train[sampled_indices]\n",
    "\n",
    "# Debug: Verify resampled shapes\n",
    "print(\"X_train_resampled shape:\", X_train_resampled.shape)\n",
    "print(\"y_train_class_resampled shape:\", y_train_class_resampled.shape)\n",
    "print(\"y_train_resampled shape:\", y_train_resampled.shape)\n",
    "\n",
    "# Prepare sequences for LSTM\n",
    "lookback = 20\n",
    "def create_sequences(data, labels, prices, lookback):\n",
    "    if not (len(data) == len(labels) == len(prices)):\n",
    "        raise ValueError(f\"Length mismatch: data={len(data)}, labels={len(labels)}, prices={len(prices)}\")\n",
    "    n_samples = min(len(data), len(labels), len(prices))\n",
    "    sequences = []\n",
    "    seq_labels = []\n",
    "    seq_prices = []\n",
    "    for i in range(lookback, n_samples):\n",
    "        sequences.append(data[i-lookback:i])\n",
    "        seq_labels.append(labels[i])\n",
    "        seq_prices.append(prices[i])\n",
    "    return np.array(sequences), np.array(seq_labels), np.array(seq_prices)\n",
    "\n",
    "# Create sequences\n",
    "X_train_seq, y_train_seq, y_train_prices = create_sequences(X_train_resampled, y_train_class_resampled, y_train_resampled, lookback)\n",
    "X_test_seq, y_test_seq, y_test_prices = create_sequences(X_test, y_test_class, y_test, lookback)\n",
    "\n",
    "# Debug: Verify sequence shapes\n",
    "print(\"Train sequence shape:\", X_train_seq.shape)\n",
    "print(\"Train labels shape:\", y_train_seq.shape)\n",
    "print(\"Train prices shape:\", y_train_prices.shape)\n",
    "print(\"Test sequence shape:\", X_test_seq.shape)\n",
    "print(\"Test labels shape:\", y_test_seq.shape)\n",
    "print(\"Test prices shape:\", y_test_prices.shape)\n",
    "print(\"Resampled class distribution:\", np.bincount(np.argmax(y_train_seq, axis=1)))\n",
    "\n",
    "# Initialize LSTM agent\n",
    "agent = LSTMAgent(sequence_length=lookback, num_features=len(features))\n",
    "\n",
    "# Train LSTM model\n",
    "agent.model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Initialize test environment with scaler_y\n",
    "test_env = TradingEnvironment(X_test, features, y_test, y_test_class, scaler_y=scaler_y, max_steps=len(y_test))\n",
    "\n",
    "# Testing phase\n",
    "print(\"\\nTesting...\")\n",
    "state = test_env.reset()\n",
    "total_reward = 0\n",
    "start_time = time.time()\n",
    "for step in range(lookback, len(X_test)):\n",
    "    sequence = test_env.normalized_data[step-lookback:step]\n",
    "    if len(sequence) < lookback:\n",
    "        sequence = np.pad(sequence, ((lookback-len(sequence), 0), (0, 0)), mode='constant')\n",
    "    action = agent.act(sequence)\n",
    "    next_state, reward, done, info = test_env.step(action)\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "    if done:\n",
    "        break\n",
    "metrics = test_env.get_metrics()\n",
    "print(f\"Test Results: Total Reward: {total_reward:.2f}, \"\n",
    "      f\"Portfolio Value: {metrics['final_portfolio_value']:.2f}, \"\n",
    "      f\"Profit/Loss: {metrics['profit_loss']:.2f}, \"\n",
    "      f\"Sharpe: {metrics['sharpe_ratio']:.2f}, \"\n",
    "      f\"Accuracy: {metrics['action_accuracy']:.2f}, \"\n",
    "      f\"Buy Count: {metrics['buy_count']}, \"\n",
    "      f\"Sell Count: {metrics['sell_count']}, \"\n",
    "      f\"Hold Count: {metrics['hold_count']}, \"\n",
    "      f\"Time: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(metrics['true_labels'], metrics['predicted_labels'])\n",
    "\n",
    "# Plot prices with buy/sell signals (denormalized)\n",
    "test_indices = np.arange(len(metrics['predicted_labels']))\n",
    "plot_prices(y_test[lookback:lookback+len(metrics['predicted_labels'])], metrics['predicted_labels'], test_indices, scaler_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PPOAgent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
